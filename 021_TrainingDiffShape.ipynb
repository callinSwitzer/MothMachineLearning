{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callin Switzer\n",
    "### 6 June 2019\n",
    "___\n",
    "### - Train Dense, Feedforward Neural Network with Keras\n",
    "### - Use data that was generated in Python\n",
    "### - Put velocity in the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow successfully installed.\n",
      "The installed version of TensorFlow includes GPU support.\n",
      "3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)] \n",
      "\n",
      "last run on 2019-06-06 09:01:05.928105\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.colors as colors\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "import subprocess\n",
    "import winsound\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow successfully installed.\")\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"The installed version of TensorFlow includes GPU support.\")\n",
    "print(sys.version, \"\\n\")\n",
    "now = datetime.now()\n",
    "print(\"last run on \" + str(now))\n",
    "\n",
    "# define directories\n",
    "baseDir = os.getcwd()\n",
    "dataDir = r'D:\\MothSimulations\\11c-AggressiveManeuver\\Qstore\\hws_am_con'\n",
    "figDir = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\Figs'\n",
    "dataOutput = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\DataOutput'\n",
    "savedModels = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\savedModels'\n",
    "randomRawData = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\PythonGeneratedData\\TrainingData'\n",
    "if not os.path.exists(dataOutput):\n",
    "    os.mkdir(dataOutput)\n",
    "if not os.path.exists(savedModels):\n",
    "    os.mkdir(savedModels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "# Keras callcacks\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training and test set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# concatenate all files (only need to do this once)\n",
    "# it takes a few minutes\n",
    "all_files = glob.glob(os.path.join(randomRawData, \"*.csv\"))     \n",
    "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "# check for duplicates\n",
    "concatenated_df.drop_duplicates(inplace=True)\n",
    "concatenated_df.shape\n",
    "\n",
    "print(concatenated_df.shape)\n",
    "concatenated_df.tail()\n",
    "\n",
    "# save to hdf5\n",
    "concatenated_df.to_hdf(os.path.join(dataOutput, \"concatenatedRandomICs.h5\"), key = \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "trainDF = pd.read_hdf(os.path.join(dataOutput, \"concatenatedRandomICs.h5\"), key = \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: check for repeats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check for repeats!\n",
    "np.sum(trainDF.iloc[:, [16,17,18]].duplicated()) # 0 means no repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9900510, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>xf</th>\n",
       "      <th>xd0</th>\n",
       "      <th>xdf</th>\n",
       "      <th>y0</th>\n",
       "      <th>yf</th>\n",
       "      <th>yd0</th>\n",
       "      <th>ydf</th>\n",
       "      <th>theta0</th>\n",
       "      <th>thetaf</th>\n",
       "      <th>thetad0</th>\n",
       "      <th>thetadf</th>\n",
       "      <th>phi0</th>\n",
       "      <th>phif</th>\n",
       "      <th>phid0</th>\n",
       "      <th>phidf</th>\n",
       "      <th>F</th>\n",
       "      <th>alpha</th>\n",
       "      <th>tau0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.367432</td>\n",
       "      <td>1240.975696</td>\n",
       "      <td>1047.460389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.222497</td>\n",
       "      <td>-321.289498</td>\n",
       "      <td>-38.377027</td>\n",
       "      <td>4.539917</td>\n",
       "      <td>6.334514</td>\n",
       "      <td>5.761704</td>\n",
       "      <td>166.050842</td>\n",
       "      <td>2.909070</td>\n",
       "      <td>4.628560</td>\n",
       "      <td>10.607821</td>\n",
       "      <td>162.092739</td>\n",
       "      <td>29718.210402</td>\n",
       "      <td>3.057406</td>\n",
       "      <td>53852.149654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.768748</td>\n",
       "      <td>-1149.257088</td>\n",
       "      <td>-1075.409015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.106142</td>\n",
       "      <td>365.093207</td>\n",
       "      <td>697.828947</td>\n",
       "      <td>3.243816</td>\n",
       "      <td>4.921125</td>\n",
       "      <td>16.957561</td>\n",
       "      <td>146.364185</td>\n",
       "      <td>3.308109</td>\n",
       "      <td>4.891662</td>\n",
       "      <td>22.615302</td>\n",
       "      <td>141.667777</td>\n",
       "      <td>28169.483755</td>\n",
       "      <td>3.504285</td>\n",
       "      <td>66579.923810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-25.280989</td>\n",
       "      <td>-1496.313673</td>\n",
       "      <td>-1133.713595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.318522</td>\n",
       "      <td>244.869357</td>\n",
       "      <td>748.985152</td>\n",
       "      <td>5.436117</td>\n",
       "      <td>-0.270820</td>\n",
       "      <td>7.760703</td>\n",
       "      <td>-563.122968</td>\n",
       "      <td>5.548527</td>\n",
       "      <td>-0.137526</td>\n",
       "      <td>0.328348</td>\n",
       "      <td>-561.725041</td>\n",
       "      <td>42042.954099</td>\n",
       "      <td>1.460073</td>\n",
       "      <td>-13638.356558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.931729</td>\n",
       "      <td>-560.948725</td>\n",
       "      <td>-26.956023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.748787</td>\n",
       "      <td>86.858301</td>\n",
       "      <td>45.961219</td>\n",
       "      <td>4.004344</td>\n",
       "      <td>4.826500</td>\n",
       "      <td>-0.067853</td>\n",
       "      <td>72.504068</td>\n",
       "      <td>0.510978</td>\n",
       "      <td>1.423037</td>\n",
       "      <td>24.197259</td>\n",
       "      <td>77.007517</td>\n",
       "      <td>32817.596791</td>\n",
       "      <td>1.943183</td>\n",
       "      <td>-59677.690610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.280447</td>\n",
       "      <td>-214.455052</td>\n",
       "      <td>-539.671502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.707153</td>\n",
       "      <td>225.808462</td>\n",
       "      <td>488.013492</td>\n",
       "      <td>6.129205</td>\n",
       "      <td>4.399997</td>\n",
       "      <td>-16.195650</td>\n",
       "      <td>-177.812751</td>\n",
       "      <td>1.609007</td>\n",
       "      <td>-0.231868</td>\n",
       "      <td>9.118306</td>\n",
       "      <td>-183.140660</td>\n",
       "      <td>35120.738164</td>\n",
       "      <td>3.267484</td>\n",
       "      <td>77836.665059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x0         xf          xd0          xdf   y0         yf         yd0  \\\n",
       "0  0.0  23.367432  1240.975696  1047.460389  0.0  -2.222497 -321.289498   \n",
       "1  0.0 -20.768748 -1149.257088 -1075.409015  0.0  11.106142  365.093207   \n",
       "2  0.0 -25.280989 -1496.313673 -1133.713595  0.0   4.318522  244.869357   \n",
       "3  0.0  -5.931729  -560.948725   -26.956023  0.0   0.748787   86.858301   \n",
       "4  0.0  -9.280447  -214.455052  -539.671502  0.0   6.707153  225.808462   \n",
       "\n",
       "          ydf    theta0    thetaf    thetad0     thetadf      phi0      phif  \\\n",
       "0  -38.377027  4.539917  6.334514   5.761704  166.050842  2.909070  4.628560   \n",
       "1  697.828947  3.243816  4.921125  16.957561  146.364185  3.308109  4.891662   \n",
       "2  748.985152  5.436117 -0.270820   7.760703 -563.122968  5.548527 -0.137526   \n",
       "3   45.961219  4.004344  4.826500  -0.067853   72.504068  0.510978  1.423037   \n",
       "4  488.013492  6.129205  4.399997 -16.195650 -177.812751  1.609007 -0.231868   \n",
       "\n",
       "       phid0       phidf             F     alpha          tau0  \n",
       "0  10.607821  162.092739  29718.210402  3.057406  53852.149654  \n",
       "1  22.615302  141.667777  28169.483755  3.504285  66579.923810  \n",
       "2   0.328348 -561.725041  42042.954099  1.460073 -13638.356558  \n",
       "3  24.197259   77.007517  32817.596791  1.943183 -59677.690610  \n",
       "4   9.118306 -183.140660  35120.738164  3.267484  77836.665059  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainDF.shape)\n",
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to be consistent with other code\n",
    "trainDF.rename(columns={\"x0\" : \"x_0\", \"y0\" : \"y_0\", \"phi0\" : \"phi_0\", \"theta0\" : \"theta_0\", \n",
    "                        \"xf\" : \"x_99\", \"yf\" : \"y_99\", \"phif\" : \"phi_99\", \"thetaf\" : \"theta_99\", \n",
    "                        \"xd0\" : \"x_dot_0\", \"yd0\" : \"y_dot_0\", \"phid0\" : \"phi_dot_0\", \"thetad0\": \"theta_dot_0\", \n",
    "                        \"xdf\" : \"x_dot_99\", \"ydf\": \"y_dot_99\", \"phidf\": \"phi_dot_99\", \"thetadf\": \"theta_dot_99\", \n",
    "                        \"tau0\" : \"tau\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to fx and fy\n",
    "trainDF[\"Fx\"] = trainDF.F * np.cos(trainDF.alpha)\n",
    "trainDF[\"Fy\"] = trainDF.F * np.sin(trainDF.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset\n",
    "X = trainDF.loc[:, [ \"phi_0\", \"theta_0\", \n",
    "                   \"x_dot_0\", \"y_dot_0\", \"phi_dot_0\", \"theta_dot_0\",\n",
    "                   \"x_dot_99\", \"y_dot_99\", \n",
    "                    \"phi_dot_99\", \"theta_dot_99\"]]\n",
    "\n",
    "Y = trainDF.loc[:, [\"Fx\", \"Fy\", \"tau\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phi_0</th>\n",
       "      <th>theta_0</th>\n",
       "      <th>x_dot_0</th>\n",
       "      <th>y_dot_0</th>\n",
       "      <th>phi_dot_0</th>\n",
       "      <th>theta_dot_0</th>\n",
       "      <th>x_dot_99</th>\n",
       "      <th>y_dot_99</th>\n",
       "      <th>phi_dot_99</th>\n",
       "      <th>theta_dot_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.909070</td>\n",
       "      <td>4.539917</td>\n",
       "      <td>1240.975696</td>\n",
       "      <td>-321.289498</td>\n",
       "      <td>10.607821</td>\n",
       "      <td>5.761704</td>\n",
       "      <td>1047.460389</td>\n",
       "      <td>-38.377027</td>\n",
       "      <td>162.092739</td>\n",
       "      <td>166.050842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.308109</td>\n",
       "      <td>3.243816</td>\n",
       "      <td>-1149.257088</td>\n",
       "      <td>365.093207</td>\n",
       "      <td>22.615302</td>\n",
       "      <td>16.957561</td>\n",
       "      <td>-1075.409015</td>\n",
       "      <td>697.828947</td>\n",
       "      <td>141.667777</td>\n",
       "      <td>146.364185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.548527</td>\n",
       "      <td>5.436117</td>\n",
       "      <td>-1496.313673</td>\n",
       "      <td>244.869357</td>\n",
       "      <td>0.328348</td>\n",
       "      <td>7.760703</td>\n",
       "      <td>-1133.713595</td>\n",
       "      <td>748.985152</td>\n",
       "      <td>-561.725041</td>\n",
       "      <td>-563.122968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.510978</td>\n",
       "      <td>4.004344</td>\n",
       "      <td>-560.948725</td>\n",
       "      <td>86.858301</td>\n",
       "      <td>24.197259</td>\n",
       "      <td>-0.067853</td>\n",
       "      <td>-26.956023</td>\n",
       "      <td>45.961219</td>\n",
       "      <td>77.007517</td>\n",
       "      <td>72.504068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.609007</td>\n",
       "      <td>6.129205</td>\n",
       "      <td>-214.455052</td>\n",
       "      <td>225.808462</td>\n",
       "      <td>9.118306</td>\n",
       "      <td>-16.195650</td>\n",
       "      <td>-539.671502</td>\n",
       "      <td>488.013492</td>\n",
       "      <td>-183.140660</td>\n",
       "      <td>-177.812751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phi_0   theta_0      x_dot_0     y_dot_0  phi_dot_0  theta_dot_0  \\\n",
       "0  2.909070  4.539917  1240.975696 -321.289498  10.607821     5.761704   \n",
       "1  3.308109  3.243816 -1149.257088  365.093207  22.615302    16.957561   \n",
       "2  5.548527  5.436117 -1496.313673  244.869357   0.328348     7.760703   \n",
       "3  0.510978  4.004344  -560.948725   86.858301  24.197259    -0.067853   \n",
       "4  1.609007  6.129205  -214.455052  225.808462   9.118306   -16.195650   \n",
       "\n",
       "      x_dot_99    y_dot_99  phi_dot_99  theta_dot_99  \n",
       "0  1047.460389  -38.377027  162.092739    166.050842  \n",
       "1 -1075.409015  697.828947  141.667777    146.364185  \n",
       "2 -1133.713595  748.985152 -561.725041   -563.122968  \n",
       "3   -26.956023   45.961219   77.007517     72.504068  \n",
       "4  -539.671502  488.013492 -183.140660   -177.812751  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fx</th>\n",
       "      <th>Fy</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-29612.961122</td>\n",
       "      <td>2498.912376</td>\n",
       "      <td>53852.149654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-26336.915345</td>\n",
       "      <td>-9994.333651</td>\n",
       "      <td>66579.923810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4645.636361</td>\n",
       "      <td>41785.500502</td>\n",
       "      <td>-13638.356558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-11940.326951</td>\n",
       "      <td>30568.337400</td>\n",
       "      <td>-59677.690610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-34842.795622</td>\n",
       "      <td>-4409.744037</td>\n",
       "      <td>77836.665059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Fx            Fy           tau\n",
       "0 -29612.961122   2498.912376  53852.149654\n",
       "1 -26336.915345  -9994.333651  66579.923810\n",
       "2   4645.636361  41785.500502 -13638.356558\n",
       "3 -11940.326951  30568.337400 -59677.690610\n",
       "4 -34842.795622  -4409.744037  77836.665059"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data \n",
    "scalerX = MinMaxScaler([-0.5, 0.5])  \n",
    "scalerY = MinMaxScaler([-0.5, 0.5])  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scalerX.fit(Xtrain)  \n",
    "scalerY.fit(Ytrain) \n",
    "\n",
    "Xtrain_scaled = scalerX.transform(Xtrain)  \n",
    "Ytrain_scaled = scalerY.transform(Ytrain)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "Xtest_scaled = scalerX.transform(Xtest)\n",
    "Ytest_scaled = scalerY.transform(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phi_0</th>\n",
       "      <th>theta_0</th>\n",
       "      <th>x_dot_0</th>\n",
       "      <th>y_dot_0</th>\n",
       "      <th>phi_dot_0</th>\n",
       "      <th>theta_dot_0</th>\n",
       "      <th>x_dot_99</th>\n",
       "      <th>y_dot_99</th>\n",
       "      <th>phi_dot_99</th>\n",
       "      <th>theta_dot_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.479611</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>-0.491811</td>\n",
       "      <td>-0.220474</td>\n",
       "      <td>0.168732</td>\n",
       "      <td>0.450440</td>\n",
       "      <td>-0.202153</td>\n",
       "      <td>-0.214663</td>\n",
       "      <td>0.088688</td>\n",
       "      <td>0.089438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.453551</td>\n",
       "      <td>0.264615</td>\n",
       "      <td>0.469097</td>\n",
       "      <td>0.195148</td>\n",
       "      <td>0.353807</td>\n",
       "      <td>-0.446865</td>\n",
       "      <td>0.205139</td>\n",
       "      <td>0.206023</td>\n",
       "      <td>-0.053019</td>\n",
       "      <td>-0.052671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.102133</td>\n",
       "      <td>-0.139825</td>\n",
       "      <td>0.486725</td>\n",
       "      <td>0.399421</td>\n",
       "      <td>0.036643</td>\n",
       "      <td>-0.265189</td>\n",
       "      <td>0.349240</td>\n",
       "      <td>0.295930</td>\n",
       "      <td>-0.282329</td>\n",
       "      <td>-0.290446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.025410</td>\n",
       "      <td>0.473391</td>\n",
       "      <td>-0.032577</td>\n",
       "      <td>-0.026313</td>\n",
       "      <td>-0.037384</td>\n",
       "      <td>-0.004011</td>\n",
       "      <td>-0.042385</td>\n",
       "      <td>0.132223</td>\n",
       "      <td>0.092369</td>\n",
       "      <td>0.083292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.492691</td>\n",
       "      <td>-0.269698</td>\n",
       "      <td>-0.191459</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>-0.416592</td>\n",
       "      <td>-0.057925</td>\n",
       "      <td>-0.084081</td>\n",
       "      <td>0.308521</td>\n",
       "      <td>-0.051496</td>\n",
       "      <td>-0.057788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phi_0   theta_0   x_dot_0   y_dot_0  phi_dot_0  theta_dot_0  x_dot_99  \\\n",
       "0 -0.479611  0.002266 -0.491811 -0.220474   0.168732     0.450440 -0.202153   \n",
       "1 -0.453551  0.264615  0.469097  0.195148   0.353807    -0.446865  0.205139   \n",
       "2 -0.102133 -0.139825  0.486725  0.399421   0.036643    -0.265189  0.349240   \n",
       "3 -0.025410  0.473391 -0.032577 -0.026313  -0.037384    -0.004011 -0.042385   \n",
       "4  0.492691 -0.269698 -0.191459  0.408163  -0.416592    -0.057925 -0.084081   \n",
       "\n",
       "   y_dot_99  phi_dot_99  theta_dot_99  \n",
       "0 -0.214663    0.088688      0.089438  \n",
       "1  0.206023   -0.053019     -0.052671  \n",
       "2  0.295930   -0.282329     -0.290446  \n",
       "3  0.132223    0.092369      0.083292  \n",
       "4  0.308521   -0.051496     -0.057788  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Xtrain_scaled, columns = X.columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scalers, to be used on test set\n",
    "scalerfileX = 'scalerX_veloc.pkl'\n",
    "pickle.dump(scalerX, open(os.path.join(dataOutput, scalerfileX), 'wb'))\n",
    "\n",
    "scalerfileY = 'scalerY_veloc.pkl'\n",
    "pickle.dump(scalerY, open(os.path.join(dataOutput, scalerfileY), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: start with small network and then build up\n",
    "# refref: start with large network and prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "def create_network(optimizer = 'rmsprop', \n",
    "                    numUnits = [32, 32, 32, 32], \n",
    "                    weightRegularization = 0.0, \n",
    "                    dropout_rate=0.1):\n",
    "    \n",
    "    '''\n",
    "    Create a feed forward network.  Assumes Xtrain & Ytrain have been created and scaled\n",
    "    \n",
    "    Params: \n",
    "    optimizer (str): choice of optimizer\n",
    "    numUnits (list): number of units in each hidden\n",
    "    weightRegularization (float): between 0 and 1\n",
    "    dropout_rate (float): between 0 and 1\n",
    "    \n",
    "    '''\n",
    "    K.clear_session()\n",
    "    inputs = Input(shape=(Xtrain_scaled.shape[1],))    \n",
    "    \n",
    "    # add layers\n",
    "    for ii in np.arange(0, len(numUnits)):\n",
    "        if ii >= 1: \n",
    "            x = Dense(numUnits[ii], activation='tanh', \n",
    "                      kernel_regularizer=regularizers.l1(weightRegularization))(x)\n",
    "\n",
    "        else: \n",
    "            x = Dense(numUnits[ii], activation='tanh')(inputs)\n",
    "\n",
    "\n",
    "        # add dropout\n",
    "        if dropout_rate > 0: \n",
    "            x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    predictions = Dense(Ytrain_scaled.shape[1], activation='linear')(x)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer = optimizer, metrics = ['mse'])\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt_rmsprop__Dro_0.0__Num_512_512_512_16__Wei_0.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               5632      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                8208      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 539,203\n",
      "Trainable params: 539,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelParams = {\"optimizer\": \"rmsprop\", \n",
    "              \"dropout_rate\" : 0.0, \n",
    "               \"numUnits\": [512, 512, 512, 16],\n",
    "               \"weightRegularization\": 0.0\n",
    "              }\n",
    "\n",
    "\n",
    "model = create_network(**modelParams)\n",
    "\n",
    "modelName = ''.join('{}_{}__'.format(key[0:3].capitalize(), val) for  key, val in modelParams.items()).replace(\"[\", \"\").replace(\"]\", \"\").replace(\", \", \"_\")[0:-2]\n",
    "print(modelName)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_config()\n",
    "earlystop = EarlyStopping(monitor='val_mean_squared_error', patience=50, \n",
    "                          verbose=1, mode='auto', min_delta = 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "historyDict = {\"mean_squared_error\": [], \n",
    "               \"val_mean_squared_error\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5544285 samples, validate on 2376123 samples\n",
      "Epoch 1/1000\n",
      " - 8s - loss: 8.0268e-04 - mean_squared_error: 8.0268e-04 - val_loss: 6.7782e-04 - val_mean_squared_error: 6.7782e-04\n",
      "Epoch 2/1000\n",
      " - 8s - loss: 7.9549e-04 - mean_squared_error: 7.9549e-04 - val_loss: 6.9379e-04 - val_mean_squared_error: 6.9379e-04\n",
      "Epoch 3/1000\n",
      " - 8s - loss: 7.9693e-04 - mean_squared_error: 7.9693e-04 - val_loss: 7.7480e-04 - val_mean_squared_error: 7.7480e-04\n",
      "Epoch 4/1000\n",
      " - 8s - loss: 7.9372e-04 - mean_squared_error: 7.9372e-04 - val_loss: 9.6072e-04 - val_mean_squared_error: 9.6072e-04\n",
      "Epoch 5/1000\n",
      " - 8s - loss: 7.9601e-04 - mean_squared_error: 7.9601e-04 - val_loss: 8.0503e-04 - val_mean_squared_error: 8.0503e-04\n",
      "Epoch 6/1000\n",
      " - 8s - loss: 7.9006e-04 - mean_squared_error: 7.9006e-04 - val_loss: 8.9924e-04 - val_mean_squared_error: 8.9924e-04\n",
      "Epoch 7/1000\n",
      " - 8s - loss: 7.9068e-04 - mean_squared_error: 7.9068e-04 - val_loss: 9.8383e-04 - val_mean_squared_error: 9.8383e-04\n",
      "Epoch 8/1000\n",
      " - 8s - loss: 7.8683e-04 - mean_squared_error: 7.8683e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 9/1000\n",
      " - 8s - loss: 7.8653e-04 - mean_squared_error: 7.8653e-04 - val_loss: 8.3532e-04 - val_mean_squared_error: 8.3532e-04\n",
      "Epoch 10/1000\n",
      " - 8s - loss: 7.8646e-04 - mean_squared_error: 7.8646e-04 - val_loss: 5.5140e-04 - val_mean_squared_error: 5.5140e-04\n",
      "Epoch 11/1000\n",
      " - 8s - loss: 7.8490e-04 - mean_squared_error: 7.8490e-04 - val_loss: 8.6271e-04 - val_mean_squared_error: 8.6271e-04\n",
      "Epoch 12/1000\n",
      " - 8s - loss: 7.8211e-04 - mean_squared_error: 7.8211e-04 - val_loss: 7.6016e-04 - val_mean_squared_error: 7.6016e-04\n",
      "Epoch 13/1000\n",
      " - 8s - loss: 7.8109e-04 - mean_squared_error: 7.8109e-04 - val_loss: 6.7581e-04 - val_mean_squared_error: 6.7581e-04\n",
      "Epoch 14/1000\n",
      " - 8s - loss: 7.7836e-04 - mean_squared_error: 7.7836e-04 - val_loss: 7.3283e-04 - val_mean_squared_error: 7.3283e-04\n",
      "Epoch 15/1000\n",
      " - 8s - loss: 7.7915e-04 - mean_squared_error: 7.7915e-04 - val_loss: 6.6467e-04 - val_mean_squared_error: 6.6467e-04\n",
      "Epoch 16/1000\n",
      " - 8s - loss: 7.7648e-04 - mean_squared_error: 7.7648e-04 - val_loss: 7.9435e-04 - val_mean_squared_error: 7.9435e-04\n",
      "Epoch 17/1000\n",
      " - 8s - loss: 7.7838e-04 - mean_squared_error: 7.7838e-04 - val_loss: 7.4977e-04 - val_mean_squared_error: 7.4977e-04\n",
      "Epoch 18/1000\n",
      " - 8s - loss: 7.7323e-04 - mean_squared_error: 7.7323e-04 - val_loss: 5.7979e-04 - val_mean_squared_error: 5.7979e-04\n",
      "Epoch 19/1000\n",
      " - 8s - loss: 7.6957e-04 - mean_squared_error: 7.6957e-04 - val_loss: 9.4284e-04 - val_mean_squared_error: 9.4284e-04\n",
      "Epoch 20/1000\n",
      " - 8s - loss: 7.7475e-04 - mean_squared_error: 7.7475e-04 - val_loss: 6.1082e-04 - val_mean_squared_error: 6.1082e-04\n",
      "Epoch 21/1000\n",
      " - 8s - loss: 7.7213e-04 - mean_squared_error: 7.7213e-04 - val_loss: 4.9659e-04 - val_mean_squared_error: 4.9659e-04\n",
      "Epoch 22/1000\n",
      " - 9s - loss: 7.6750e-04 - mean_squared_error: 7.6750e-04 - val_loss: 9.5324e-04 - val_mean_squared_error: 9.5324e-04\n",
      "Epoch 23/1000\n",
      " - 9s - loss: 7.6852e-04 - mean_squared_error: 7.6852e-04 - val_loss: 8.9090e-04 - val_mean_squared_error: 8.9090e-04\n",
      "Epoch 24/1000\n",
      " - 8s - loss: 7.6981e-04 - mean_squared_error: 7.6981e-04 - val_loss: 5.0057e-04 - val_mean_squared_error: 5.0057e-04\n",
      "Epoch 25/1000\n",
      " - 8s - loss: 7.6709e-04 - mean_squared_error: 7.6709e-04 - val_loss: 6.8563e-04 - val_mean_squared_error: 6.8563e-04\n",
      "Epoch 26/1000\n",
      " - 8s - loss: 7.6529e-04 - mean_squared_error: 7.6529e-04 - val_loss: 5.9319e-04 - val_mean_squared_error: 5.9319e-04\n",
      "Epoch 27/1000\n",
      " - 8s - loss: 7.6418e-04 - mean_squared_error: 7.6418e-04 - val_loss: 6.9712e-04 - val_mean_squared_error: 6.9712e-04\n",
      "Epoch 28/1000\n",
      " - 8s - loss: 7.6373e-04 - mean_squared_error: 7.6373e-04 - val_loss: 7.3471e-04 - val_mean_squared_error: 7.3471e-04\n",
      "Epoch 29/1000\n",
      " - 8s - loss: 7.6599e-04 - mean_squared_error: 7.6599e-04 - val_loss: 5.3317e-04 - val_mean_squared_error: 5.3317e-04\n",
      "Epoch 30/1000\n",
      " - 8s - loss: 7.6191e-04 - mean_squared_error: 7.6191e-04 - val_loss: 5.7178e-04 - val_mean_squared_error: 5.7178e-04\n",
      "Epoch 31/1000\n",
      " - 8s - loss: 7.6026e-04 - mean_squared_error: 7.6026e-04 - val_loss: 7.8710e-04 - val_mean_squared_error: 7.8710e-04\n",
      "Epoch 32/1000\n",
      " - 8s - loss: 7.6106e-04 - mean_squared_error: 7.6106e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 33/1000\n",
      " - 8s - loss: 7.5634e-04 - mean_squared_error: 7.5634e-04 - val_loss: 7.0948e-04 - val_mean_squared_error: 7.0948e-04\n",
      "Epoch 34/1000\n",
      " - 8s - loss: 7.5979e-04 - mean_squared_error: 7.5979e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 35/1000\n",
      " - 8s - loss: 7.5683e-04 - mean_squared_error: 7.5683e-04 - val_loss: 8.4461e-04 - val_mean_squared_error: 8.4461e-04\n",
      "Epoch 36/1000\n",
      " - 8s - loss: 7.5946e-04 - mean_squared_error: 7.5946e-04 - val_loss: 5.2676e-04 - val_mean_squared_error: 5.2676e-04\n",
      "Epoch 37/1000\n",
      " - 8s - loss: 7.5227e-04 - mean_squared_error: 7.5227e-04 - val_loss: 6.0255e-04 - val_mean_squared_error: 6.0255e-04\n",
      "Epoch 38/1000\n",
      " - 9s - loss: 7.5472e-04 - mean_squared_error: 7.5472e-04 - val_loss: 7.3582e-04 - val_mean_squared_error: 7.3582e-04\n",
      "Epoch 39/1000\n",
      " - 9s - loss: 7.5242e-04 - mean_squared_error: 7.5242e-04 - val_loss: 8.5027e-04 - val_mean_squared_error: 8.5027e-04\n",
      "Epoch 40/1000\n",
      " - 8s - loss: 7.5412e-04 - mean_squared_error: 7.5412e-04 - val_loss: 8.3916e-04 - val_mean_squared_error: 8.3916e-04\n",
      "Epoch 41/1000\n",
      " - 8s - loss: 7.5417e-04 - mean_squared_error: 7.5417e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 42/1000\n",
      " - 8s - loss: 7.5240e-04 - mean_squared_error: 7.5240e-04 - val_loss: 7.6838e-04 - val_mean_squared_error: 7.6838e-04\n",
      "Epoch 43/1000\n",
      " - 8s - loss: 7.5294e-04 - mean_squared_error: 7.5294e-04 - val_loss: 4.5899e-04 - val_mean_squared_error: 4.5899e-04\n",
      "Epoch 44/1000\n",
      " - 8s - loss: 7.5169e-04 - mean_squared_error: 7.5169e-04 - val_loss: 6.6473e-04 - val_mean_squared_error: 6.6473e-04\n",
      "Epoch 45/1000\n",
      " - 8s - loss: 7.4771e-04 - mean_squared_error: 7.4771e-04 - val_loss: 9.0200e-04 - val_mean_squared_error: 9.0200e-04\n",
      "Epoch 46/1000\n",
      " - 8s - loss: 7.5173e-04 - mean_squared_error: 7.5173e-04 - val_loss: 7.3253e-04 - val_mean_squared_error: 7.3253e-04\n",
      "Epoch 47/1000\n",
      " - 8s - loss: 7.4807e-04 - mean_squared_error: 7.4807e-04 - val_loss: 5.1636e-04 - val_mean_squared_error: 5.1636e-04\n",
      "Epoch 48/1000\n",
      " - 8s - loss: 7.4682e-04 - mean_squared_error: 7.4682e-04 - val_loss: 7.4108e-04 - val_mean_squared_error: 7.4108e-04\n",
      "Epoch 49/1000\n",
      " - 8s - loss: 7.4614e-04 - mean_squared_error: 7.4614e-04 - val_loss: 6.4598e-04 - val_mean_squared_error: 6.4598e-04\n",
      "Epoch 50/1000\n",
      " - 8s - loss: 7.4776e-04 - mean_squared_error: 7.4776e-04 - val_loss: 9.2992e-04 - val_mean_squared_error: 9.2992e-04\n",
      "Epoch 51/1000\n",
      " - 8s - loss: 7.4304e-04 - mean_squared_error: 7.4304e-04 - val_loss: 7.5290e-04 - val_mean_squared_error: 7.5290e-04\n",
      "Epoch 52/1000\n",
      " - 8s - loss: 7.4569e-04 - mean_squared_error: 7.4569e-04 - val_loss: 7.9634e-04 - val_mean_squared_error: 7.9634e-04\n",
      "Epoch 53/1000\n",
      " - 8s - loss: 7.3994e-04 - mean_squared_error: 7.3994e-04 - val_loss: 6.1963e-04 - val_mean_squared_error: 6.1963e-04\n",
      "Epoch 54/1000\n",
      " - 8s - loss: 7.4171e-04 - mean_squared_error: 7.4171e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 55/1000\n",
      " - 8s - loss: 7.4130e-04 - mean_squared_error: 7.4130e-04 - val_loss: 8.1169e-04 - val_mean_squared_error: 8.1169e-04\n",
      "Epoch 56/1000\n",
      " - 8s - loss: 7.3966e-04 - mean_squared_error: 7.3966e-04 - val_loss: 8.1621e-04 - val_mean_squared_error: 8.1621e-04\n",
      "Epoch 57/1000\n",
      " - 8s - loss: 7.4379e-04 - mean_squared_error: 7.4379e-04 - val_loss: 4.7394e-04 - val_mean_squared_error: 4.7394e-04\n",
      "Epoch 58/1000\n",
      " - 8s - loss: 7.3908e-04 - mean_squared_error: 7.3908e-04 - val_loss: 5.1204e-04 - val_mean_squared_error: 5.1204e-04\n",
      "Epoch 59/1000\n",
      " - 8s - loss: 7.3771e-04 - mean_squared_error: 7.3771e-04 - val_loss: 6.2856e-04 - val_mean_squared_error: 6.2856e-04\n",
      "Epoch 60/1000\n",
      " - 9s - loss: 7.3612e-04 - mean_squared_error: 7.3612e-04 - val_loss: 6.4874e-04 - val_mean_squared_error: 6.4874e-04\n",
      "Epoch 61/1000\n",
      " - 8s - loss: 7.3329e-04 - mean_squared_error: 7.3329e-04 - val_loss: 6.0387e-04 - val_mean_squared_error: 6.0387e-04\n",
      "Epoch 62/1000\n",
      " - 8s - loss: 7.3987e-04 - mean_squared_error: 7.3987e-04 - val_loss: 8.4401e-04 - val_mean_squared_error: 8.4401e-04\n",
      "Epoch 63/1000\n",
      " - 8s - loss: 7.3483e-04 - mean_squared_error: 7.3483e-04 - val_loss: 6.8855e-04 - val_mean_squared_error: 6.8855e-04\n",
      "Epoch 64/1000\n",
      " - 8s - loss: 7.3484e-04 - mean_squared_error: 7.3484e-04 - val_loss: 8.0776e-04 - val_mean_squared_error: 8.0776e-04\n",
      "Epoch 65/1000\n",
      " - 8s - loss: 7.3571e-04 - mean_squared_error: 7.3571e-04 - val_loss: 5.8263e-04 - val_mean_squared_error: 5.8263e-04\n",
      "Epoch 66/1000\n",
      " - 8s - loss: 7.3310e-04 - mean_squared_error: 7.3310e-04 - val_loss: 5.9777e-04 - val_mean_squared_error: 5.9777e-04\n",
      "Epoch 67/1000\n",
      " - 8s - loss: 7.3244e-04 - mean_squared_error: 7.3244e-04 - val_loss: 7.5117e-04 - val_mean_squared_error: 7.5117e-04\n",
      "Epoch 68/1000\n",
      " - 8s - loss: 7.3257e-04 - mean_squared_error: 7.3257e-04 - val_loss: 8.3529e-04 - val_mean_squared_error: 8.3529e-04\n",
      "Epoch 69/1000\n",
      " - 8s - loss: 7.3264e-04 - mean_squared_error: 7.3264e-04 - val_loss: 6.1938e-04 - val_mean_squared_error: 6.1938e-04\n",
      "Epoch 70/1000\n",
      " - 8s - loss: 7.3147e-04 - mean_squared_error: 7.3147e-04 - val_loss: 5.3254e-04 - val_mean_squared_error: 5.3254e-04\n",
      "Epoch 71/1000\n",
      " - 8s - loss: 7.2918e-04 - mean_squared_error: 7.2918e-04 - val_loss: 8.4681e-04 - val_mean_squared_error: 8.4681e-04\n",
      "Epoch 72/1000\n",
      " - 8s - loss: 7.3088e-04 - mean_squared_error: 7.3088e-04 - val_loss: 7.0474e-04 - val_mean_squared_error: 7.0474e-04\n",
      "Epoch 73/1000\n",
      " - 8s - loss: 7.2981e-04 - mean_squared_error: 7.2981e-04 - val_loss: 7.6994e-04 - val_mean_squared_error: 7.6994e-04\n",
      "Epoch 74/1000\n",
      " - 8s - loss: 7.2584e-04 - mean_squared_error: 7.2584e-04 - val_loss: 5.9612e-04 - val_mean_squared_error: 5.9612e-04\n",
      "Epoch 75/1000\n",
      " - 8s - loss: 7.2653e-04 - mean_squared_error: 7.2653e-04 - val_loss: 5.3702e-04 - val_mean_squared_error: 5.3702e-04\n",
      "Epoch 76/1000\n",
      " - 8s - loss: 7.2747e-04 - mean_squared_error: 7.2747e-04 - val_loss: 6.0784e-04 - val_mean_squared_error: 6.0784e-04\n",
      "Epoch 77/1000\n",
      " - 8s - loss: 7.2443e-04 - mean_squared_error: 7.2443e-04 - val_loss: 4.7180e-04 - val_mean_squared_error: 4.7180e-04\n",
      "Epoch 78/1000\n",
      " - 8s - loss: 7.2615e-04 - mean_squared_error: 7.2615e-04 - val_loss: 4.6090e-04 - val_mean_squared_error: 4.6090e-04\n",
      "Epoch 79/1000\n",
      " - 8s - loss: 7.2307e-04 - mean_squared_error: 7.2307e-04 - val_loss: 8.4515e-04 - val_mean_squared_error: 8.4515e-04\n",
      "Epoch 80/1000\n",
      " - 8s - loss: 7.2671e-04 - mean_squared_error: 7.2671e-04 - val_loss: 5.1544e-04 - val_mean_squared_error: 5.1544e-04\n",
      "Epoch 81/1000\n",
      " - 8s - loss: 7.2082e-04 - mean_squared_error: 7.2082e-04 - val_loss: 7.3554e-04 - val_mean_squared_error: 7.3554e-04\n",
      "Epoch 82/1000\n",
      " - 8s - loss: 7.2467e-04 - mean_squared_error: 7.2467e-04 - val_loss: 7.2501e-04 - val_mean_squared_error: 7.2501e-04\n",
      "Epoch 83/1000\n",
      " - 8s - loss: 7.2292e-04 - mean_squared_error: 7.2292e-04 - val_loss: 5.5767e-04 - val_mean_squared_error: 5.5767e-04\n",
      "Epoch 84/1000\n",
      " - 8s - loss: 7.2124e-04 - mean_squared_error: 7.2124e-04 - val_loss: 7.2695e-04 - val_mean_squared_error: 7.2695e-04\n",
      "Epoch 85/1000\n",
      " - 8s - loss: 7.2002e-04 - mean_squared_error: 7.2002e-04 - val_loss: 6.4038e-04 - val_mean_squared_error: 6.4038e-04\n",
      "Epoch 86/1000\n",
      " - 8s - loss: 7.2015e-04 - mean_squared_error: 7.2015e-04 - val_loss: 9.9712e-04 - val_mean_squared_error: 9.9712e-04\n",
      "Epoch 87/1000\n",
      " - 8s - loss: 7.1853e-04 - mean_squared_error: 7.1853e-04 - val_loss: 5.3411e-04 - val_mean_squared_error: 5.3411e-04\n",
      "Epoch 88/1000\n",
      " - 8s - loss: 7.1767e-04 - mean_squared_error: 7.1767e-04 - val_loss: 6.8708e-04 - val_mean_squared_error: 6.8708e-04\n",
      "Epoch 89/1000\n",
      " - 8s - loss: 7.1866e-04 - mean_squared_error: 7.1866e-04 - val_loss: 6.0541e-04 - val_mean_squared_error: 6.0541e-04\n",
      "Epoch 90/1000\n",
      " - 8s - loss: 7.1348e-04 - mean_squared_error: 7.1348e-04 - val_loss: 8.7180e-04 - val_mean_squared_error: 8.7180e-04\n",
      "Epoch 91/1000\n",
      " - 8s - loss: 7.1680e-04 - mean_squared_error: 7.1680e-04 - val_loss: 6.2945e-04 - val_mean_squared_error: 6.2945e-04\n",
      "Epoch 92/1000\n",
      " - 8s - loss: 7.1632e-04 - mean_squared_error: 7.1632e-04 - val_loss: 7.0418e-04 - val_mean_squared_error: 7.0418e-04\n",
      "Epoch 93/1000\n",
      " - 8s - loss: 7.1524e-04 - mean_squared_error: 7.1524e-04 - val_loss: 5.1967e-04 - val_mean_squared_error: 5.1967e-04\n",
      "Epoch 00093: early stopping\n",
      "759.6446239948273\n"
     ]
    }
   ],
   "source": [
    "# # fit model without regularization\n",
    "stt = time.time()\n",
    "history = model.fit(Xtrain_scaled, Ytrain_scaled, epochs = 1000, verbose = 2, \n",
    "                        batch_size=2**13, callbacks = [earlystop], validation_split = 0.3)\n",
    "winsound.PlaySound(\"*\", winsound.SND_ALIAS)\n",
    "endd = time.time() - stt\n",
    "print(endd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyDict[\"mean_squared_error\"].extend(history.history[\"mean_squared_error\"])\n",
    "historyDict[\"val_mean_squared_error\"].extend(history.history[\"val_mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(savedModels,  modelName + \"_velocity\" + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29a549577b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "# summarize history for accuracy\n",
    "axs.plot(historyDict[\"mean_squared_error\"])\n",
    "axs.plot(historyDict[\"val_mean_squared_error\"])\n",
    "axs.set_title('Model MSE = '+ str((historyDict['val_mean_squared_error'][-1]))[:8])\n",
    "axs.set_ylabel('Mean squared error')\n",
    "axs.set_xlabel('Epoch')\n",
    "axs.legend(['train', 'val'], loc='best')\n",
    "plt.yscale('log') #logarithmic scale for y axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: Can I overfit, if I turn off all regularization? With a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: visualize poossible state space from certain starting spaces using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['val_loss'][-1]# MSE for final epoch testing\n",
    "# why are val MSE vs. loss different????  answer : I think loss incorporates regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['val_mean_squared_error'][-1] # it is the same when there is no regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['loss'][-1]# MSE for final epoch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_e(n):\n",
    "    a = '%E' % n\n",
    "    return a.split('E')[0].rstrip('0').rstrip('.') + 'E' + a.split('E')[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nonzero weights\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts\n",
    "\n",
    "def plot_model_history(model_history, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history.history['mean_squared_error'])+1),\n",
    "             model_history.history['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "             model_history.history['val_mean_squared_error'])\n",
    "    axs.set_title('Model MSE = '+ str(format_e(model_history.history['val_mean_squared_error'][-1])) + \"\\n\" +  str(nzwts) + \" non-zero weights\")\n",
    "    axs.set_ylabel('mean_squared_error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "                   len(model_history.history['val_mean_squared_error'])/10)\n",
    "    axs.legend(['train', 'val'], loc='best')\n",
    "    plt.yscale('log') #logarithmic scale for y axis\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining_\" + modelName + \".png\"), dpi = 120, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "plot_model_history(history, saveFig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(os.path.join(savedModels,  modelName + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "wts = model.get_weights().copy()\n",
    "\n",
    "wtsFile = modelName + '_wts.pkl'\n",
    "pickle.dump(wts, open(os.path.join(dataOutput, wtsFile), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file back in \n",
    "wt2 = pickle.load(open(os.path.join(dataOutput, wtsFile), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END\n",
    "os.path.join(dataOutput, wtsFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## START NEW ITEM: train and trim weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and trim weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelParams = {\"optimizer\": \"rmsprop\", \n",
    "              \"dropout_rate\" : 0.0, \n",
    "               \"numUnits\": [32, 32, 32, 32],\n",
    "               \"weightRegularization\": 0\n",
    "              }\n",
    "\n",
    "\n",
    "model = create_network(**modelParams)\n",
    "wts = model.get_weights().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcolors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = model.get_weights().copy()\n",
    "\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,5, figsize=np.array([30, 10])/2, facecolor='w', edgecolor='k', sharex = \"none\", sharey = \"none\")\n",
    "fig.subplots_adjust(hspace = -0.7, wspace=0.1, bottom = -0.1 )\n",
    "axs = axs.ravel(\"F\")\n",
    "\n",
    "\n",
    "viridis = cm.get_cmap('viridis', 256)\n",
    "newcolors = viridis(np.linspace(0, 1, 256))\n",
    "white = np.array([0.8, 0.8, 0.8, 1])\n",
    "newcolors[256//2-5:256//2+5, :] = white\n",
    "newcmp = ListedColormap(newcolors)\n",
    "\n",
    "\n",
    "for jj, ii in enumerate(np.arange(0, len(wts), 1)):\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    if len(wts[ii].shape) < 2: \n",
    "        wtsTmp = wts[ii].reshape(1,-1).copy()\n",
    "    else:\n",
    "        wtsTmp = wts[ii].copy()\n",
    "    \n",
    "    im = axs[jj].matshow(wtsTmp, cmap = newcmp, norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if np.mod(ii+1, 2) == 0:\n",
    "        axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].axes.set_ylim([-1, 1])\n",
    "        axs[jj].axes.set_xlim([-1, 32])\n",
    "        \n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].tick_params(top = False, labelbottom = True, labeltop = False)\n",
    "        \n",
    "\n",
    "    \n",
    "    if (np.mod(jj, 2) == 0) and (jj != 0):\n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([])\n",
    "        axs[jj].get_yaxis().set_ticks([])\n",
    "            \n",
    "        axs[jj].set_title(str(wtsTmp.shape[0]) + \"x\" + str(wtsTmp.shape[1]) + \" matrix\", loc = \"left\")\n",
    "\n",
    "\n",
    "        #axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].axes.set_ylim([-1, 32])\n",
    "        axs[jj].axes.set_xlim([-1, 32])\n",
    "        #axs[jj].axis('off')\n",
    "        \n",
    "        \n",
    "    if ii == 9:\n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([])\n",
    "        axs[jj].get_yaxis().set_ticks([])\n",
    "\n",
    "\n",
    "        #axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([0, 5])\n",
    "        axs[jj].axes.set_xlim([-1, 32])\n",
    "        \n",
    "    if ii == 0:\n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([])\n",
    "        #axs[jj].get_yaxis().set_ticks([])\n",
    "\n",
    "\n",
    "        #axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].get_yaxis().set_ticks([0, 5])\n",
    "        axs[jj].set_title(str(wtsTmp.shape[0]) + \"x\" + str(wtsTmp.shape[1]) + \" matrix\", loc = \"left\")\n",
    "\n",
    "\n",
    "        #axs[jj].axes.set_ylim([-1, 32])\n",
    "\n",
    "    \n",
    "    #axs[jj].axis('off')\n",
    "\n",
    "cbaxes = inset_axes(axs[8], width=\"5%\", height=\"95%\", loc= \"center\") \n",
    "cbar = fig.colorbar(im, cax=cbaxes, orientation=\"vertical\", ticks = [-10, -1, 0, 1, 10])    \n",
    "    \n",
    "#fig.colorbar(im, orientation=\"horizontal\", pad=0.2)\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(os.path.join(figDir, \"RandomWeightMatrices.png\"), dpi = 500, bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\savedModels\\Opt_rmsprop__Dro_0.0__Num_32_32_32_32__Wei_0_pruned_bias.h5\")\n",
    "\n",
    "modelName = ''.join('{}_{}__'.format(key[0:3].capitalize(), val) for  key, val in modelParams.items()).replace(\"[\", \"\").replace(\"]\", \"\").replace(\", \", \"_\")[0:-2]\n",
    "print(modelName)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = model.get_weights().copy()\n",
    "\n",
    "wtsFile = modelName + '_wts.pkl'\n",
    "pickle.dump(wts, open(os.path.join(dataOutput, wtsFile), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "#wts =  pickle.load(open(os.path.join(dataOutput, wtsFile), 'rb'))\n",
    "\n",
    "# print sizes of each weight matrix\n",
    "wtLengths = []\n",
    "for ii in range(len(wts)):\n",
    "    print(wts[ii].shape)\n",
    "    wtLengths.append(np.prod(wts[ii].shape))\n",
    "\n",
    "print(np.sum(wtLengths), \"total weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "historyDict = {\"mean_squared_error\": [], \n",
    "               \"val_mean_squared_error\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCuts = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: \n",
    "# train until I have very good results\n",
    "# then prune, and retrain until results are close\n",
    "# change pruning rate as data get closer to 100% removed\n",
    "def prune_percent_updater(x):\n",
    "    logit = np.exp(x*8) / (np.exp(x*8) + 1)\n",
    "    return((logit - 0.5)*2*50)\n",
    "\n",
    "\n",
    "# cuts a smaller portion as the percent gets closer to 100%\n",
    "cutPercent = prune_percent_updater(np.linspace(0, 1, 25))\n",
    "\n",
    "while True:   \n",
    "   \n",
    "    for numEpocs in range(100):\n",
    "        \n",
    "        MSE_tmp = []\n",
    "\n",
    "        history = model.fit(Xtrain_scaled, Ytrain_scaled, validation_split = 0.3, \n",
    "                            verbose = 2, batch_size=2**14, epochs = 1, \n",
    "                            callbacks = [earlystop])\n",
    "        \n",
    "        # refref: earlystop doesn't do anything here\n",
    "        \n",
    "        # save history\n",
    "        historyDict[\"mean_squared_error\"].append(history.history[\"mean_squared_error\"][0])\n",
    "        historyDict[\"val_mean_squared_error\"].append(history.history[\"val_mean_squared_error\"][0])\n",
    "        \n",
    "        # local MSE\n",
    "        MSE_tmp.append(history.history[\"mean_squared_error\"][0])\n",
    "\n",
    "        # set weights that are close to 0 all the way back to 0, and then retrain for one epoch\n",
    "        # get nonzero weights\n",
    "        wts = model.get_weights().copy()\n",
    "\n",
    "        # set weights close to 0 to 0 (but ignore biases)\n",
    "        for ii in np.arange(0, len(wts), 1):\n",
    "            qants = np.percentile(np.reshape(wts[ii], -1), q = (50 - cutPercent[numCuts], 50 + cutPercent[numCuts]), )\n",
    "            wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "\n",
    "        # print nonzero weights\n",
    "        # calculate number of nonzero weights\n",
    "        nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "        print(nzwts, \"of\", np.sum(wtLengths), \"weights retained\")\n",
    "\n",
    "        # set new weights and calculate new loss\n",
    "        model.set_weights(wts)\n",
    "        \n",
    "        # check the change in mean squared error, and if it's not changing much, then cut out more data\n",
    "        # calculate slope of loss, based on previous 5 data points\n",
    "        if numEpocs > 5:\n",
    "            inputData = historyDict[\"mean_squared_error\"][-5:]\n",
    "\n",
    "            m = np.shape(inputData)\n",
    "            X = np.matrix([np.ones(m), np.arange(0, len(inputData))]).T\n",
    "            y = np.matrix(np.log(inputData)).T\n",
    "\n",
    "            # Solve for projection matrix\n",
    "            intercept, slope = np.array(np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)).reshape(-1,)\n",
    "            print(\"change in log loss:\", slope)\n",
    "    \n",
    "            # break if slope has stopped changing or if the overall min has been surpassed\n",
    "            # in the first training, it will automatically prune after 5 epochs, because the min will be passed\n",
    "            if (np.abs(slope) < 0.001) or (history.history[\"mean_squared_error\"][0] < np.min(historyDict[\"mean_squared_error\"][:-1])): \n",
    "                print(\"************************************************ PRUNING ********************************************************\")\n",
    "                print(\"************************************************ PRUNING ********************************************************\")\n",
    "                print(\"************************************************ PRUNING ********************************************************\")\n",
    "                break\n",
    "                       \n",
    "                    \n",
    "    ## refref: may want to save weights before each pruning, so I can go back, if I need to\n",
    "    ## refref: should I be pruning the biases too?\n",
    "    \n",
    "    ## keep running tally of min mse, and if we can't get back to the min, then break\n",
    "    print(\"Min MSE for this prune \", np.min(MSE_tmp), \"______overall Min MSE \", np.min(historyDict[\"mean_squared_error\"]))\n",
    "    if np.min(MSE_tmp) > np.min(historyDict[\"mean_squared_error\"]):\n",
    "        print(\"no more gain by pruning:  STOPPING Pruning\")\n",
    "        break\n",
    "    \n",
    "    numCuts += 1\n",
    "    if numCuts > len(cutPercent):\n",
    "        break\n",
    "\n",
    "        \n",
    "        #cutPercent += 0.2\n",
    "winsound.PlaySound(\"*\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numCuts)\n",
    "(50 - cutPercent[numCuts]) * 2 # percent of original network size that is used the pruned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numCuts = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history_fromDict(model_history_dictionary, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history_dictionary['mean_squared_error'])+1),\n",
    "             model_history_dictionary['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history_dictionary['val_mean_squared_error'])+1),\n",
    "             model_history_dictionary['val_mean_squared_error'])\n",
    "    axs.set_title('Model MSE = '+ str(format_e(model_history_dictionary['val_mean_squared_error'][-1])) + \"\\n\" +  str(nzwts) + \" non-zero weights\")\n",
    "    axs.set_ylabel('mean_squared_error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(1,len(model_history_dictionary['val_mean_squared_error'])+1),\n",
    "                   len(model_history_dictionary['val_mean_squared_error'])/10)\n",
    "    axs.legend(['train', 'val'], loc='best')\n",
    "    plt.yscale('log') #logarithmic scale for y axis\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining_\" + modelName + \"_pruned_2.png\"), dpi = 120, bbox_inches='tight')\n",
    "        print(os.path.join(figDir, \"ModelTraining_\" + modelName + \"_pruned.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "plot_model_history_fromDict(historyDict, saveFig = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = model.get_weights().copy()\n",
    "\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,5, figsize=np.array([30, 10])/2, facecolor='w', edgecolor='k', sharex = \"none\", sharey = \"none\")\n",
    "fig.subplots_adjust(hspace = -0.7, wspace=0.1, bottom = -0.1 )\n",
    "axs = axs.ravel(\"F\")\n",
    "\n",
    "\n",
    "viridis = cm.get_cmap('viridis', 256)\n",
    "newcolors = viridis(np.linspace(0, 1, 256))\n",
    "white = np.array([0.8, 0.8, 0.8, 1])\n",
    "newcolors[256//2-5:256//2+5, :] = white\n",
    "newcmp = ListedColormap(newcolors)\n",
    "\n",
    "for jj, ii in enumerate(np.arange(0, len(wts), 1)):\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    if len(wts[ii].shape) < 2: \n",
    "        wtsTmp = wts[ii].reshape(1,-1).copy()\n",
    "    else:\n",
    "        wtsTmp = wts[ii].copy()\n",
    "    \n",
    "    im = axs[jj].matshow(wtsTmp, cmap = newcmp, norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if np.mod(ii+1, 2) == 0:\n",
    "        axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].axes.set_ylim([-1, 1])\n",
    "        axs[jj].axes.set_xlim([-1, 32])\n",
    "        \n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].tick_params(top = False, labelbottom = True, labeltop = False)\n",
    "        \n",
    "\n",
    "    \n",
    "    if (np.mod(jj, 2) == 0) and (jj != 0):\n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([])\n",
    "        axs[jj].get_yaxis().set_ticks([])\n",
    "            \n",
    "        axs[jj].set_title(str(wtsTmp.shape[0]) + \"x\" + str(wtsTmp.shape[1]) + \" matrix\", loc = \"left\")\n",
    "\n",
    "\n",
    "        #axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].axes.set_ylim([-1, 32])\n",
    "        axs[jj].axes.set_xlim([-1, 32])\n",
    "        #axs[jj].axis('off')\n",
    "        \n",
    "        \n",
    "    if ii == 9:\n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([])\n",
    "        axs[jj].get_yaxis().set_ticks([])\n",
    "\n",
    "\n",
    "        #axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([0, 5])\n",
    "        axs[jj].axes.set_xlim([-1, 32])\n",
    "        \n",
    "    if ii == 0:\n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([])\n",
    "        #axs[jj].get_yaxis().set_ticks([])\n",
    "\n",
    "\n",
    "        #axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].get_yaxis().set_ticks([0, 5])\n",
    "        axs[jj].set_title(str(wtsTmp.shape[0]) + \"x\" + str(wtsTmp.shape[1]) + \" matrix\", loc = \"left\")\n",
    "\n",
    "\n",
    "        #axs[jj].axes.set_ylim([-1, 32])\n",
    "\n",
    "    \n",
    "    #axs[jj].axis('off')\n",
    "\n",
    "cbaxes = inset_axes(axs[8], width=\"5%\", height=\"95%\", loc= \"center\") \n",
    "cbar = fig.colorbar(im, cax=cbaxes, orientation=\"vertical\", ticks = [-10, -1, 0, 1, 10])    \n",
    "    \n",
    "#fig.colorbar(im, orientation=\"horizontal\", pad=0.2)\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(os.path.join(figDir, \"Pruned_WeightMatrices.png\"), dpi = 700, bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how good can I get without trimming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot matrices\n",
    "\n",
    "wts = model.get_weights().copy()\n",
    "for ii in np.arange(0, len(wts), 2):\n",
    "    plt.matshow(wts[ii], cmap = \"PRGn\", norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot biases\n",
    "\n",
    "for ii in np.arange(1, len(wts), 2):\n",
    "    plt.matshow(wts[ii].reshape(1, -1), cmap = \"PRGn\", norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: save weights and model\n",
    "model.save(os.path.join(savedModels,  modelName + '_pruned_bias.h5'))\n",
    "\n",
    "# save weights\n",
    "wts = model.get_weights().copy()\n",
    "\n",
    "wtsFile = modelName + '_pruned_wts_bias.pkl'\n",
    "pickle.dump(wts, open(os.path.join(dataOutput, wtsFile), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(1,5, figsize=(20, 5), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.2)\n",
    "\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "for jj, ii in enumerate(np.arange(0, len(wts), 2)):\n",
    "    im = axs[jj].matshow(wts[ii], cmap = \"PRGn\", norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    \n",
    "    \n",
    "plt.colorbar(im,ax=axs[jj], orientation = \"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(5,1, figsize=(30, 10), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.5)\n",
    "\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "for jj, ii in enumerate(np.arange(1, len(wts), 2)):\n",
    "    im = axs[jj].matshow(wts[ii].reshape(1, -1), cmap = \"PRGn\", norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    axs[jj].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.colorbar(im,ax=axs[jj], orientation = \"horizontal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,5, figsize=(30, 10), facecolor='w', edgecolor='k', )\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.1)\n",
    "\n",
    "\n",
    "axs = axs.ravel(\"F\")\n",
    "\n",
    "for jj, ii in enumerate(np.arange(0, len(wts), 1)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(wts[ii].shape) < 2: \n",
    "        wtsTmp = wts[ii].reshape(1,-1).copy()\n",
    "    else:\n",
    "        wtsTmp = wts[ii].copy()\n",
    "    \n",
    "    im = axs[jj].matshow(wtsTmp, cmap = \"PRGn\", norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    if np.mod(ii+1, 2) == 0:\n",
    "        axs[jj].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.colorbar(im,ax=axs[jj], orientation = \"horizontal\")\n",
    "plt.savefig(os.path.join(figDir, \"PrunedWeightMatrices.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(dataOutput, wtsFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: if whole node is basically 0, then remove the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(wts[2].reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_network(**modelParams)\n",
    "\n",
    "history = model.fit(Xtrain_scaled, Ytrain_scaled, validation_split = 0.3, \n",
    "                        verbose = 2, batch_size=2**14, epochs = 1, \n",
    "                        callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if model saved: \n",
    "K.clear_session()\n",
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(savedModels, 'my_model_400Units_newData.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nonzero weights\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,8, figsize=np.array((20,3)) , facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    axs[jj].hist(wts[jj].reshape(-1), bins = 100)\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(wts[jj].shape))\n",
    "    axs[jj+1].hist(wts[jj+1], bins = 100)\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(wts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(Xtest_scaled, Ytest_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nonzero weights\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim weights\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (5, 95), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "    \n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new weights and calculate new loss\n",
    "model.set_weights(wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.05*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.05*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# this is the original model\n",
    "inputs = Input(shape=(Xtrain_scaled.shape[1],))\n",
    "x = Dense(400, activation='tanh')(inputs)\n",
    "x = Dense(400, activation='tanh')(x)\n",
    "x = Dense(400, activation='tanh')(x)\n",
    "x = Dense(16, activation='tanh')(x)\n",
    "predictions = Dense(Ytrain_scaled.shape[1], activation='linear')(x)\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics = ['mse'])\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_mean_squared_error', patience=50, \n",
    "                          verbose=1, mode='auto', min_delta = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "print(nzwts)\n",
    "\n",
    "# trim weights\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (2, 98), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "    \n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "print(nzwts)\n",
    "model.set_weights(wts)\n",
    "\n",
    "\n",
    "# start training\n",
    "history = model.fit(Xtrain_scaled, Ytrain_scaled, validation_split = 0.3, \n",
    "                    verbose = 2, batch_size=2**14, epochs = 1, \n",
    "                    callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = model.get_weights().copy()\n",
    "\n",
    "# trim weights\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (2.5, 97.5), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "\n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "print(nzwts)\n",
    "model.set_weights(wts)\n",
    "\n",
    "model.evaluate(Xtest_scaled, Ytest_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history.history['mean_squared_error'])+1),\n",
    "             model_history.history['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "             model_history.history['val_mean_squared_error'])\n",
    "    axs.set_title('Model MSE')\n",
    "    axs.set_ylabel('mean_squared_error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "                   len(model_history.history['val_mean_squared_error'])/10)\n",
    "    axs.legend(['train', 'val'], loc='best')\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining.png\"), dpi = 120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "plot_model_history(history)\n",
    "print(history.history[\"loss\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model that was trained for much longer\n",
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(savedModels, 'my_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nonzero weights\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new weights and calculate new loss\n",
    "model.set_weights(wts)\n",
    "\n",
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.05*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.05*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = model.get_weights().copy()\n",
    "nzwts = [np.nonzero(wts[ii].reshape(-1))[0] for ii in range(len(wts))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,8, figsize=np.array((20,5)) , facecolor='w', edgecolor='k')\n",
    "#fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    axs[jj].hist(nzwts[jj].reshape(-1), bins = 30)\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(nzwts[jj].shape))\n",
    "    axs[jj+1].hist(nzwts[jj+1], bins = 30)\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(nzwts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,8, figsize=np.array((10,10)) , facecolor='w', edgecolor='k')\n",
    "#fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    axs[jj].hist(wts[jj].reshape(-1), bins = 30)\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(wts[jj].shape))\n",
    "    axs[jj+1].hist(wts[jj+1], bins = 30)\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(wts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "nnpreds = model.predict(Xtest_scaled[ :])\n",
    "\n",
    "# rescale\n",
    "nnpreds_unscaled = scalerY.inverse_transform(nnpreds)\n",
    "\n",
    "# show residuals\n",
    "# combine residual and regular plots\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,7, figsize=np.array((30, 8)) / 1.7, facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.5)\n",
    "axs = axs.ravel()\n",
    "\n",
    "# replace lightest colors with white\n",
    "import matplotlib.colors\n",
    "cmap = plt.cm.magma_r\n",
    "cmaplist = np.array([cmap(i) for i in range(cmap.N)])\n",
    "cmaplist[:,0:3] = np.divide(cmaplist[:, 0:3], 1.1)\n",
    "cmaplist[0] = (1,1,1,0.5)\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('mcm',cmaplist, cmap.N)\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    \n",
    "    try:\n",
    "        axs[ii].hexbin(y = Ytest.iloc[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[ii].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[ii].ticklabel_format(style='sci',  axis='y', scilimits=(3,4))\n",
    "        axs[ii].axes.xaxis.set_ticklabels([])\n",
    "        if(ii == 0):\n",
    "            axs[ii].set_ylabel(\"Actual Value\")\n",
    "        axs[ii].set_title(nms2[ii])\n",
    "        axs[ii].plot(Ytest.iloc[0:1000,ii], Ytest.iloc[0:1000,ii], 'grey', linewidth = 1, linestyle  = \"--\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    jj = ii + len(Y.columns)\n",
    "    \n",
    "    try:\n",
    "        axs[jj].hexbin(y = Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[jj].set_xlabel(\"Predicted Value\")\n",
    "        axs[jj].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[jj].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        mmin = np.min(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        mmax = np.max(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        \n",
    "        upper = np.max([np.abs(mmin), np.abs(mmax)])\n",
    "        axs[jj].set_ylim(-upper, upper)\n",
    "\n",
    "        if(ii == 0):\n",
    "            axs[jj].set_ylabel(\"Actual - Predicted\")\n",
    "        axs[jj].hlines(y = 0, xmin = np.min(nnpreds_unscaled[:,ii]), \n",
    "                       xmax = np.max(nnpreds_unscaled[:,ii]), linestyle =  \"--\", linewidth = 1)\n",
    "    except:\n",
    "        pass\n",
    "plt.tight_layout()\n",
    "#fig.savefig(os.path.join(figDir, \"SmallModelResids.png\"), dpi = 120, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim distribution of weights -- cut out middle 20%\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (40, 60), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "    \n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show new histogram of weights (excluding the 0's)\n",
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,5, figsize=np.array((15, 6)) , facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    \n",
    "    d1 = wts[jj].reshape(-1)\n",
    "    axs[jj].hist(d1[d1!=0], bins = 30, facecolor = '#d6bddb' )\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(wts[jj].shape))\n",
    "\n",
    "    d2 = wts[jj+1]\n",
    "    axs[jj+1].hist(d2[d2!=0], bins = 30, facecolor = '#d6bddb')\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(wts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the validation.split is the last X% of the data\n",
    "int(0.3*Xtrain_scaled.shape[0])\n",
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new weights and calculate new loss\n",
    "model.set_weights(wts)\n",
    "\n",
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "nnpreds = model.predict(Xtest_scaled[ :])\n",
    "\n",
    "# rescale\n",
    "nnpreds_unscaled = scalerY.inverse_transform(nnpreds)\n",
    "\n",
    "# show residuals\n",
    "# combine residual and regular plots\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,7, figsize=np.array((30, 8)) / 1.7, facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.5)\n",
    "axs = axs.ravel()\n",
    "\n",
    "# replace lightest colors with white\n",
    "import matplotlib.colors\n",
    "cmap = plt.cm.magma_r\n",
    "cmaplist = np.array([cmap(i) for i in range(cmap.N)])\n",
    "cmaplist[:,0:3] = np.divide(cmaplist[:, 0:3], 1.1)\n",
    "cmaplist[0] = (1,1,1,0.5)\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('mcm',cmaplist, cmap.N)\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    \n",
    "    try:\n",
    "        axs[ii].hexbin(y = Ytest.iloc[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[ii].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[ii].ticklabel_format(style='sci',  axis='y', scilimits=(3,4))\n",
    "        axs[ii].axes.xaxis.set_ticklabels([])\n",
    "        if(ii == 0):\n",
    "            axs[ii].set_ylabel(\"Actual Value\")\n",
    "        axs[ii].set_title(nms2[ii])\n",
    "        axs[ii].plot(Ytest.iloc[0:1000,ii], Ytest.iloc[0:1000,ii], 'grey', linewidth = 1, linestyle  = \"--\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    jj = ii + len(Y.columns)\n",
    "    \n",
    "    try:\n",
    "        axs[jj].hexbin(y = Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[jj].set_xlabel(\"Predicted Value\")\n",
    "        axs[jj].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[jj].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        mmin = np.min(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        mmax = np.max(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        \n",
    "        upper = np.max([np.abs(mmin), np.abs(mmax)])\n",
    "        axs[jj].set_ylim(-upper, upper)\n",
    "\n",
    "        if(ii == 0):\n",
    "            axs[jj].set_ylabel(\"Actual - Predicted\")\n",
    "        axs[jj].hlines(y = 0, xmin = np.min(nnpreds_unscaled[:,ii]), \n",
    "                       xmax = np.max(nnpreds_unscaled[:,ii]), linestyle =  \"--\", linewidth = 1)\n",
    "    except:\n",
    "        pass\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of hyperparameters\n",
    "\n",
    "\n",
    "# regularization, num layers, num nodes, learning rate, optimizer, activation function, batch size\n",
    "\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Create hyperparameter space\n",
    "NumHiddenLayers = randint(low = 2, high = 20)#[4, 2, 8]\n",
    "numUnits  = [2**4, 2**5, 2**6, 2**7, 2**8, 2**9, 2**10]\n",
    "epochs = [200]\n",
    "batches1 = [2**12, 2**10, 2**8, 2**14] \n",
    "optimizers = ['rmsprop', 'adam']\n",
    "dropout_rate =  uniform(loc = 0, scale = 0.5) #[0.0, 0.2, 0.5]\n",
    "weightRegularization = uniform(loc = 0, scale = 0.001) #[0, 0.0001, 0.001, 0.01]\n",
    "secondToLastUnits = [8, 16, 32, 64]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(optimizer=optimizers, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batches1,\n",
    "                        dropout_rate = dropout_rate, \n",
    "                        numUnits = numUnits, \n",
    "                        NumHiddenLayers = NumHiddenLayers, \n",
    "                        weightRegularization = weightRegularization, \n",
    "                        secondToLastUnits = secondToLastUnits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: \n",
    "# train until I have very good results\n",
    "# then prune, and retrain until results are close\n",
    "# change pruning rate as data get closer to 100% removed\n",
    "cutPercent = 49.7\n",
    "for numCuts in range(3):\n",
    "    for numEpocs in range(100):\n",
    "\n",
    "        history = model.fit(Xtrain_scaled, Ytrain_scaled, validation_split = 0.3, \n",
    "                            verbose = 2, batch_size=2**14, epochs = 1, \n",
    "                            callbacks = [earlystop])\n",
    "        # save history\n",
    "        historyDict[\"mean_squared_error\"].append(history.history[\"mean_squared_error\"][0])\n",
    "        historyDict[\"val_mean_squared_error\"].append(history.history[\"val_mean_squared_error\"][0])\n",
    "\n",
    "        # set weights that are close to 0 all the way back to 0, and then retrain for one epoch\n",
    "        # get nonzero weights\n",
    "        wts = model.get_weights().copy()\n",
    "\n",
    "        # set weights close to 0 to 0 (but ignore biases)\n",
    "        for ii in np.arange(0, len(wts), 2):\n",
    "            qants = np.percentile(np.reshape(wts[ii], -1), q = (50 - cutPercent, 50 + cutPercent), )\n",
    "            wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "\n",
    "        # print nonzero weights\n",
    "        # calculate number of nonzero weights\n",
    "        nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "        print(nzwts, \"of\", np.sum(wtLengths), \"weights retained\")\n",
    "\n",
    "        # set new weights and calculate new loss\n",
    "        model.set_weights(wts)\n",
    "        #cutPercent += 0.2\n",
    "winsound.PlaySound(\"*\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
