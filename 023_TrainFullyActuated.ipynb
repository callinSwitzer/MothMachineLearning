{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callin Switzer\n",
    "### 6 June 2019\n",
    "___\n",
    "### - Train Dense, Feedforward Neural Network with Keras\n",
    "### - Train with fully actuated system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow successfully installed.\n",
      "The installed version of TensorFlow includes GPU support.\n",
      "3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)] \n",
      "\n",
      "last run on 2019-06-10 16:32:05.324821\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.colors as colors\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "import subprocess\n",
    "import winsound\n",
    "import pickle\n",
    "import glob\n",
    "import sqlite3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow successfully installed.\")\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"The installed version of TensorFlow includes GPU support.\")\n",
    "print(sys.version, \"\\n\")\n",
    "now = datetime.now()\n",
    "print(\"last run on \" + str(now))\n",
    "\n",
    "# define directories\n",
    "baseDir = os.getcwd()\n",
    "dataDir = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\PythonGeneratedData_twoTorque'\n",
    "figDir = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\Figs'\n",
    "dataOutput = r'D:/Dropbox/AcademiaDropbox/mothMachineLearning_dataAndFigs/DataOutput_twoTorque'\n",
    "savedModels = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\savedModels'\n",
    "if not os.path.exists(dataOutput):\n",
    "    os.mkdir(dataOutput)\n",
    "if not os.path.exists(savedModels):\n",
    "    os.mkdir(savedModels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "# Keras callcacks\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training and test set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n"
     ]
    }
   ],
   "source": [
    "# get table names in database\n",
    "con1 = sqlite3.connect(os.path.join(dataDir, \"twoTorqueData.db\"))\n",
    "cursorObj = con1.cursor()\n",
    "res = cursorObj.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tableNames = [name[0] for name in res]\n",
    "con1.close()\n",
    "print(tableNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "con1 = sqlite3.connect(os.path.join(dataDir, \"twoTorqueData.db\"))\n",
    "trainDF = pd.read_sql_query(\"SELECT * FROM train\", con1)\n",
    "testDF = pd.read_sql_query(\"SELECT * FROM test\", con1)\n",
    "con1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>xd_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>yd_0</th>\n",
       "      <th>theta_0</th>\n",
       "      <th>thetad_0</th>\n",
       "      <th>phi_0</th>\n",
       "      <th>phid_0</th>\n",
       "      <th>x_f</th>\n",
       "      <th>xd_f</th>\n",
       "      <th>y_f</th>\n",
       "      <th>yd_f</th>\n",
       "      <th>theta_f</th>\n",
       "      <th>thetad_f</th>\n",
       "      <th>phi_f</th>\n",
       "      <th>phid_f</th>\n",
       "      <th>F</th>\n",
       "      <th>alpha</th>\n",
       "      <th>tau0</th>\n",
       "      <th>tau_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1147.407904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-238.429602</td>\n",
       "      <td>1.123847</td>\n",
       "      <td>23.498330</td>\n",
       "      <td>4.482445</td>\n",
       "      <td>12.691013</td>\n",
       "      <td>18.723606</td>\n",
       "      <td>691.882850</td>\n",
       "      <td>-4.739545</td>\n",
       "      <td>-293.363224</td>\n",
       "      <td>4.014081</td>\n",
       "      <td>308.066760</td>\n",
       "      <td>8.139222</td>\n",
       "      <td>345.558353</td>\n",
       "      <td>30853.584922</td>\n",
       "      <td>1.797866</td>\n",
       "      <td>-546297.092872</td>\n",
       "      <td>10262.953817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1152.043723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>369.845453</td>\n",
       "      <td>4.645359</td>\n",
       "      <td>5.927584</td>\n",
       "      <td>3.398032</td>\n",
       "      <td>-19.816137</td>\n",
       "      <td>-23.448395</td>\n",
       "      <td>-706.563802</td>\n",
       "      <td>8.272804</td>\n",
       "      <td>684.314462</td>\n",
       "      <td>14.631620</td>\n",
       "      <td>851.354510</td>\n",
       "      <td>11.946214</td>\n",
       "      <td>778.225185</td>\n",
       "      <td>31872.475362</td>\n",
       "      <td>2.658456</td>\n",
       "      <td>961528.396769</td>\n",
       "      <td>36965.947717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-592.993151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-525.687613</td>\n",
       "      <td>5.220594</td>\n",
       "      <td>6.983077</td>\n",
       "      <td>4.307985</td>\n",
       "      <td>-23.696882</td>\n",
       "      <td>-4.641471</td>\n",
       "      <td>-608.960470</td>\n",
       "      <td>-5.348081</td>\n",
       "      <td>430.757954</td>\n",
       "      <td>9.520878</td>\n",
       "      <td>942.874265</td>\n",
       "      <td>8.779522</td>\n",
       "      <td>955.717081</td>\n",
       "      <td>21305.283236</td>\n",
       "      <td>2.463747</td>\n",
       "      <td>-313643.967698</td>\n",
       "      <td>45809.941477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1338.965230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.191760</td>\n",
       "      <td>3.628833</td>\n",
       "      <td>-17.119889</td>\n",
       "      <td>2.109267</td>\n",
       "      <td>-11.436424</td>\n",
       "      <td>26.318800</td>\n",
       "      <td>623.841539</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>863.443021</td>\n",
       "      <td>16.589887</td>\n",
       "      <td>1316.282852</td>\n",
       "      <td>15.180057</td>\n",
       "      <td>1307.874474</td>\n",
       "      <td>19428.750439</td>\n",
       "      <td>0.374967</td>\n",
       "      <td>-203911.489339</td>\n",
       "      <td>47599.081146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1361.776972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>664.606866</td>\n",
       "      <td>1.468084</td>\n",
       "      <td>8.355107</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>8.528276</td>\n",
       "      <td>-27.521247</td>\n",
       "      <td>-1473.234824</td>\n",
       "      <td>14.165396</td>\n",
       "      <td>774.628129</td>\n",
       "      <td>3.269968</td>\n",
       "      <td>165.211558</td>\n",
       "      <td>1.712961</td>\n",
       "      <td>160.158577</td>\n",
       "      <td>8084.383659</td>\n",
       "      <td>1.102396</td>\n",
       "      <td>63102.747684</td>\n",
       "      <td>6365.517419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_0         xd_0  y_0        yd_0   theta_0   thetad_0     phi_0  \\\n",
       "0  0.0  1147.407904  0.0 -238.429602  1.123847  23.498330  4.482445   \n",
       "1  0.0 -1152.043723  0.0  369.845453  4.645359   5.927584  3.398032   \n",
       "2  0.0  -592.993151  0.0 -525.687613  5.220594   6.983077  4.307985   \n",
       "3  0.0  1338.965230  0.0   61.191760  3.628833 -17.119889  2.109267   \n",
       "4  0.0 -1361.776972  0.0  664.606866  1.468084   8.355107  0.008353   \n",
       "\n",
       "      phid_0        x_f         xd_f        y_f        yd_f    theta_f  \\\n",
       "0  12.691013  18.723606   691.882850  -4.739545 -293.363224   4.014081   \n",
       "1 -19.816137 -23.448395  -706.563802   8.272804  684.314462  14.631620   \n",
       "2 -23.696882  -4.641471  -608.960470  -5.348081  430.757954   9.520878   \n",
       "3 -11.436424  26.318800   623.841539   0.005384  863.443021  16.589887   \n",
       "4   8.528276 -27.521247 -1473.234824  14.165396  774.628129   3.269968   \n",
       "\n",
       "      thetad_f      phi_f       phid_f             F     alpha           tau0  \\\n",
       "0   308.066760   8.139222   345.558353  30853.584922  1.797866 -546297.092872   \n",
       "1   851.354510  11.946214   778.225185  31872.475362  2.658456  961528.396769   \n",
       "2   942.874265   8.779522   955.717081  21305.283236  2.463747 -313643.967698   \n",
       "3  1316.282852  15.180057  1307.874474  19428.750439  0.374967 -203911.489339   \n",
       "4   165.211558   1.712961   160.158577   8084.383659  1.102396   63102.747684   \n",
       "\n",
       "          tau_w  \n",
       "0  10262.953817  \n",
       "1  36965.947717  \n",
       "2  45809.941477  \n",
       "3  47599.081146  \n",
       "4   6365.517419  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>xd_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>yd_0</th>\n",
       "      <th>theta_0</th>\n",
       "      <th>thetad_0</th>\n",
       "      <th>phi_0</th>\n",
       "      <th>phid_0</th>\n",
       "      <th>x_f</th>\n",
       "      <th>xd_f</th>\n",
       "      <th>y_f</th>\n",
       "      <th>yd_f</th>\n",
       "      <th>theta_f</th>\n",
       "      <th>thetad_f</th>\n",
       "      <th>phi_f</th>\n",
       "      <th>phid_f</th>\n",
       "      <th>F</th>\n",
       "      <th>alpha</th>\n",
       "      <th>tau0</th>\n",
       "      <th>tau_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1290.475733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1031.459220</td>\n",
       "      <td>5.901014</td>\n",
       "      <td>9.348979</td>\n",
       "      <td>3.643856</td>\n",
       "      <td>17.157149</td>\n",
       "      <td>26.235253</td>\n",
       "      <td>1137.037978</td>\n",
       "      <td>20.847415</td>\n",
       "      <td>289.689136</td>\n",
       "      <td>18.537756</td>\n",
       "      <td>1221.875359</td>\n",
       "      <td>15.098098</td>\n",
       "      <td>1167.837706</td>\n",
       "      <td>10507.464077</td>\n",
       "      <td>0.559591</td>\n",
       "      <td>799932.656919</td>\n",
       "      <td>61301.505115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-643.287407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-812.471891</td>\n",
       "      <td>0.923325</td>\n",
       "      <td>5.317080</td>\n",
       "      <td>3.764966</td>\n",
       "      <td>-24.582937</td>\n",
       "      <td>-9.018544</td>\n",
       "      <td>-1514.026571</td>\n",
       "      <td>-11.076141</td>\n",
       "      <td>-1014.695373</td>\n",
       "      <td>-10.546085</td>\n",
       "      <td>-1478.783791</td>\n",
       "      <td>-6.871128</td>\n",
       "      <td>-1435.593398</td>\n",
       "      <td>34686.690435</td>\n",
       "      <td>1.063056</td>\n",
       "      <td>-737949.278098</td>\n",
       "      <td>-97316.403803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>817.563373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1295.713095</td>\n",
       "      <td>3.073726</td>\n",
       "      <td>4.217052</td>\n",
       "      <td>0.532687</td>\n",
       "      <td>-4.945442</td>\n",
       "      <td>13.080550</td>\n",
       "      <td>-325.919134</td>\n",
       "      <td>-23.274466</td>\n",
       "      <td>-817.967701</td>\n",
       "      <td>17.014216</td>\n",
       "      <td>1687.486984</td>\n",
       "      <td>14.956286</td>\n",
       "      <td>1693.827720</td>\n",
       "      <td>27441.473272</td>\n",
       "      <td>0.549398</td>\n",
       "      <td>-575984.856139</td>\n",
       "      <td>87908.705311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-818.423313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1230.998812</td>\n",
       "      <td>0.037942</td>\n",
       "      <td>-24.356912</td>\n",
       "      <td>1.783597</td>\n",
       "      <td>17.914569</td>\n",
       "      <td>-13.849188</td>\n",
       "      <td>-1982.346897</td>\n",
       "      <td>19.697223</td>\n",
       "      <td>1963.656391</td>\n",
       "      <td>-11.793649</td>\n",
       "      <td>-1541.793049</td>\n",
       "      <td>-10.608393</td>\n",
       "      <td>-1556.139800</td>\n",
       "      <td>4053.551364</td>\n",
       "      <td>3.837047</td>\n",
       "      <td>617346.567027</td>\n",
       "      <td>-53108.537683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1240.409449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1375.065310</td>\n",
       "      <td>3.503992</td>\n",
       "      <td>9.262691</td>\n",
       "      <td>2.659674</td>\n",
       "      <td>-6.227951</td>\n",
       "      <td>-10.039456</td>\n",
       "      <td>-1427.009664</td>\n",
       "      <td>-9.628598</td>\n",
       "      <td>-1844.939378</td>\n",
       "      <td>5.908790</td>\n",
       "      <td>590.782422</td>\n",
       "      <td>4.559646</td>\n",
       "      <td>516.295982</td>\n",
       "      <td>17916.938852</td>\n",
       "      <td>1.558088</td>\n",
       "      <td>967963.511996</td>\n",
       "      <td>52430.831318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_0         xd_0  y_0         yd_0   theta_0   thetad_0     phi_0  \\\n",
       "0  0.0  1290.475733  0.0  1031.459220  5.901014   9.348979  3.643856   \n",
       "1  0.0  -643.287407  0.0  -812.471891  0.923325   5.317080  3.764966   \n",
       "2  0.0   817.563373  0.0 -1295.713095  3.073726   4.217052  0.532687   \n",
       "3  0.0  -818.423313  0.0  1230.998812  0.037942 -24.356912  1.783597   \n",
       "4  0.0 -1240.409449  0.0 -1375.065310  3.503992   9.262691  2.659674   \n",
       "\n",
       "      phid_0        x_f         xd_f        y_f         yd_f    theta_f  \\\n",
       "0  17.157149  26.235253  1137.037978  20.847415   289.689136  18.537756   \n",
       "1 -24.582937  -9.018544 -1514.026571 -11.076141 -1014.695373 -10.546085   \n",
       "2  -4.945442  13.080550  -325.919134 -23.274466  -817.967701  17.014216   \n",
       "3  17.914569 -13.849188 -1982.346897  19.697223  1963.656391 -11.793649   \n",
       "4  -6.227951 -10.039456 -1427.009664  -9.628598 -1844.939378   5.908790   \n",
       "\n",
       "      thetad_f      phi_f       phid_f             F     alpha           tau0  \\\n",
       "0  1221.875359  15.098098  1167.837706  10507.464077  0.559591  799932.656919   \n",
       "1 -1478.783791  -6.871128 -1435.593398  34686.690435  1.063056 -737949.278098   \n",
       "2  1687.486984  14.956286  1693.827720  27441.473272  0.549398 -575984.856139   \n",
       "3 -1541.793049 -10.608393 -1556.139800   4053.551364  3.837047  617346.567027   \n",
       "4   590.782422   4.559646   516.295982  17916.938852  1.558088  967963.511996   \n",
       "\n",
       "          tau_w  \n",
       "0  61301.505115  \n",
       "1 -97316.403803  \n",
       "2  87908.705311  \n",
       "3 -53108.537683  \n",
       "4  52430.831318  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check for repeats!\n",
    "np.sum(trainDF.iloc[:, [16,17,18]].duplicated()) # 0 means no repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000000, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>xd_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>yd_0</th>\n",
       "      <th>theta_0</th>\n",
       "      <th>thetad_0</th>\n",
       "      <th>phi_0</th>\n",
       "      <th>phid_0</th>\n",
       "      <th>x_f</th>\n",
       "      <th>xd_f</th>\n",
       "      <th>y_f</th>\n",
       "      <th>yd_f</th>\n",
       "      <th>theta_f</th>\n",
       "      <th>thetad_f</th>\n",
       "      <th>phi_f</th>\n",
       "      <th>phid_f</th>\n",
       "      <th>F</th>\n",
       "      <th>alpha</th>\n",
       "      <th>tau0</th>\n",
       "      <th>tau_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1147.407904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-238.429602</td>\n",
       "      <td>1.123847</td>\n",
       "      <td>23.498330</td>\n",
       "      <td>4.482445</td>\n",
       "      <td>12.691013</td>\n",
       "      <td>18.723606</td>\n",
       "      <td>691.882850</td>\n",
       "      <td>-4.739545</td>\n",
       "      <td>-293.363224</td>\n",
       "      <td>4.014081</td>\n",
       "      <td>308.066760</td>\n",
       "      <td>8.139222</td>\n",
       "      <td>345.558353</td>\n",
       "      <td>30853.584922</td>\n",
       "      <td>1.797866</td>\n",
       "      <td>-546297.092872</td>\n",
       "      <td>10262.953817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1152.043723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>369.845453</td>\n",
       "      <td>4.645359</td>\n",
       "      <td>5.927584</td>\n",
       "      <td>3.398032</td>\n",
       "      <td>-19.816137</td>\n",
       "      <td>-23.448395</td>\n",
       "      <td>-706.563802</td>\n",
       "      <td>8.272804</td>\n",
       "      <td>684.314462</td>\n",
       "      <td>14.631620</td>\n",
       "      <td>851.354510</td>\n",
       "      <td>11.946214</td>\n",
       "      <td>778.225185</td>\n",
       "      <td>31872.475362</td>\n",
       "      <td>2.658456</td>\n",
       "      <td>961528.396769</td>\n",
       "      <td>36965.947717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-592.993151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-525.687613</td>\n",
       "      <td>5.220594</td>\n",
       "      <td>6.983077</td>\n",
       "      <td>4.307985</td>\n",
       "      <td>-23.696882</td>\n",
       "      <td>-4.641471</td>\n",
       "      <td>-608.960470</td>\n",
       "      <td>-5.348081</td>\n",
       "      <td>430.757954</td>\n",
       "      <td>9.520878</td>\n",
       "      <td>942.874265</td>\n",
       "      <td>8.779522</td>\n",
       "      <td>955.717081</td>\n",
       "      <td>21305.283236</td>\n",
       "      <td>2.463747</td>\n",
       "      <td>-313643.967698</td>\n",
       "      <td>45809.941477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1338.965230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.191760</td>\n",
       "      <td>3.628833</td>\n",
       "      <td>-17.119889</td>\n",
       "      <td>2.109267</td>\n",
       "      <td>-11.436424</td>\n",
       "      <td>26.318800</td>\n",
       "      <td>623.841539</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>863.443021</td>\n",
       "      <td>16.589887</td>\n",
       "      <td>1316.282852</td>\n",
       "      <td>15.180057</td>\n",
       "      <td>1307.874474</td>\n",
       "      <td>19428.750439</td>\n",
       "      <td>0.374967</td>\n",
       "      <td>-203911.489339</td>\n",
       "      <td>47599.081146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1361.776972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>664.606866</td>\n",
       "      <td>1.468084</td>\n",
       "      <td>8.355107</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>8.528276</td>\n",
       "      <td>-27.521247</td>\n",
       "      <td>-1473.234824</td>\n",
       "      <td>14.165396</td>\n",
       "      <td>774.628129</td>\n",
       "      <td>3.269968</td>\n",
       "      <td>165.211558</td>\n",
       "      <td>1.712961</td>\n",
       "      <td>160.158577</td>\n",
       "      <td>8084.383659</td>\n",
       "      <td>1.102396</td>\n",
       "      <td>63102.747684</td>\n",
       "      <td>6365.517419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_0         xd_0  y_0        yd_0   theta_0   thetad_0     phi_0  \\\n",
       "0  0.0  1147.407904  0.0 -238.429602  1.123847  23.498330  4.482445   \n",
       "1  0.0 -1152.043723  0.0  369.845453  4.645359   5.927584  3.398032   \n",
       "2  0.0  -592.993151  0.0 -525.687613  5.220594   6.983077  4.307985   \n",
       "3  0.0  1338.965230  0.0   61.191760  3.628833 -17.119889  2.109267   \n",
       "4  0.0 -1361.776972  0.0  664.606866  1.468084   8.355107  0.008353   \n",
       "\n",
       "      phid_0        x_f         xd_f        y_f        yd_f    theta_f  \\\n",
       "0  12.691013  18.723606   691.882850  -4.739545 -293.363224   4.014081   \n",
       "1 -19.816137 -23.448395  -706.563802   8.272804  684.314462  14.631620   \n",
       "2 -23.696882  -4.641471  -608.960470  -5.348081  430.757954   9.520878   \n",
       "3 -11.436424  26.318800   623.841539   0.005384  863.443021  16.589887   \n",
       "4   8.528276 -27.521247 -1473.234824  14.165396  774.628129   3.269968   \n",
       "\n",
       "      thetad_f      phi_f       phid_f             F     alpha           tau0  \\\n",
       "0   308.066760   8.139222   345.558353  30853.584922  1.797866 -546297.092872   \n",
       "1   851.354510  11.946214   778.225185  31872.475362  2.658456  961528.396769   \n",
       "2   942.874265   8.779522   955.717081  21305.283236  2.463747 -313643.967698   \n",
       "3  1316.282852  15.180057  1307.874474  19428.750439  0.374967 -203911.489339   \n",
       "4   165.211558   1.712961   160.158577   8084.383659  1.102396   63102.747684   \n",
       "\n",
       "          tau_w  \n",
       "0  10262.953817  \n",
       "1  36965.947717  \n",
       "2  45809.941477  \n",
       "3  47599.081146  \n",
       "4   6365.517419  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainDF.shape)\n",
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_dot_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_dot_0</th>\n",
       "      <th>theta_0</th>\n",
       "      <th>theta_dot_0</th>\n",
       "      <th>phi_0</th>\n",
       "      <th>phi_dot_0</th>\n",
       "      <th>x_99</th>\n",
       "      <th>x_dot_99</th>\n",
       "      <th>y_99</th>\n",
       "      <th>y_dot_99</th>\n",
       "      <th>theta_99</th>\n",
       "      <th>theta_dot_99</th>\n",
       "      <th>phi_99</th>\n",
       "      <th>phi_dot_99</th>\n",
       "      <th>F</th>\n",
       "      <th>alpha</th>\n",
       "      <th>tau</th>\n",
       "      <th>tau_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1147.407904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-238.429602</td>\n",
       "      <td>1.123847</td>\n",
       "      <td>23.498330</td>\n",
       "      <td>4.482445</td>\n",
       "      <td>12.691013</td>\n",
       "      <td>18.723606</td>\n",
       "      <td>691.882850</td>\n",
       "      <td>-4.739545</td>\n",
       "      <td>-293.363224</td>\n",
       "      <td>4.014081</td>\n",
       "      <td>308.066760</td>\n",
       "      <td>8.139222</td>\n",
       "      <td>345.558353</td>\n",
       "      <td>30853.584922</td>\n",
       "      <td>1.797866</td>\n",
       "      <td>-546297.092872</td>\n",
       "      <td>10262.953817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1152.043723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>369.845453</td>\n",
       "      <td>4.645359</td>\n",
       "      <td>5.927584</td>\n",
       "      <td>3.398032</td>\n",
       "      <td>-19.816137</td>\n",
       "      <td>-23.448395</td>\n",
       "      <td>-706.563802</td>\n",
       "      <td>8.272804</td>\n",
       "      <td>684.314462</td>\n",
       "      <td>14.631620</td>\n",
       "      <td>851.354510</td>\n",
       "      <td>11.946214</td>\n",
       "      <td>778.225185</td>\n",
       "      <td>31872.475362</td>\n",
       "      <td>2.658456</td>\n",
       "      <td>961528.396769</td>\n",
       "      <td>36965.947717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-592.993151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-525.687613</td>\n",
       "      <td>5.220594</td>\n",
       "      <td>6.983077</td>\n",
       "      <td>4.307985</td>\n",
       "      <td>-23.696882</td>\n",
       "      <td>-4.641471</td>\n",
       "      <td>-608.960470</td>\n",
       "      <td>-5.348081</td>\n",
       "      <td>430.757954</td>\n",
       "      <td>9.520878</td>\n",
       "      <td>942.874265</td>\n",
       "      <td>8.779522</td>\n",
       "      <td>955.717081</td>\n",
       "      <td>21305.283236</td>\n",
       "      <td>2.463747</td>\n",
       "      <td>-313643.967698</td>\n",
       "      <td>45809.941477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1338.965230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.191760</td>\n",
       "      <td>3.628833</td>\n",
       "      <td>-17.119889</td>\n",
       "      <td>2.109267</td>\n",
       "      <td>-11.436424</td>\n",
       "      <td>26.318800</td>\n",
       "      <td>623.841539</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>863.443021</td>\n",
       "      <td>16.589887</td>\n",
       "      <td>1316.282852</td>\n",
       "      <td>15.180057</td>\n",
       "      <td>1307.874474</td>\n",
       "      <td>19428.750439</td>\n",
       "      <td>0.374967</td>\n",
       "      <td>-203911.489339</td>\n",
       "      <td>47599.081146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1361.776972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>664.606866</td>\n",
       "      <td>1.468084</td>\n",
       "      <td>8.355107</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>8.528276</td>\n",
       "      <td>-27.521247</td>\n",
       "      <td>-1473.234824</td>\n",
       "      <td>14.165396</td>\n",
       "      <td>774.628129</td>\n",
       "      <td>3.269968</td>\n",
       "      <td>165.211558</td>\n",
       "      <td>1.712961</td>\n",
       "      <td>160.158577</td>\n",
       "      <td>8084.383659</td>\n",
       "      <td>1.102396</td>\n",
       "      <td>63102.747684</td>\n",
       "      <td>6365.517419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_0      x_dot_0  y_0     y_dot_0   theta_0  theta_dot_0     phi_0  \\\n",
       "0  0.0  1147.407904  0.0 -238.429602  1.123847    23.498330  4.482445   \n",
       "1  0.0 -1152.043723  0.0  369.845453  4.645359     5.927584  3.398032   \n",
       "2  0.0  -592.993151  0.0 -525.687613  5.220594     6.983077  4.307985   \n",
       "3  0.0  1338.965230  0.0   61.191760  3.628833   -17.119889  2.109267   \n",
       "4  0.0 -1361.776972  0.0  664.606866  1.468084     8.355107  0.008353   \n",
       "\n",
       "   phi_dot_0       x_99     x_dot_99       y_99    y_dot_99   theta_99  \\\n",
       "0  12.691013  18.723606   691.882850  -4.739545 -293.363224   4.014081   \n",
       "1 -19.816137 -23.448395  -706.563802   8.272804  684.314462  14.631620   \n",
       "2 -23.696882  -4.641471  -608.960470  -5.348081  430.757954   9.520878   \n",
       "3 -11.436424  26.318800   623.841539   0.005384  863.443021  16.589887   \n",
       "4   8.528276 -27.521247 -1473.234824  14.165396  774.628129   3.269968   \n",
       "\n",
       "   theta_dot_99     phi_99   phi_dot_99             F     alpha  \\\n",
       "0    308.066760   8.139222   345.558353  30853.584922  1.797866   \n",
       "1    851.354510  11.946214   778.225185  31872.475362  2.658456   \n",
       "2    942.874265   8.779522   955.717081  21305.283236  2.463747   \n",
       "3   1316.282852  15.180057  1307.874474  19428.750439  0.374967   \n",
       "4    165.211558   1.712961   160.158577   8084.383659  1.102396   \n",
       "\n",
       "             tau         tau_w  \n",
       "0 -546297.092872  10262.953817  \n",
       "1  961528.396769  36965.947717  \n",
       "2 -313643.967698  45809.941477  \n",
       "3 -203911.489339  47599.081146  \n",
       "4   63102.747684   6365.517419  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns to be consistent with other code\n",
    "trainDF.rename(columns={\"x0\" : \"x_0\", \"y0\" : \"y_0\", \"phi0\" : \"phi_0\", \"theta0\" : \"theta_0\", \n",
    "                        \"x_f\" : \"x_99\", \"y_f\" : \"y_99\", \"phi_f\" : \"phi_99\", \"theta_f\" : \"theta_99\", \n",
    "                        \"xd_0\" : \"x_dot_0\", \"yd_0\" : \"y_dot_0\", \"phid_0\" : \"phi_dot_0\", \"thetad_0\": \"theta_dot_0\", \n",
    "                        \"xd_f\" : \"x_dot_99\", \"yd_f\": \"y_dot_99\", \"phid_f\": \"phi_dot_99\", \"thetad_f\": \"theta_dot_99\", \n",
    "                        \"tau0\" : \"tau\"}, inplace=True)\n",
    "\n",
    "testDF.rename(columns={\"x0\" : \"x_0\", \"y0\" : \"y_0\", \"phi0\" : \"phi_0\", \"theta0\" : \"theta_0\", \n",
    "                        \"x_f\" : \"x_99\", \"y_f\" : \"y_99\", \"phi_f\" : \"phi_99\", \"theta_f\" : \"theta_99\", \n",
    "                        \"xd_0\" : \"x_dot_0\", \"yd_0\" : \"y_dot_0\", \"phid_0\" : \"phi_dot_0\", \"thetad_0\": \"theta_dot_0\", \n",
    "                        \"xd_f\" : \"x_dot_99\", \"yd_f\": \"y_dot_99\", \"phid_f\": \"phi_dot_99\", \"thetad_f\": \"theta_dot_99\", \n",
    "                        \"tau0\" : \"tau\"}, inplace=True)\n",
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to fx and fy\n",
    "trainDF[\"Fx\"] = trainDF.F * np.cos(trainDF.alpha)\n",
    "trainDF[\"Fy\"] = trainDF.F * np.sin(trainDF.alpha)\n",
    "\n",
    "# convert to fx and fy\n",
    "testDF[\"Fx\"] = testDF.F * np.cos(testDF.alpha)\n",
    "testDF[\"Fy\"] = testDF.F * np.sin(testDF.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset\n",
    "Xtrain = trainDF.loc[:, [ \"phi_0\", \"theta_0\", \n",
    "                   \"x_dot_0\", \"y_dot_0\", \"phi_dot_0\", \"theta_dot_0\",\n",
    "                   \"x_99\", \"y_99\", \n",
    "                    \"phi_99\", \"theta_99\"]]\n",
    "\n",
    "Ytrain = trainDF.loc[:, [\"Fx\", \"Fy\", \"tau\", \"tau_w\", \"x_dot_99\", \"y_dot_99\", \"phi_dot_99\", \"theta_dot_99\",] ]\n",
    "\n",
    "# make dataset\n",
    "Xtest = testDF.loc[:, Xtrain.columns]\n",
    "\n",
    "Ytest = testDF.loc[:, Ytrain.columns ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phi_0</th>\n",
       "      <th>theta_0</th>\n",
       "      <th>x_dot_0</th>\n",
       "      <th>y_dot_0</th>\n",
       "      <th>phi_dot_0</th>\n",
       "      <th>theta_dot_0</th>\n",
       "      <th>x_99</th>\n",
       "      <th>y_99</th>\n",
       "      <th>phi_99</th>\n",
       "      <th>theta_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.482445</td>\n",
       "      <td>1.123847</td>\n",
       "      <td>1147.407904</td>\n",
       "      <td>-238.429602</td>\n",
       "      <td>12.691013</td>\n",
       "      <td>23.498330</td>\n",
       "      <td>18.723606</td>\n",
       "      <td>-4.739545</td>\n",
       "      <td>8.139222</td>\n",
       "      <td>4.014081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.398032</td>\n",
       "      <td>4.645359</td>\n",
       "      <td>-1152.043723</td>\n",
       "      <td>369.845453</td>\n",
       "      <td>-19.816137</td>\n",
       "      <td>5.927584</td>\n",
       "      <td>-23.448395</td>\n",
       "      <td>8.272804</td>\n",
       "      <td>11.946214</td>\n",
       "      <td>14.631620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.307985</td>\n",
       "      <td>5.220594</td>\n",
       "      <td>-592.993151</td>\n",
       "      <td>-525.687613</td>\n",
       "      <td>-23.696882</td>\n",
       "      <td>6.983077</td>\n",
       "      <td>-4.641471</td>\n",
       "      <td>-5.348081</td>\n",
       "      <td>8.779522</td>\n",
       "      <td>9.520878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.109267</td>\n",
       "      <td>3.628833</td>\n",
       "      <td>1338.965230</td>\n",
       "      <td>61.191760</td>\n",
       "      <td>-11.436424</td>\n",
       "      <td>-17.119889</td>\n",
       "      <td>26.318800</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>15.180057</td>\n",
       "      <td>16.589887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008353</td>\n",
       "      <td>1.468084</td>\n",
       "      <td>-1361.776972</td>\n",
       "      <td>664.606866</td>\n",
       "      <td>8.528276</td>\n",
       "      <td>8.355107</td>\n",
       "      <td>-27.521247</td>\n",
       "      <td>14.165396</td>\n",
       "      <td>1.712961</td>\n",
       "      <td>3.269968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phi_0   theta_0      x_dot_0     y_dot_0  phi_dot_0  theta_dot_0  \\\n",
       "0  4.482445  1.123847  1147.407904 -238.429602  12.691013    23.498330   \n",
       "1  3.398032  4.645359 -1152.043723  369.845453 -19.816137     5.927584   \n",
       "2  4.307985  5.220594  -592.993151 -525.687613 -23.696882     6.983077   \n",
       "3  2.109267  3.628833  1338.965230   61.191760 -11.436424   -17.119889   \n",
       "4  0.008353  1.468084 -1361.776972  664.606866   8.528276     8.355107   \n",
       "\n",
       "        x_99       y_99     phi_99   theta_99  \n",
       "0  18.723606  -4.739545   8.139222   4.014081  \n",
       "1 -23.448395   8.272804  11.946214  14.631620  \n",
       "2  -4.641471  -5.348081   8.779522   9.520878  \n",
       "3  26.318800   0.005384  15.180057  16.589887  \n",
       "4 -27.521247  14.165396   1.712961   3.269968  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fx</th>\n",
       "      <th>Fy</th>\n",
       "      <th>tau</th>\n",
       "      <th>tau_w</th>\n",
       "      <th>x_dot_99</th>\n",
       "      <th>y_dot_99</th>\n",
       "      <th>phi_dot_99</th>\n",
       "      <th>theta_dot_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6945.877556</td>\n",
       "      <td>30061.578261</td>\n",
       "      <td>-546297.092872</td>\n",
       "      <td>10262.953817</td>\n",
       "      <td>691.882850</td>\n",
       "      <td>-293.363224</td>\n",
       "      <td>345.558353</td>\n",
       "      <td>308.066760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-28224.423763</td>\n",
       "      <td>14806.640027</td>\n",
       "      <td>961528.396769</td>\n",
       "      <td>36965.947717</td>\n",
       "      <td>-706.563802</td>\n",
       "      <td>684.314462</td>\n",
       "      <td>778.225185</td>\n",
       "      <td>851.354510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-16595.229877</td>\n",
       "      <td>13360.892152</td>\n",
       "      <td>-313643.967698</td>\n",
       "      <td>45809.941477</td>\n",
       "      <td>-608.960470</td>\n",
       "      <td>430.757954</td>\n",
       "      <td>955.717081</td>\n",
       "      <td>942.874265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18078.833179</td>\n",
       "      <td>7115.626082</td>\n",
       "      <td>-203911.489339</td>\n",
       "      <td>47599.081146</td>\n",
       "      <td>623.841539</td>\n",
       "      <td>863.443021</td>\n",
       "      <td>1307.874474</td>\n",
       "      <td>1316.282852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3649.772460</td>\n",
       "      <td>7213.627391</td>\n",
       "      <td>63102.747684</td>\n",
       "      <td>6365.517419</td>\n",
       "      <td>-1473.234824</td>\n",
       "      <td>774.628129</td>\n",
       "      <td>160.158577</td>\n",
       "      <td>165.211558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Fx            Fy            tau         tau_w     x_dot_99  \\\n",
       "0  -6945.877556  30061.578261 -546297.092872  10262.953817   691.882850   \n",
       "1 -28224.423763  14806.640027  961528.396769  36965.947717  -706.563802   \n",
       "2 -16595.229877  13360.892152 -313643.967698  45809.941477  -608.960470   \n",
       "3  18078.833179   7115.626082 -203911.489339  47599.081146   623.841539   \n",
       "4   3649.772460   7213.627391   63102.747684   6365.517419 -1473.234824   \n",
       "\n",
       "     y_dot_99   phi_dot_99  theta_dot_99  \n",
       "0 -293.363224   345.558353    308.066760  \n",
       "1  684.314462   778.225185    851.354510  \n",
       "2  430.757954   955.717081    942.874265  \n",
       "3  863.443021  1307.874474   1316.282852  \n",
       "4  774.628129   160.158577    165.211558  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data \n",
    "scalerX = MinMaxScaler([-0.5, 0.5])  \n",
    "scalerY = MinMaxScaler([-0.5, 0.5])  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scalerX.fit(Xtrain)  \n",
    "scalerY.fit(Ytrain) \n",
    "\n",
    "Xtrain_scaled = scalerX.transform(Xtrain)  \n",
    "Ytrain_scaled = scalerY.transform(Ytrain)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "Xtest_scaled = scalerX.transform(Xtest)\n",
    "Ytest_scaled = scalerY.transform(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phi_0</th>\n",
       "      <th>theta_0</th>\n",
       "      <th>x_dot_0</th>\n",
       "      <th>y_dot_0</th>\n",
       "      <th>phi_dot_0</th>\n",
       "      <th>theta_dot_0</th>\n",
       "      <th>x_dot_99</th>\n",
       "      <th>y_dot_99</th>\n",
       "      <th>phi_dot_99</th>\n",
       "      <th>theta_dot_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.213403</td>\n",
       "      <td>-0.321134</td>\n",
       "      <td>0.382469</td>\n",
       "      <td>-0.079477</td>\n",
       "      <td>0.253820</td>\n",
       "      <td>0.469967</td>\n",
       "      <td>0.243012</td>\n",
       "      <td>-0.059803</td>\n",
       "      <td>0.100510</td>\n",
       "      <td>0.011596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.040814</td>\n",
       "      <td>0.239332</td>\n",
       "      <td>-0.384015</td>\n",
       "      <td>0.123282</td>\n",
       "      <td>-0.396323</td>\n",
       "      <td>0.118552</td>\n",
       "      <td>-0.305912</td>\n",
       "      <td>0.109870</td>\n",
       "      <td>0.177605</td>\n",
       "      <td>0.232007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.185637</td>\n",
       "      <td>0.330883</td>\n",
       "      <td>-0.197664</td>\n",
       "      <td>-0.175229</td>\n",
       "      <td>-0.473938</td>\n",
       "      <td>0.139662</td>\n",
       "      <td>-0.061115</td>\n",
       "      <td>-0.067737</td>\n",
       "      <td>0.113476</td>\n",
       "      <td>0.125913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.164300</td>\n",
       "      <td>0.077547</td>\n",
       "      <td>0.446322</td>\n",
       "      <td>0.020397</td>\n",
       "      <td>-0.228728</td>\n",
       "      <td>-0.342398</td>\n",
       "      <td>0.341873</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.243093</td>\n",
       "      <td>0.272659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.498671</td>\n",
       "      <td>-0.266347</td>\n",
       "      <td>-0.453926</td>\n",
       "      <td>0.221536</td>\n",
       "      <td>0.170566</td>\n",
       "      <td>0.167102</td>\n",
       "      <td>-0.358925</td>\n",
       "      <td>0.186705</td>\n",
       "      <td>-0.029628</td>\n",
       "      <td>-0.003851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phi_0   theta_0   x_dot_0   y_dot_0  phi_dot_0  theta_dot_0  x_dot_99  \\\n",
       "0  0.213403 -0.321134  0.382469 -0.079477   0.253820     0.469967  0.243012   \n",
       "1  0.040814  0.239332 -0.384015  0.123282  -0.396323     0.118552 -0.305912   \n",
       "2  0.185637  0.330883 -0.197664 -0.175229  -0.473938     0.139662 -0.061115   \n",
       "3 -0.164300  0.077547  0.446322  0.020397  -0.228728    -0.342398  0.341873   \n",
       "4 -0.498671 -0.266347 -0.453926  0.221536   0.170566     0.167102 -0.358925   \n",
       "\n",
       "   y_dot_99  phi_dot_99  theta_dot_99  \n",
       "0 -0.059803    0.100510      0.011596  \n",
       "1  0.109870    0.177605      0.232007  \n",
       "2 -0.067737    0.113476      0.125913  \n",
       "3  0.002068    0.243093      0.272659  \n",
       "4  0.186705   -0.029628     -0.003851  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Xtrain_scaled, columns = X.columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save scalers, to be used on test set\n",
    "# scalerfileX = 'scalerX_fullact.pkl'\n",
    "# pickle.dump(scalerX, open(os.path.join(dataOutput, scalerfileX), 'wb'))\n",
    "\n",
    "# scalerfileY = 'scalerY_fullact.pkl'\n",
    "# pickle.dump(scalerY, open(os.path.join(dataOutput, scalerfileY), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: start with small network and then build up\n",
    "# refref: start with large network and prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "def create_network(optimizer = 'rmsprop', \n",
    "                    numUnits = [32, 32, 32, 32], \n",
    "                    weightRegularization = 0.0, \n",
    "                    dropout_rate=0.1):\n",
    "    \n",
    "    '''\n",
    "    Create a feed forward network.  Assumes Xtrain & Ytrain have been created and scaled\n",
    "    \n",
    "    Params: \n",
    "    optimizer (str): choice of optimizer\n",
    "    numUnits (list): number of units in each hidden\n",
    "    weightRegularization (float): between 0 and 1\n",
    "    dropout_rate (float): between 0 and 1\n",
    "    \n",
    "    '''\n",
    "    K.clear_session()\n",
    "    inputs = Input(shape=(Xtrain_scaled.shape[1],))    \n",
    "    \n",
    "    # add layers\n",
    "    for ii in np.arange(0, len(numUnits)):\n",
    "        if ii >= 1: \n",
    "            x = Dense(numUnits[ii], activation='tanh', \n",
    "                      kernel_regularizer=regularizers.l1(weightRegularization))(x)\n",
    "\n",
    "        else: \n",
    "            x = Dense(numUnits[ii], activation='tanh')(inputs)\n",
    "\n",
    "\n",
    "        # add dropout\n",
    "        if dropout_rate > 0: \n",
    "            x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    predictions = Dense(Ytrain_scaled.shape[1], activation='linear')(x)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer = optimizer, metrics = ['mse'])\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt_rmsprop__Dro_0__Num_512_512_512_16__Wei_0_2019_06_11__11_27_06\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               5632      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                8208      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 539,288\n",
      "Trainable params: 539,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelParams = {\"optimizer\": \"rmsprop\", \n",
    "              \"dropout_rate\" : 0, \n",
    "               \"numUnits\": [512, 512, 512, 16],\n",
    "               \"weightRegularization\": 0\n",
    "              }\n",
    "\n",
    "\n",
    "model = create_network(**modelParams)\n",
    "\n",
    "modelName = ''.join('{}_{}__'.format(key[0:3].capitalize(), val) for  key, val in modelParams.items()).\\\n",
    "                            replace(\"[\", \"\").replace(\"]\", \"\").replace(\", \", \"_\")[0:-2] + \"_\" + datetime.now().strftime(\"%Y_%m_%d__%I_%M_%S\")\n",
    "print(modelName)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_config()\n",
    "earlystop = EarlyStopping(monitor='val_mean_squared_error', patience=50, \n",
    "                          verbose=1, mode='auto', min_delta = 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "historyDict = {\"mean_squared_error\": [], \n",
    "               \"val_mean_squared_error\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000000 samples, validate on 3000000 samples\n",
      "Epoch 1/100\n",
      " - 11s - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 2/100\n",
      " - 11s - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 3/100\n",
      " - 11s - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 4/100\n",
      " - 11s - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 5/100\n",
      " - 11s - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 6/100\n",
      " - 11s - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 7/100\n",
      " - 11s - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 8/100\n",
      " - 11s - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n",
      "Epoch 9/100\n",
      " - 11s - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 10/100\n",
      " - 11s - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 11/100\n",
      " - 11s - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 12/100\n",
      " - 11s - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 13/100\n",
      " - 11s - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 14/100\n",
      " - 11s - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 15/100\n",
      " - 11s - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 16/100\n",
      " - 11s - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 17/100\n",
      " - 11s - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 18/100\n",
      " - 11s - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 19/100\n",
      " - 11s - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 20/100\n",
      " - 11s - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 21/100\n",
      " - 11s - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 22/100\n",
      " - 11s - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 23/100\n",
      " - 11s - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 24/100\n",
      " - 11s - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 25/100\n",
      " - 11s - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 26/100\n",
      " - 11s - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 27/100\n",
      " - 11s - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 28/100\n",
      " - 11s - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 29/100\n",
      " - 11s - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 30/100\n",
      " - 11s - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 31/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 32/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 33/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 34/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 35/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 36/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 37/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 38/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 39/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 40/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 41/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 42/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 43/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 44/100\n",
      " - 11s - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 45/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 46/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 47/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 48/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 49/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 50/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 51/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 52/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 53/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 54/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 55/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 56/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 57/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 58/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 59/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 60/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 61/100\n",
      " - 11s - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 62/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 63/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 64/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 65/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 66/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 67/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 68/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 69/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 70/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 71/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 72/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 73/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 74/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 75/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 76/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 77/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 78/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 79/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 80/100\n",
      " - 11s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 81/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 82/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 83/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 84/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 85/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 86/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 87/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 88/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 89/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 90/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 91/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 92/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 93/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 94/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 95/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 96/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 97/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 98/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 99/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 100/100\n",
      " - 11s - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "1094.0973534584045\n"
     ]
    }
   ],
   "source": [
    "# # fit model without regularization\n",
    "stt = time.time()\n",
    "history = model.fit(Xtrain_scaled, Ytrain_scaled, epochs = 100, verbose = 2, \n",
    "                        batch_size=2**13, callbacks=[earlystop], validation_split = 0.3)\n",
    "winsound.PlaySound(\"*\", winsound.SND_ALIAS)\n",
    "endd = time.time() - stt\n",
    "print(endd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyDict[\"mean_squared_error\"].extend(history.history[\"mean_squared_error\"])\n",
    "historyDict[\"val_mean_squared_error\"].extend(history.history[\"val_mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(savedModels,  modelName + \"_fullActuated\" + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn4AAAFNCAYAAACe1mL+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xd8V9X9x/HXJ4skZBAII2GFvRUEQRFxC4oobtBat7Voq1Ztba11VG1/VTtcxUWdaEVFUcEBZSh7ioQVdsJKGAkJZOf8/rhfIEQCCeSbb8b7+XjcR/K999z7/Vz4gzfn3HOuOecQERERkbovKNAFiIiIiEj1UPATERERqScU/ERERETqCQU/ERERkXpCwU9ERESknlDwExEREaknFPxEpFYxsyQzc2YWUoG2N5nZ99VRl4hIbaDgJyJ+Y2YbzazAzOLL7F/qC29JganssAC5uMz+eF/NG0vtG2Rms80sy8x2m9ksMzvVd+wmMys2s5wyW2IV19vbzBaZ2X7fz95HadvYzCaY2T4z22Rm15U5fp1v/z4z+9TMGh/hGp3MLM/M3i217w9l7jHXzEoO/P36vve/ZrbTt71nZjGlzv+zmf1oZkVm9liV/MGISKUo+ImIv20ARh34YGa9gIjAlfMTDc2sZ6nP1+HVDIAvuHwBvAA0BloCjwP5pc6Z45yLKrNtraoCzSwM+Ax4F4gD3gI+8+0/kpeAAqA5cD3wbzPr4btWD+AV4Abf8f3Ay+VcY0HpHc65p0vfI/B/wHTn3E5fkyd99bUHOviu/1ipS6wFfgt8WeGbF5EqpeAnIv72DvDzUp9vBN4u3cDMYs3sbTPL8PVE/dHMgnzHgs3sWV8P0npg2BHOfcPMtpnZFjN70syCK1nfjaU+/7xMfZ0BnHPvO+eKnXO5zrlvnHPLKvEdJ+psIAT4p3Mu3zn3PGDAuWUbmllD4ErgEedcjnPue2AiXtADLwh+7pyb6ZzLAR4BrjCz6FLXGAlkAlPLK8jMzHfNt0rtbgd86pzb65zLAiYAPQ4cdM695ZybDGRX9g9ARKqGgp+I+NtcIMbMuvkC2bV4PVelvQDE4vUUnYUXvm72HbsduAToA/QDripz7ltAEdDR1+ZC4LZK1PcuMNIXMLsB0cC8UsfXAMVm9paZXWRmcZW49k+Y2TIzyyxnO1LPG3jhaZk7/B2byygVqkrpDBQ759aU2vdDqbY9fJ8BcM6tw+sd7OyrLwZ4Arj/GLdyJl6P3sel9r0EXGJmcb4/pyuByce4johUIwU/EakOB3r9LgBWAVsOHCgVBn/vnMt2zm0EnuNQD9U1eD1dqc653cBfSp3bHLgIuNc5t885lw78AxhZidrSgNXA+RyhN9I5txcYBDjgNSDDzCb6vvuA08oEuHXlfZlz7iTnXKNyttHlnBYFZJXZl4UXUivb9ljH/wy84ZxLLe8efG4EPvL1Gh6wGAgDdvm2Yo48jCwiAXLMWXEiIlXgHWAm3lDg22WOxeOFhU2l9m3Ce5YOIBFILXPsgLZAKLDNG3kEvP/QHiu0lPU2cBMwEBgMdCp90Dm30nccM+uK10v4Tw49uzjXOTeokt9ZGTlATJl9MRx5yPRYbcs97pswcj5ez2m5zCwCuBq4rMyh8Xi9iZfhDUU/i/dndc3Rrici1Uc9fiLid865TXgTJi4GPilzeCdQiBfiDmjDoV7BbUDrMscOSMWbZBFfqtcsxjl3pCHQo/kY79nB9b5aj3Yvq4A3gZ5Ha1ceM0s+wgzgA9uYck5LBk6yUukWOMm3v6w1QIiZlQ6vJ5dqm+z7fKCe9kAD33lnA0nAZjPbDjwAXFl25jNwBbAbmF5m/8nAK77e1xxgDN7fuYjUEAp+IlJdbgXOdc7tK73TOVcMfAg8ZWbRZtYW+A2HngP8EPi1mbXyPTf2UKlztwHfAM+ZWYyZBZlZBzM7qzKF+Wo6lyM8G2hmXc3sfjNr5fvcGq+nb25lvqPUd/U4wgzgA9ud5Zw2HW/Y9Ndm1sDM7vbt/1859/IJ8ISZNTSzM/B64N7xNXkPGG5mZ/omgjwBfOKcywZexZuN29u3jcGbgTukzNfcCLxd5plD8GYB32ZmEb5ewTso9TyhmYWaWTjevz0hZhZeyYk4InKCFPxEpFo459Y55xaWc/hXwD5gPfA9MA4Y6zv2GvA1XoBYzE97DH+ON1S8AtgDfAQkHEd9C30THcrKBgYA88xsH17gW87hkx9OP0Lv3amVreEotRUAI/DuNRO4BRjh239gfb3SkyhG4y2Zkw68D/zSOZfsu1YycCdeAEzHe7ZvtO/Yfufc9gMb3rBwnnMu48CFzawlXkguO2SPr64kvOcmt+BN1rmp1PHXgFy84Pyw7/cbEJFqYz/9D5uIiIiI1EXq8RMRERGpJxT8REREROoJBT8RERGRekLBT0RERKSeUPATERERqSf05o5yxMfHu6SkpECXISIiInJMixYt2umca3qsdgp+5UhKSmLhwvKWHBMRERGpOczsqG8dOkBDvSIiIiL1hIKfiIiISD2h4CciIiJST+gZPxEREanVCgsLSUtLIy8vL9Cl+F14eDitWrUiNDT0uM5X8BMREZFaLS0tjejoaJKSkjCzQJfjN845du3aRVpaGu3atTuua2ioV0RERGq1vLw8mjRpUqdDH4CZ0aRJkxPq2VTwExERkVqvroe+A070PhX8RERERKpZVFRUQL5XwU9ERESkntDkjgD5ITWTVdv3cu2pbQJdioiIiJyg3/3ud7Rt25bRo0cD8Nhjj2FmzJw5kz179lBYWMiTTz7JZZddFtA61eMXIF8lb+fhCctxzgW6FBERETlBI0eO5L///e/Bzx9++CE333wzEyZMYPHixUybNo37778/4P/uq8cvQOIiQykqceTkFxEdfnxr8YiIiEgZkx+C7T9W7TVb9IKL/nrUJn369CE9PZ2tW7eSkZFBXFwcCQkJ3HfffcycOZOgoCC2bNnCjh07aNGiRdXWVwkKfgHSKDIMgMz9hQp+IiIidcBVV13FRx99xPbt2xk5ciTvvfceGRkZLFq0iNDQUJKSkgK+yHS9Cn5mNgIYBjQDXnLOfROoWuJ8wW/P/gJaN44MVBkiIiJ1yzF65vxp5MiR3H777ezcuZMZM2bw4Ycf0qxZM0JDQ5k2bRqbNm0KWG0H+O0ZPzNrbWbTzGylmSWb2T0ncK2xZpZuZsuPcGyoma02s7Vm9tDRruOc+9Q5dztwE3Dt8dZTFeIivV6+PfsLA1mGiIiIVJEePXqQnZ1Ny5YtSUhI4Prrr2fhwoX069eP9957j65duwa6RL/2+BUB9zvnFptZNLDIzL51zq040MDMmgG5zrnsUvs6OufWlrnWm8CLwNuld5pZMPAScAGQBiwws4lAMPCXMte4xTmX7vv9j77zAubQUG9BIMsQERGRKvTjj4eeL4yPj2fOnDlHbJeTk1NdJR3Gb8HPObcN2Ob7PdvMVgItgRWlmp0F/NLMLnbO5ZnZ7cDlwMVlrjXTzJKO8DX9gbXOufUAZvYBcJlz7i/AJWUbm7fc9V+Byc65xSd4iyfkYI/fPgU/ERERqR7V8oyfL7T1AeaV3u+cG29m7YAPzGw8cAte711FtQRSS31OAwYcpf2vgPOBWF/P4pgj1DocGN6xY8dKlFF5sREa6hUREZHq5fd1/MwsCvgYuNc5t7fscefc34A84N/Apc65yvR9HumFdeUukOOce94519c5d+eRQp+vzefOuTtiY2MrUUblhQQHERMeoqFeERERqTZ+DX5mFooX+t5zzn1STpszgZ7ABODRSn5FGtC61OdWwNbjKDUg4hqGqcdPREREqo0/Z/Ua8Aaw0jn393La9AFeAy4DbgYam9mTlfiaBUAnM2tnZmHASGDiiVVefRpFhrFHPX4iIiJSTfzZ43cGcANwrpkt9W0Xl2kTCVztnFvnnCsBbgR+ssiNmb0PzAG6mFmamd0K4JwrAu4GvgZWAh8655L9d0tVKy4ylEz1+ImIiEg18ees3u858jN4pdvMKvO5EK8HsGy7UUe5xiRg0nGWGVBxkWGsTQ/MdG4RERGpOpmZmYwbN47Ro0dX6ryLL76YcePG0ahRIz9Vdji/T+6Q8jVSj5+IiEidkJmZycsvv/yT/cXFxUc9b9KkSdUW+qCevbKtpomLDCMnv4iCohLCQpTBRUREaquHHnqIdevW0bt3b0JDQ4mKiiIhIYGlS5eyYsUKRowYQWpqKnl5edxzzz3ccccdACQlJbFw4UJycnK46KKLGDRoELNnz6Zly5Z89tlnREREVGmdShsBdGAR58xcTfAQERGpzf7617/SoUMHli5dyjPPPMP8+fN56qmnWLHCe2/F2LFjWbRoEQsXLuT5559n165dP7lGSkoKd911F8nJyTRq1IiPP/64yutUj18AxTU88Nq2QppFhwe4GhERkdrv8c+TWbH1J8sGn5DuiTE8OrxHpc7p378/7dq1O/j5+eefZ8KECQCkpqaSkpJCkyZNDjunXbt29O7dG4C+ffuycePGEyv8CBT8AijO975evbZNRESkbmnYsOHB36dPn86UKVOYM2cOkZGRnH322eTl5f3knAYNGhz8PTg4mNzc3CqvS8EvgBodeF+v1vITERGpEpXtmasq0dHRZGdnH/FYVlYWcXFxREZGsmrVKubOnVvN1R2i4BdAB3v8NLNXRESkVmvSpAlnnHEGPXv2JCIigubNmx88NnToUMaMGcNJJ51Ely5dOO200wJWp4JfAB0KfurxExERqe3GjRt3xP0NGjRg8uTJRzx24Dm++Ph4li9ffnD/Aw88UOX1gWb1BlREWDANQoK0lp+IiIhUCwW/AIuLDNPkDhEREakWCn4B1igyVM/4iYiISLVQ8AuwuMgwMvWMn4iIyAlxzgW6hGpxovep4BdgcQ1DNblDRETkBISHh7Nr1646H/6cc+zatYvw8ON/6YNm9QZYo8gwTe4QERE5Aa1atSItLY2MjIxAl+J34eHhtGrV6rjPV/ALsLjIUDJzC3HOYWaBLkdERKTWCQ0NPez1aFI+DfUGWFxkGMUljr15RYEuRUREROo4Bb8Aa+RbxFkTPERERMTfFPwCLO7g+3r1nJ+IiIj4l4JfoOTnwOa5B3v8NLNXRERE/E3BL1CmPArvXE5caDGgoV4RERHxPwW/QOk2HAr3E7d9FgB79mmoV0RERPxLwS9Q2g6CyCbErP2U4CBjx968QFckIiIidZyCX6AEh0C34QSnfE3fNrHMWFP3F50UERGRwFLwC6TuI6BwHxfG72bV9mw27doX6IpERESkDlPwC6SkMyGiMUPyvwHgm+QdAS5IRERE6jIFv0AKDoFul9B60yd0axHFNyu2B7oiERERqcMU/AKt+wgoyGFI82wWbtpDRnZ+oCsSERGROkrBL9DaDYboBC7MGItzMHWlhntFRETEPxT8Ai04FC76G912TaV1RAHfrFDwExEREf9Q8KsJul+KdR/OhYXT+D4lg105Gu4VERGRqqfgV1Nc/CyjIuZQVFzMmBnrAl2NiIiI1EEKfjVFdAs6XngnVwR/x1uzN7AtKzfQFYmIiEgdo+BXk/S+nnuip+NKinnhf2sDXY2IiIjUMQp+NUloOK1Pu4JRQVP5cMFmNu7UmzxERESk6ij41TT9buHusC8JoZhHPltOUXFJoCsSERGROkLBr6aJakazky/ksdB3+S5lJ09PWhXoikRERKSOUPCriU77JSPta25us4OxszbwwfzNga5IRERE6gAFv5qoRS/odTUP77ifwU328sdPl7MlU7N8RURE5MQo+NVUI8YQcsp1/CH7KYpKHHPX7Qx0RSIiIlLLKfjVVMEhcOmLdB5wMdHsY+GqDYGuSERERGo5Bb+azIygk67ilKAUFqXmBLoaERERqeUU/Gq66Bb0DUphTaYjK7cw0NWIiIhILabgV9NFtaCfrQZg8eY9AS5GREREajMFv5ouJIzeUZkEU8KijQp+IiIicvxCAl2AHFtkbBO65+1m4abdgS5FREREajH1+NUG0Qn0Dd3I0tRMCvUKNxERETlOCn61QXQL+rlk8gpLWLF1b6CrERERkVpKwa82iE6kb8FCABZt0nN+IiIicnwU/GqD6BYk2C5axoSySDN7RURE5Dgp+NUG0QkAdGxkbN61P8DFiIiISG2l4FcbxHjBL6FBPtuy8gJcjIiIiNRWCn61ga/Hr0VINjtz8iko0sxeERERqTwFv9ogMh6CQkjAW8dvx171+omIiEjlKfjVBkFBENWCFiU7ANiu4CciIiLHQcGvtohuQUJhKoCe8xMREZHjouBXW8QkkJC/FoDtWbkBLkZERERqIwW/2iI6geh9m4hqEMLWTPX4iYiISOUp+NUW0S0gL4sWMWFs11CviIiIHAcFv9oiOhGAhEjYpskdIiIichwU/GqL6BYAtAgv0jN+IiIiclwU/GoL3yLOCaH7SM/Op7BYiziLiIhI5Sj41RYHXtsWnIlzkJGdH+CCREREpLY5avAzs2Aze7e6ipGjaBADoZG0cBmA1vITERGRyjtq8HPOFQNNzSysmuqR8ph5izgXpQFoZq+IiIhUWkgF2mwEZpnZRGDfgZ3Oub/7qygpR1w7EvYmAxeyTRM8REREpJIq8ozfVuALX9voUptUt8TexOxcQkRokIZ6RUREpNKO2ePnnHscwMyivY8ux+9VyZEl9sFcEQkNTUO9IiIiUmnH7PEzs55mtgRYDiSb2SIz6+H/0uQnEvsA0CJ0v4Z6RUREpNIqMtT7KvAb51xb51xb4H7gNf+WJUcU0xIaNiXBpavHT0RERCqtIsGvoXNu2oEPzrnpQEO/VSTlM4PEPiQUbGJHdj7FJS7QFYmIiEgtUpHgt97MHjGzJN/2R2CDvwuTciT2oUXuGopLHDtztIiziIiIVFxFgt8tQFPgE98WD9zsz6LkKBJ6k8AuALZk6jk/ERERqbijzuo1s2DgD865X1dTPXIsiX3oEpQKwNLNmZzSJi7ABYmIiEhtUZE3d/StplqkImISaBUdQvvwHGamZAS6GhEREalFKvLmjiW+t3aM5/A3d3zit6rk6BL7MHh9Mh+sjyG/qJgGIcGBrkhERERqgYo849cY2AWcCwz3bZf4syg5hsQ+nJn/HXmFJSzcuCfQ1YiIiEgtUZFn/JY55/5RTfVIRbTqy2lBzxFKMTOnTOSMJudBXNtAVyUiIiI1XEWe8bu0mmqRiupwHg2HPUXfiG3M3JQH464JdEUiIiJSC1RkqHe2mb1oZmea2SkHNr9XJuUzg/63M/is81jp2pKevg0yNwe6KhEREanhKhL8BgI9gCeA53zbs/4sSipmcKemAHxf0gvWTw9sMSIiIlLjHXNWr3PunOooRCqve0IMTRqGMbNoAFesmwan/DzQJYmIiEgNdswePzNrbmZvmNlk3+fuZnar/0uTYwkKMs7p2oxvC09m77p5UFIS6JJERESkBqvIUO+bwNdAou/zGuBefxUklXPj6UnsKwnhw5yTYcePgS5HREREarCKBL9459yHQAmAc64IKPZrVVJhvVrF0r91FP8pGkJRyrRAlyMiIiI1WEWC3z4zawI4ADM7Dcjya1VSKbee3YUtNOXrZZsCXYqIiIjUYBV5ZdtvgIlABzObBTQFrvJrVVIp53drTpvwXN7YmsSwwjwIDQ90SSIiIlIDHbPHzzm3GDgLb1mXXwA9nHPL/F2YVFxwkHHzyQ1ZXNKRRQu+C3Q5IiIiUkNVZKgX51yRcy7ZObfcOVfo76Kk8q4+/wziLId/zdgS6FJERESkhqpQ8JOaLyo6hjtbpzEzqynz1yj8iYiIyE8p+NUhP7/wNJqyh2c/X4hzLtDliIiISA1T7uSOY72P1/fsn9QgER0GcnfM2zyacSmz1u5iUKf4QJckIiIiNcjRevwOvJf3JWAe8Crwmu/35/1fmlSaGSNP60giO3lm0jL1+omIiMhhyg1+zrlzfO/p3QSc4pzr55zrC/QB1lZXgVI5DU65ll+HfMIP23KZujI90OWIiIhIDVKRZ/y6OucOvgvMObcc6O2/kuSENGrDlR0cbYN38dy3qykpUa+fiIiIeCoS/Faa2etmdraZnWVmrwEr/V2YHL/QPqO4N+gDVm7LZvLy7YEuR0RERGqIigS/m4Fk4B7gXmCFb5/UVN0v5dLwZXSKyObv366mWL1+IiIiQsXe3JEHjAEecs5d7pz7h2+f1FRhDQnucRm/4T3WZezj8x+2BroiERERqQGOGfzM7FJgKfCV73NvM5vo78KqkpmNMLPXzOwzM7sw0PVUi96jGFLyHR1iSvjP7I2BrkZERERqgIoM9T4K9AcyAZxzS4EkP9Z0GDMba2bpZra8zP6hZrbazNaa2UNHu4Zz7lPn3O3ATcC1fiy35mgzkKC4NtwQOZcfUjNZlpYZ6IpEREQkwCoS/Iqcc1l+r6R8bwJDS+8ws2C89QUvAroDo8ysu5n1MrMvymzNSp36R995dV9QEPS+jiv2jCUyNIi352wKdEUiIiISYBUJfsvN7Dog2Mw6mdkLwGw/13WQc24msLvM7v7AWufceudcAfABcJlz7kfn3CVltnTz/B8wuV69ceTkkcTYfkY028HnP2xlz76CQFckIiIiAVSR4PcroAeQD4wDsvBm9wZSSyC11Oc0377y/Ao4H7jKzO4sr5GZ3WFmC81sYUZGRtVUGkhxSdB9BDdkvkx+UQnjF6Ue8xQRERGpu44a/HxDqo875x52zp3q2/5YA2b12hH2lbtmiXPueedcX+fcnc65MUdp96rvDSX9mjZtWiWFBtzZv6db8RpOjd3Lu3M3a0FnERGReuyowc85Vwz0raZaKiMNaF3qcytAa5YcSbOucNI13JD3Ppt372dGSh3oyRQREZHjUpGh3iVmNtHMbjCzKw5sfq/s6BYAncysnZmFASOBWrXETLU663cMtTnEhxbwjiZ5iIiI1FsVCX6NgV3AucBw33aJP4sqzczeB+YAXcwszcxudc4VAXcDX+O9Pu5D51xyddVU6zTpQFifaxnFV0xblU7q7v2BrkhEREQCIORYDZxzAX09m3NuVDn7JwGTqrmc2qvfLVy3+EpetuG8O28Tv7+oW6ArEhERkWp2zOBnZuHArXgze8MP7HfO3eLHuqSqJfYhIb4xF2Rt4MMFYdx3fmfCQ4MDXZWIiIhUo4oM9b4DtACGADPwJlJk+7Mo8QMz6HU1N+R/wJ79hXp/r4iISD1UkeDX0Tn3CLDPOfcWMAzo5d+yxC96Xc3AoOV0jc7nxWlrKSgqCXRFIiIiUo0qEvwKfT8zzawnEEs1vqtXqlCTDljLU/hdxGds2rWf9+Zphq+IiEh9UpHg96qZxQGP4C2ZsgL4m1+rEv856RrOzvqUgW0ieH5qClm5hcc+R0REROqEYwY/59zrzrk9zrkZzrn2zrlmR3v7hdRwPS7HgoL4Q6Op7NlfyJgZ6wJdkYiIiFSTiszq/dOR9jvnnqj6csTvolvAaaPpOedFLm/Th7Hfb+C6/m1o3Tgy0JWJiIiIn1VkqHdfqa0YuAg941e7XfAEdLqQ36b/jhAr4ZHPluOc3uErIiJS11VkqPe5UttTwNlAS79XJv4TFAxXvkFC03h+Ezye6aszmPTj9kBXJSIiIn5WkR6/siKB9lVdiFSz8Bi48g1udJ/RIyaPxz9PZm+eJnqIiIjUZccMfmb2o5kt823JwGrgX/4vLTDMbLiZvZqVlRXoUvyvRU9C2p3B0yGvk5GTz/9NXhXoikRERMSPKtLjdwkw3LddCCQ65170a1UB5Jz73Dl3R2xsbKBLqR79b+fk/bO5tTu8N28z01alB7oiERER8ZOKBL/sUlsuEGNmjQ9sfq1O/K/LMIhpyQPFb9C1RTQPfrSMXTn5ga5KRERE/KAiwW8xkAGsAVJ8vy/ybQv9V5pUi+AQ6HcL4Run8s8LG7E3t5CHPvlRs3xFRETqoIoEv6+A4c65eOdcE7yh30+cc+2cc5rkURecciMEh9H1q1H8tvkCvl2xgzemLgt0VSIiIlLFKhL8TnXOTTrwwTk3GTjLfyVJtYtqCpe/Aom9uSXoS4YELeAvUzYx+/P/QHFRoKsTERGRKlKR4LfTzP5oZklm1tbMHgZ2+bswqWY9r4CR7xF09zyeu/s62oXt5e5Z4aSN+1WgKxMREZEqUpHgNwpoCkwAPvX9PsqfRUlgRSV25pW7L6MwOJLbVpxEVurKQJckIiIiVaAib+7Y7Zy7xznXB+gH/Mk5t9v/pUkgdWgWzcvX9mCdS+SWtxezv0BDviIiIrVdRRZwHmdmMWbWEEgGVpvZg/4vTQLtzJM68q/Oy1iSHcMv35pHQVFJoEsSERGRE1CRod7uzrm9wAhgEtAGuMGvVUmNcfGwK3g65HVmrMvkrnGLFf5ERERqsYoEv1AzC8ULfp855woBLfJWXzTvwchOjsejJvDtih2M/r9XKPj8gUBXJSIiIsehIsHvFWAj0BCYaWZtgb3+LEpqmNPv4sai8TwR8h+mZLfhF3MasW/dnEPHtyyCea8Grj4RERGpEKvsGxrMzIBg51ydftq/X79+buFCvZgEAOdg81yIS2Lc8hz+OHEVXcN388Z915DQoBBePh32psFvVkJMYqCrFRERqXfMbJFzrt+x2lWkx+8wzlOnQ5+UYQZtT4eYBK4b2Ik3Buxgc14kl/1rGks+eRb2bvHarfoysHWKiIjIUVU6+ImcM2wUH8c+T1j+bq5Z1pe3Wv8Z17gTrPoi0KWJiIjIUSj4SeWFNaTLWdfwZchvGdxgLY+mtOfuonvI3LAEcvcEujoREREpR0hFGpnZQCCpdHvn3Nt+qklqg363EJuxitf6ns0raxvx3DerWFjyFM/MmMrgoVcFujoRERE5gmNO7jCzd4AOwFKg2LfbOed+7efaAsLMhgPDO3bseHtKSkqgy6k1fkzdw31jPmFtcQuu7dea313UlcYNw7yDaQshthVEtwhskSIiInVURSd3VCT4rcRbxLlerd2nWb2VlzfxQf6+II83ii8iOjyEB4d04dqIRYRMuBWadIJfzIDQiECXKSIiUudU5aze5YC6auSYwntczB81vAXPAAAgAElEQVSC32bSmZvo0qwhD09YzpD3d/FV5KW4jNXw7aOBLlFERKReq0jwiwdWmNnXZjbxwObvwqQWShoEzbrTZd5DfLDzKl5p8DwWEsadu65hRNgYZs/5DlKmBLpKERGReqsiQ71nHWm/c26GXyqqITTUe5yKi2DTLFjxGezdQtHwl/hkdS7//HY1W7PyOTN0Fb++qDenDjwv0JWKiIjUGVX2jF99peBXtfIKi3l3ynxenrmZ3S6K/g3Tueu8Lgwe0B8LPsrk8kVveq+Eu/SFaqtVRESktqnKyR2nAS8A3YAwIBjY55yLqYpCayoFP//I3bePDz7+L6+uDGWba0yv4E2Mbr2ZC666g5D49oc3ztsL/+wJeVkwei406xaYokVERGq4qpzc8SIwCkgBIoDbfPtEKi2iYUNu/vktzPj9UP5vQAHZofH8cuOZDPr7HJ77eAapu/cfarzwDS/0WRAsHRe4okVEROqIivT4LXTO9TOzZc65k3z7ZjvnBlZLhQGiHr/qUVRcwpR5S/ngq2nMKOgMBDGoUzzX9mnOBVOG0CChO4SEw5bFcF8yHG1YWEREpJ6qaI9fRf4V3W9mYcBSM/sbsA1oeKIFigCEBAcxdOApDO2VyJa3bmX89maMTx3O3Sk7acxjXNG6Kde03U/n1ZNg/TTodEGgSxYREam1KjLUe4Ov3d3APqA1cKU/i5J6KLoFLe8Yz70DYpnpbuGt0L8yICqdN38s4MKJwVxQ8Bz/nLSElB3ZPz1371b47C548VTI3l79tYuIiNQSFZrVa2YRQBvn3Gr/l1QzaKg3gFZMhGlPwfB/sbNxHyb9uI0vps9iQVYMjiA6NYti2EkJDOscTaeU12HOS1BS5J3b62q4/N+BrV9ERKSaVeWs3uHAs0CYc66dmfUGnnDOXVo1pdZMCn41zNYlpL8ygsmRI/jSBrFgdwQOo5OlMSxhL8MuGk6nTR/A9/+AW7+F1v29WcHrp0GXiyE4NNB3ICIi4jdVGfwWAecC051zfXz7Dk70qKsU/GoY52DxW966fluXkO4aMbnRKL4MPo8F2wpwDjo2jeTsnEmcGZPOgOG3Ef7FaMjcDB3Ph6vfhAbRsCMZlrwHp90JjdqU/335OfDKYDh9NJx6W3XdpYiIyHGpyuA3zzk3wMyWKPhJjbBnI+zbBS1PATPS9+Yxefl2vlmxnQXrd1JQYoRRwICwTZyZFMngTS/SpXkM1uoUWPIOuBKIbQ0//wyadDjyd8x/DSY9AGFR8KtFEK3XVYuISM1VlcHvDWAq8BDepI5fA6HOuTurotCaSsGvdsrNL2LeuMeZmZ3Id4VdScnw1gVsZpmcHrSCfu2a0ffkXnSZehvBIaHw809/ujB0SQm81B9wXo9h9xFw5Wuwax2Mv9HrQTz/seq+NRERkXJVZfCLBB4GLgQM+Br4s3MuryoKrakU/OqGbVm5fLdmJzOSNzI/dT8Z+7xJIFFhRh+3klNKkunbIpg+J59C9IAboEEUpHwL710FV7wGO9fAzGdg6F9h5rOQuwdcMVz2MvS5PsB3JyIi4tG7ek+Qgl/d45wjbU8uizbtYeGm3Sxan87q9FxKMIwSuoTtpF/PHvTZ8RG99s2l/X3fEGK+3r+sVGjUFq77ECY/CJvnwS2ToWXfQN+WiIjIiQc/M5t4tBM1q1fqguy8QpamZrJo8UIWLfuRJSUdyHHhAISHBtEtIYaeUfvonjOHzuf8jI5JbYgt2Quvnu0tITNqHCT2CexNiIhIvVcVwS8DSAXeB+bhDfMe5JybUQV11lgKfvXQ1iUUvzeSdfsjSL7wfZbvcizfkkXy1r3k5BcdbNY8pgGdGxkdM76hc/FaOvU9j04nDSA2dRpsnuMtKJ2zA9qcDlf9B0LCjv3dznnrEbYbDAl1et6UiIj4QVUEv2DgAmAUcBLwJfC+cy65KgutqRT86qnsHV5oKxW+Skq8IeKU9GzW7MghJT2blB05rE3PJrew5GC7Zuyhc3gmHaMK6NxwH522fU7nkwcSe8U/wMwLd4X7ITTS+1za+hnw9qXQuAOMnluxsCgiIuJTpc/4mVkDvAD4DN7izS+ceIk1k2/B6uEdO3a8PSUlJdDlSA1WUuLYsmc/KUtmkLIzlzUlLVm7u4iU9Bz2FxQfbNe0QREdY4rpsG8pHQpW0TF4Ox0i80m45GGsh++JiTcvga1LoCAHLngCzrjneAqC+a96k1GueQuSBlXRnYqISE1XJcHPF/iG4YW+JGAiMNY5t6WK6qyx1OMnx6ukxLE1K5eU7XtJmfIGa7Znsa4kkbXWmuyS8IPtIsmjffNGdIotoev6N+l26rl0y/yOplu+PbR2YMF+CA6D4JCjf2nmZvh0NGz8zvs84Jdw0V/9eJciIlKTVMVQ71tAT2Ay8IFzbnnVllizKfhJlSjYD3NfgqTBuFansnNfIesycli3aTPrpr3D2uAOpBTGs60o6uAp8ZZFt8gsukdk0m3vbLq1a037G14gNCT4yN+Rm+lNNtm3E4Y+DT98AIW5cMe06rlHEREJuKoIfiXAPt/H0o0McM65mBOusgZT8BO/S54A428CYM+Zj7Ey6QZWbstmxaKZrNyew1rXigK8nr6wIEfXxEac06UZF3RvTo/EGOzAc4P//Rms+QpumgRtBsCUx2H28/BQKoRFBvAGRUSkulQ0+JU7fuScC6rakkTkMD0uh42zYNWXxJ1xCwPDYxnYIR4GtoXMTRTGtGFdeg4rP36Slem5LC4ZwfNTU/jX1BTah+zkZ13gyubbiV31BQz5ixf6ANqcBt//HbYsgnZnBvYeRUSkRtECzuVQj59Um6KCo8/izd4B/x4IRXnszA9iSvhQPswfwOK8BMLJ57ImW7hh1A30bNXIa79/N/ytHZz7CAx+wOsVnPUv6Db88HcTJ0+A5j0hvpN/709ERPyuoj1+6tUTCbRjLd0S3RyueAWimhE/9CFGPvgSnzx2G1/8rCWXty1g4t6OXPLiLEa8NIuPF6WRFxoL8V0gdZ53/vppMOVRb7bvAdnbYfzNMOP//HdfIiJS46jHrxzq8ZPaIiu3kE8Wp/HO3E2sz9hHXGQot8Ync3PWv2n4u1Uw7mpYOwUaxMADKRAaDvNegcm/hdg2cN+Pgb4FERE5QerxE6knYiNCufmMdkz9zVmMu20AfdrE8ezmzgzOeow3J3xBUcr/oO0gyN8L6/7nnbT8E+9n1mbvTSMiIlIvKPiJ1BFmxsCO8Yy96VQ+ua4NnYPSeGxBMMMK/8q8/v+EiMaQ/AlkpkLqXOgyzDvxwJCwiIjUeQp+InXQKb16Mi52DGNC/05OaBOufWcN9wT/gR0rZ8MP73uNzn8MQiIgdX4gSxURkWqk4CdSF5lhbQYwNHghU37Rk1+d25HJuxM5N+cJXp+6jOKEU6BpZ2h5CmyeG+hqRUSkmij4idRVZz4Al/yDiMSu3H9hF765dxD9Q9fzZMEorthzN2t2ZEPrAbB9mfeGERERqfMU/ETqqlZ9od8tBz8mNYtl7IAdvBD6Aqn5DRn2/He8nnUqrrgIti4OYKEiIlJdFPxE6hE792GG3/pHvr3/HM7p0ownFzjuLbyL3I16zk9EpD5Q8BOpTyIbQ9IgmkQ14JUb+vLgkC5MLBnIldPjSd2t4V4RkbpOwU+knjIz7jqnI2O7LiI1L4JLX/yeWWt3BrosERHxIwU/kXrunH69mBj2R+KLdnDDG/MY+/0G9EYfEZG6ScFPpL7rcTntRjzMhLA/cV7wUp74YgWPTUymqLgk0JWJiEgVU/ATqe/M4JSfEzV6KmNafs3tYd/w1pxN3PHOInILio/vmiu/0KvgRERqoHoV/Mysm5mNMbOPzOyXga5HpEZp3J7gUe/ycMQE/tz0f0xbnc7tby8kr7CS4S93D/z3epj7sn/qFBGR4+bX4GdmjXwha5WZrTSz04/zOmPNLN3Mlh/h2FAzW21ma83soaNdxzm30jl3J3AN0O94ahGp0xq1gUv+zg3Zr/NMzzRmrdvJne8uIr+oEuFv61Lv5671/qlRRESOm797/P4FfOWc6wqcDKwsfdDMmplZdJl9HY9wnTeBoWV3mlkw8BJwEdAdGGVm3c2sl5l9UWZr5jvnUuB7YOqJ355IHdTrKjhpJFetfYinWy9k+uoM7npvMQVFFXzmb+sS7+fudf6rUUREjovfgp+ZxQCDgTcAnHMFzrnMMs3OAj4zs3DfObcDz5e9lnNuJrD7CF/TH1jrnFvvnCsAPgAuc8796Jy7pMyW7rvWROfcQOD6KrpVkbpn2HMw4JeMyn6TP4eMZcrKdO75YEnFJnxs8/X47d4AJZogIiJSk/izx689kAH8x8yWmNnrZtawdAPn3HjgK+ADM7seuAVvGLaiWgKppT6n+fYdkZmdbWbPm9krwKRy2gw3s1ezsrIqUYZIHdMgCoY+Dfev5oZBXXgk5G0mL9/OfR/+cOzwt3UJWDAU50O2JniIiNQk/gx+IcApwL+dc32AfcBPnsFzzv0NyAP+DVzqnMupxHfYEfaVuwCZc266c+7XzrlfOOdeKqfN5865O2JjYytRhkgdFRIGg+7j1gb/4/ft1vH5D1u5e9yS8id87N8NmZuh/dne510a7hURqUn8GfzSgDTn3Dzf54/wguBhzOxMoCcwAXj0OL6jdanPrQB1MYhUpaim0OMKfrHrb/xpaAe+St7Ozf9ZQHZe4U/bHni+r+eV3s/dmuAhIlKT+C34Oee2A6lm1sW36zxgRek2ZtYHeA24DLgZaGxmT1biaxYAncysnZmFASOBiSdcvIgcrv8dUJDNLZHf8Y9rT2bBxt1c+8pctmTmHt7uQPDrchEEN9AEDxGRGsbfs3p/BbxnZsuA3sDTZY5HAlc759Y550qAG4FNZS9iZu8Dc4AuZpZmZrcCOOeKgLuBr/FmDH/onEv2292I1Fet+kLiKTD/NS7v3ZLXb+xH6u79XPbi9yzYWGre1dYl0Lg9RDaGxu28CR4H/OdimPFM9dcuIiIHhfjz4s65pRxlvTzn3KwynwvxegDLtht1lGtMopyJGiJShfrfAZ/eCc914eyo5kxo1pzbtw7nujH7ebB3EbdecwXB236A1v299o3bH3rGb/cG2DTLe/5v8APe20JERKTa1as3d4jICeh1NQz5C3QeAtEJdIwu4tOT5nFO5AaeXhrO1f/4knV7iiCxj9e+cXvY41vSZf00b19W6qHlXo5l/24YexHsUCe+iEhV8WuPn4jUIcEhcProw3bFAq/s38NnL/yGRzOGcjF/4YH0aG4pcQQ3bg9FeZC9Ddb9Dxo29cLcys8PhcOjWf4xbJ4NqyZB8x7+uScRkXpGPX4ickIsMo4Rt/6Bb6P/zOCgZTw1N5+rx8wm2SV5DXaugQ0zofNQSDrDC34VsexD72dFewhFROSYFPxE5MTFd6LZDa/z6rAm/Gtkb9bv3MewT/L4RcG9JM/7BvKyoMM50O1SLwhmrD769XZvgLT5EBR66N2/IiJywhT8RKRqtDkNO+NuLuvdkhkPnsO953VkdklPhi0bxB0F97E8/FToOsxre6xev+Ufez/73QJ702DfTv/WLiJSTyj4iUiVi40I5d4LuvB94vPcFzKeOfTiktd/5PZPt7K86cVHD37OwY/joc3p0O0Sb596/UREqoSCn4j4TWx8S+4JmcD3Z67gvvM7M2/9Li5J/Rm3bTqX+Z+/hisu+ulJO5IhYxX0ugoSTvb2bVty9C/66g/eBBIRETkqBT8R8Z/G7QGI7TKYe87vxPcPnctvzmnLQuvFNbMSueSJd/h4xmLyi0q9+3fxW2DB0H0EhMd61zhaj9/uDTD3JZjzsp9vRkSk9lPwExH/6XwhJJ0JbU4DICY8lF8P6cmcP13K06fmUVBYxP2Tt3HGk5P4xzerSZ/4GMx/FU4eCQ3jvWsk9IZty8r/jrVTvJ+bZkFRgX/vR0SkllPwExH/aX823PQFhDQ4bHdEgxCuu/JKvvntUN5p+SknFSzlX/9byxmz+/CbmL+z+OQncM55jRN7Q9Zmbw3AI0n5FjAo3O/NBBYRkXIp+IlIwFij1pw5+t+MvSCYaZEPcX37/XyT1YorXpnHOc9O559T1rAxspfXeOsRnvMrzPPWCDzpWm94eN206r0BEZFaxg7+r1oO069fP7dw4cJAlyFSf5QUQ1Aw2XmFTF6+nU+XbGHO+l04B6fYGi7vHsOwK26gccOwQ+es+x+8czlcNx5mPgOuGG7XJA8RqX/MbJFzrt+x2umVbSJSMwQFAxAdHso1/VpzTb/WbMvK5bOlW5nw7XYeSW7B4yuncHaXZlzUswXndG1G45QpENwAkgbBloVe+MvdAxFxAb4ZEZGaSUO9IlJjJcRGcOdZHfj65O+Z3PBxbm2fyfItmdw//gf6PfktV89uxStRd7I2swTX7mxwJbDhu+P7Mufgx48gN7NK70FEpCbRUG85NNQrUoPsXg9f3Afrp1MSlcjyzr9kSl43pi5ZTbJrB0CbxhGck/0lZ3eK47SRvyciLLhy35EyBd67Ek6+Di7/96H9hXmwK8V71VxwGHQbXoU3JiJSNSo61KvgVw4FP5EaaOP33nDu+ukHd239+Rympjdk+qp0Zq3eSp4LoUFIEKe1b8KZneIZ0K4J3RKiCQk+xgDH25cduu5tU6FVP8hYA/+5CPaXemXc3YsgvmOV35qIyIlQ8DtBCn4iNVjmZlj2XyjYB+c9CmYA5M0by7wv/sP0qIuZ7vqwIdNbGDqqQQh928YxoH1jBrRrTK+WjQgLKRUEt/8IYwbBmQ/AknchJhGu+y+8fr63TMzQv3rrCr59GZzzMJz120Dc9bEVF3rD3WWWzxGRuk/B7wQp+InUQiXFsHAsTP8L7N/FtkFPMT/+cuZv2M38DbtJSc8BIDw0iD6tG3FK2zj6tI6j9/KniE/5CH6TDGu+hgm/gKgWkJcJN02CVn29648d6j0DeNfcitWzejIU5UOPEX664TI+u8t7k8nNk6rn+0SkxlDwO0EKfiK1WF4WfHKHN3T76yVeDx6wa+t6Fnz3FfPXZTA/uwkrXVuKfXPcWofn0qdLB3q3iqXPD4/SPWMyDa4dC90vPXTdea/C5Adh9Fxo1u3oNezdBi/09WYr378Kwhr66WZL+Ucv2JsGv9sE4TEVO2fTHIhLgpgEv5YmIv5V0eCnWb0iUveEx3rDsyXFMONv3r6922jy3hCGrvwDf2r2PV8M2sDymHsZH/Y4fwgZR6+2zViwcTdPfLmSy9NG0rPwLS7+No7ffvQDb8/ZyKJNe8jtdAlYECz/5PDv27ECxt8M0572ZgcDTHnMGybO3wvJn/r/nvft9N5w4kpgy6KKnVOU762D+MV9/q1NRGoMreMnInVT43bQ9yZv6HfAnfD5r71nAu/8Dlp4bwOJOPcRTp33CqeGRcLAwQBsz8pjaeoelqZmkbw1iykr0/lwYRoAQQYdQp+n53eb6RG6np6x+XRf9QIxK8ZBcCgUF3hhr9ulsOwDGHQfrPwcFr8Nfa737/1uXXro99T50OGcip1TlAspX0NWGsS28l99IlIjKPiJSN01+EFY+h6MHeI9r3fVfw6GPgAiGsHZvzvslBax4QyNTWBoT2/o0znHtqw8lm/JYvnWvaxYsZw521oz4cuVvjOG0aLB+XRs1oyO+cvpOPMbOs2fTceGHWly5gPeYtLf/gnSV0Gzrv671wOvtGvUFlIr+AzigXbOeeH0nD/4pzYRqTH0jF8ZZjYcGN6xY8fbU1JSAl2OiJyoqX+G756F0++GIU+d+PX27YLnOpNRHElyq1GsaHUNa/eGsDYjh7XpOewvKD7YNC4ylI5NGtBx6xd0SGpLh8HX0LFpFC0bRRAUZCdWx8bvISTcW3YG4IPrIWMVtBvsLUT9u40H34ZSrvdHQcZqaNzem9l833Kv51JEah1N7jhBmtwhUkcU5sHab6HzRRBcRYMcK7/wniNsd+Zhu0tKHNuycklZl8La3GjWpuewLiOHtanb2VN8aImV8NAg2sdH0aFZFO3jG9K+aUOSmjSkXdOGxISXCl7OHVyq5jB7NsHLp0FUc2/yihn8vTu0HQgdz/dmJf9yNjTvUf49OAfPdIDOQ71Fqd8fCde+qwWqRWopvatXRAQgNLzqw0y3S464OyjIaBkXSct+J3N26QNrp7L7nRtZe+oTrGt2PuvSc1ibkcPS1D188cNWSv/3Oz4qzAuBIbtot2k87YMzaN8gk+a9ziPm4sexoCD48n7vWcI9G7zn+eKSYO8WSOwDrQd4F9o89+jBb9da2L/La9/pQohp5T0PWduC384Ub/Z2/9sDXYlIraDgJyLibx3OpXHngfRf8hD9b58K/XtB+kr46F7y9q0h1TVjAwlsGPQcG7KDWL9zH9M35TG+6FooAvKBWRA8axJx4dCpoD9d21xKh21f0mrGt7Tu2o+WLpTwhN5eCGzYzAuEp95afk2bfc/3tTnNGxLueyNMe8rrzSwdbAv2Vc9SNAeU18tZnu/+Dj+M83ouG7X2X10idYSGesuhoV4RqVL7dsK/z4AG0TDkafj4Nq838vS7IKYlfHwrDPkLnD4acvfAM53I7nc3G0+6j/U7c8j44Wuy1swmnUasDunC6pLW5BYWH/YVzaLDaBUXSau9S2lduJFWF95N6/RptCpKJXHY7wkLK/V//U/vgtVfwoPrISgI8nO8N5NsXwbXfQgJJ8Pk33pL19z0hTeM7G9pC73lZW75Gpp3P3Z75+C5rpCzHS57Cfr8zP81itRQesbvBCn4iUiV2zAT3roUcBDfBX72ETRq4x17ZTBYMNwxDZa+D5/eeeidwQfMewW+/wdcP56SZj1J/3EKqR8/TFpwa1JD25HW+QbS9uSSum07W/cHUcyhyR2Go0VsBK3iImgdF0nLte+RGNOAxCH3kBgbTkKjCKKK98Kbl3hDyGFRkLsbQiO9mdA3fVm5nrjj8c7lsO5/cMETcMY9x26/fTmMOcP7vdc1cOVr/q1PpLKK8r0F5aOa+f2r9IyfiEhN026wN7N481y49HlvqZcDel0N3/wRdq2DlRO9Z+5a9j38/AG/gP53gBlBQIue59Di2yxOzVkNXUfCFSd77VLnU/T6ELYHJ5J26u9J25RCatoW0hJGkZZvzFufwbbswZRkB8HY+QcvHxMeQmLMkyQW/kACOST2PofE/A0k/vgiiT9Mp3nPsw5/x3HqAu9NIT0uP/p9p0yBWf/0ehLDIo/cJm2hF/rAe5tIRYLfuqnezzYDYcOMnw4TFxXAgtch4SRIGnTs64lUtbSF8ObF8LNPoON5ga4GUPATEalep9/lbWX1vBK+ecSbYLF2qvd83pF62ErvCw6Bk66G2S9AYu9D+1v2I2TY32jV7ixaNe3s9TiMGQR75sGQJ2HLYoq+f54dV3/OtqjubMnMZVtWHlszc9mamcfW4L4sycpjz+wsoDHwJ/hgP2aTaRrVgMRGESRGOhI3fkxCyTZaXtCChHbdSGgUTnzDBocvVeMcTHkUdiyHFZ9B71FH/nOZ8TeIaAztz/Ima5SUeEPQR7N2KjTtBieP9Bbozlh9aK3ErUvh09GQngxNOsHdC/zfYylS1vZl3s+jTbSqZgp+IvL/7d15fJTVucDx35N9mySQhX0TkQqIggqKIm5FsALWDWytWGyttq693uvW21VbtbVab12qLbe4e12hFgVFAaWIln0XZIdAMkmYGSB7nvvHeZEhZAKRJBMyz/fzmc/MnLzzzjl5Z3nmnPc8x7QGmZ1dr9SnT4HWuNU/jsTg69zqIL3DehPi4g6e5ZqS5ZJXTx4F/3ctAAlp7ejS9zS6JKYQaWxoX2W1CwgXvEnBp6+z/aQfUxCfR8HuMtZsWM9HVcMpIxmmlwDzAEiKj6NjVgqds1PonJVKZ91Jp+35dIo/jZx502nffRz5mckkJ4TlGNyx2K0ecv7PwNcJVr4F/i8aTnhduRe2zHc9oMeNcGUb57jHrPknvPo9SM+Fwde65NRb5jf9eYoVIZdL0XIfmkh2LneTrXwdo12Tr1jgZ4wxrcVJV8KmjyGj44G0LIeTezzctvTw23U9DW5f5lK4JGVAep6bXNKAtKQEeudl0HvUeFj3MGy8AQZ9D3zAtj+jl/+F3ZWV7Hjnt+w4/W4KcoexvXQfBbvL2BGoYMHGEnburqKGH7jZyVuBhz9CBDr4UujWPpWuCQG6Fc2lk1xMXtbl5CfsI1/bkbtpPgn7A7/yIKRkHly5TfPcEnm9z3czmbN7uHMoT57g1h7u0A+unQYJyW6t5EXPNW3gV1MFT57pnn/s4023X9O2FCw7eLWgVsACP2OMaS36jYV374J+4w4/zPl1ZHZ2l8ZKSHbn58156ECP5ElXIiePpx3QbuVz9F91r8sHuP4Dd27dqN9Bh/5UP3M5RcPvp6Dbtyh56YcU97iYgu7fYuu2LWzduITPKtKYyjBqORte3r8M3hPIm0rOzPfJTaklv2QR+Tntye83nPzMVPJ8KeSv+jf5cd3I7zSUVHC9fqumwvu/gL1F8J1XIa29291JV7gJM6MedL2fH/3W9TBWlQMKV0xu/JfzF+9BYCssfgGG/9QFnwALp8DeQjd03/64xu2zpsol5/Z1hOSMxj02tBP+cTt86w+25nJrUV3pVtPpc2G0a3IQm9Ubgc3qNcZEhX+dG+5s7Bd/SwnugHUzYcAVB+q4axU8c667f/yFENgGm+e5IEsV7ljpeuxev96tojL6YZh2qwtwzruPyr7j8JcrhaEKCoPlFM5+mqKSAIX9J1G0+hMK91ZTVJtFEe2o5tCAOCM5gXaJlbTfu5GOUky3zp3pOmgkeb4UcjKSyN23gbzXxrkE2KUb4NMn3YSQjDzXS9hhAEz8R+POAXxpvDtxvyIIp3wXxjwGGz+GKWE5ELsNhcv/emDm9uHM/Jk7XxPcxNXMDjUAABXMSURBVJ/RD8PAq47ssTPug/l/hgt/BWfffuTt2K9iDySmHn6Zv7ZI1b0Oeg5v2h9cBUvdbP0r/hcGXNZ0+43A0rkcJQv8jDGmEcpKITnTBQ61tbDgaZj1KzjrdjjvHrdNeGDUdQhc/Qqk5xy6r/lPwox74Mop8NpEuOAXkJRO7fS7KY3LprAmnSLNpvDkmyjMHUpRqILSQJDiVbMpkA5si+tEeVXtIbtNooocguRmJJHT+ThyfcnkhlaTu+Ft8oZdS+4JQ1ygmJFMu7Qk4ncuhe0L3Wom4ek4ggXwaD/XtvIALH4ebvwEXrjCTbi5+lVYO90ll87u5vIShgfyNVWw4g3QWjjlO66sIuSW3es8CHqfBwv/DinZ8KM5h//flwfh0f4uCO11jgtiGyO0y6XFGXwtXPDzxj22Ldgw2+WwbOoAbdHzMO1muHmhOyWjmVk6F2OMMS0nPDVNXJxLRH3qRJcHcL+eZ7vAxNcJxvzJ9TDVp/sZ7nraLZCW6yZwJGcQl92dnPWzyOk0kBO7nAr5/Q7upZv3OXTrj3YbQsneSvx7KvHvqcC/p4KiVXPxr5yNP+dU/NnfwL+3gjU7QxTvzaSq5nswV2HuggNNQGlPgFypIE8mk5PlI/e4k8nN70Tujg/JrR5AbucryPWlkLPwBRL/NtIFXpNmQN4J7tJxALx4pVs7+arn3RD0qqmuVy+wxT1Rbl/oeiosfcU9/oKfu/Mx45Ngxr2uBzi3T8P/+8XPHwj6Ns93vXdH2mOs6mZE7y1ys6RjMfBb+6673vRx0wZ+O5dDYnrjh/ybmfX4RWA9fsYYEyU11fBgd6jaCyMfgGE3H/0+a2vdzN7uZx40nKeqBJZNx//Gnfj7XYc/qTP+gs34d2zCn9kPf9ZA/CV+F0DW+ignud7dZxMiNz2B3A5dyMlIJi8jmdyMJLJ2zid79YvkZKTSde9KOkoxSd0Gw5k3u5VRsrrB9TPhyTPcqi4/9HIZBgvgjyfCiLsO9JhG+l89Psid13fuXa7n6upXoe+oI/u/7O+Van8clG6Cu7e4eoTzr3Pnbp42yZ3v2ZaowuOnuLbnfQN+suCwDzlik0e5Xt3rZzbdPhtgPX7GGGOOTfEJrtdv18qG1xtujLg46HnWIcUiQvbAi8le/BTHr/mtt20iXHAHnHvbgXPeQrvQ177P3s0L8WsWxefcT1HHc1xAWFKKf8tqitN74N9bxaodQfyhCkIV1UAOcCuUHnjOpE1xJG+LIzPuEbqXrKb7o38hu2Qw6f0uwjdvI3k+Fzjmdb6E/KVTSR9xFxLp3LPVU13v4egHXVCbmOaCtCMJ/Eo3w3v3uHPbzroNXrzCnbfY+zz395IN8NHvYMXrLoBJTHO9uC2pYKkbBm+uBNz+dS7oa9fTTcTY63dpgI5Wba1bWebkCUe/ryZmgZ8xxpjW59In3XJXkYaDm5IITHjJBQAZHdwXf91JDr4OyMRpZHz4azI2fULPcy4JW4WkB3AKdVVU1xAoqyKwr4qiUMVXibL3VdZQUV1D6d5KtqzZyayiNEKMomJ5EixfFbYHl+w69efvkZeZSr4vmXbpSWQnKdmlK8jes56s4FpyMsaQn3wGeYEaUrt8k5QvPiZttBIf18BkldpamOolEh/3BKRmAwJbF7jAr7bWLaG3p9D1Tq6b6ZKLNxT4lQdh1dvQ9+KmCZ5U4c0b3ISiO1a4yUJN7Yv33PX5/+3Wy94y353TWZ+yUvjLCDjrVjj9Bw3vt3QjVIZaXSoXsMDPGGNMa9TSCW9TMt3Sbg2JT3DrCB+h5IR48n3x5PtS6NPBV/9G2xX+egGMuJvq4f9JsLyaolAFRaEKCouLKfrnAxR1OJvCnNMpDJWz1R9iefFOdtdkUc4wYJjrTfzLp94OxwMQd9/0r4ac8zPddU5GMlmpie6y7UOyviwlc8RDZNXmklWbiC//JOK3zHe72fIvFwhf9qybWZzdHabfCdsXQZfBbib3S+Mhry9842Io2w3/etwFRz1ecRNMjnaG8I5FrhcOXNB59h1Ht7/6fDHDzeo+cYxLxr35X5EDv7l/gN2bYe4jMHjiwYm7a2uhfLcLVtNzDqzYcbjXVBRY4GeMMcZES5fB8JPPoV0PEuLjaJ+eRPv0JPp29EGfXNhUA1segrN/4yazvP1TSN0A41+kvOd5BMqq8O+poDBUgT9UQfnuXVTMfoRA73EUZXaj0AsiV28ppLhcqdb9Q8Z5wL3wAfDBbK/sbnzsI+uhD8mrKiC/+k6y1vQmdeMK0uKH0F4vJefd98g5qyM5M/6b9uXxtCvaSMq6O9wcmz4XufbM/p1bgq+hcxOPxJKXXDDW6RQ303voTQcnHVd1k2J6nw++Do3ff9lu18N39u3u3MWup7s0RPUp2QCfPeOOQeEqlxR84JVQVeZmc2+eByggMPh7IHEg8W5JwVbGAj9jjDEmmhpK9THsVhecvH2Tu5+Y5pJpHzeCFCAlMZ4OmSl8tRKsdoWVqyChAi5/05XtK4H/GYzWllJGMoG4bIKJOQTGPUdAMgmWVbkh6c1LCa6aw+68SynaUMDGxOMJrt/Nvspq9lXWUF17FawH1i8GJrl9ByApXshOiSO7MJWsUCJZKXlkfrCOrO3vk9mhB1mpiWTu72lMTSQzNeGr26mJ8Uh9+ROrymH56/CNS9zw8pQxsPQlN8Fkv0+fdDOfTxwL458/dB+V+2DPzsizar+c5ZKR97nI3e8xDOb+3qXoqTus/MEvIS4BrnkDpoyFT59wicFn/Ro2fwJn/MSl7indDJ8/C7XVkN//sKvjRIMFfsYYY0xr1X0o3Lkeila78++6DnFpYiIRcQmlP3rArSoy6BoXnJQHkR/NJq1wDWnLXqHT0Jugb500MSclwrqJIBsgYR5c8/ZXEz1UleCmRZT873coJpPiXmMo7ncdgbIqdpdVEthXxe597vb2lN6sDiUTXBEgtGJdg81LjBcyU1wQ6AsPDsu2kh0aSVbcFWT7e5KZNYHsD6eR1enbZGekkFW8nNSZP0dSsmH1NChcDfle79rKt11v4cY5UF0O334GTh5/6JOvfgdS27v0OeACP62FrZ9Bn28e2G7LApeG59x73Mo3Z/7YLQs4+0EXfA65AUb99sD2p02CD38DPQ6dTNQaWDqXCCydizHGmGNSTTW8eLnL6Tf6IReknHGTW0bvcB4d4Jai83VyK67UPU/vbyMhVAA3zjt0/eRwxV/Cx3+kevV09pRXEOg5muAFDxOoUAKL3iC4/F0CpBPIPZVgt/MJBEoJFG0nWJNIML4dgWCIQE0iNUQ+TzCeGnwpSWRWFOBLTSKzY28yqwrJ3PExmcnx+PK6kVm2jczSlWQO/xG+3mfgS3G9jb7Cf5Px8jgShv0YLnrA7bByr0sjNOwWuPCXB57o75eA/wu4dTEkpbuexEf7Q1kJ5J4AN8wJm+gTPbZyx1GywM8YY8wxa28xPDPCBXHpeXDLwiObFfvGD2D5ay74GXn/oX8vD7pexbq5/iKprnRDnzPuhYHjXW/k85dC/8ugQz+Y9RuX/LusxCWtrq1xw6+AnvVT9gy/1/Uq7qsksHgqgTVz2b27hID4CA2cRCgxl+DGhYSKthLMOYVQ8XaCcVkE47LZW1lz2OqlJcXhS0n0Lgn4/EvwUUZm/5H4UhPxle3At/hpfAPH4Os/8qvtMpdNJuvzx/BNepO4rqce2f+imVngd5Qs8DPGGHNM27HYzbwd9TsYcPmRPWbhFLeSx43zGh5Sbqw5v4eP7nc5Etv3gh9+5FYXWfsufP43N6R88tUuhc+SF2DDHLj0KXfeXF27VkFNhVveDtySc38a6IZ1fZ3ghtng60h1TS17KqoJlvoJvHYLoeIdBLtfQEhTCG1ZTmjQDYSSOxEqryZUUUWovJpgcQGh0iJCSfmEapPqXfovXHyc0C4tkYzkBFKTEkhNjCMtKYHUpHjSvEtqYgLXntmDnrnpTff/rIcFfkfJAj9jjDHHPNWDl7U7nJpq8K+FDv0Pv21j6zHzZ255ue+/53r7mtKM++CzZ+G6f0K30w/9e1U5zHvMrZ9cUwFn/Lj+oW9VePUal+blogeonH4PoXPvJ3TSRBcgllcR9K5dT2QVJfsq2VPuJsCUVVVTVlnj3fauK2uYMmkIp/Zod+jzNSEL/I6SBX7GGGNME6upOjj/XVOprXXDxYdLHF38Jax5x03IiJQcfF8JPHUWhHa4YfLblrWKc/gO50gDvwhrwBhjjDHGNLHmCPrALcl3JKuF5PR2y9M1tCJMWnu47BmXvmX4fxwTQV9jWDoXY4wxxphwvYbDnetcENjGWI+fMcYYY0xdbTDoAwv8jDHGGGNiRkwFfiJyoog8LSKvi8hN0a6PMcYYY0xLavbAT0TiRWSxiLxzFPuYLCKFIrKinr+NEpG1IrJeRO5uaD+qulpVbwSuAg4788UYY4wxpi1piR6/24DV9f1BRPJFxFenrL7Vqv8OjKrn8fHAE8BooB9wtYj0E5GTROSdOpd87zFjgU+AWUfTKGOMMcaYY02zBn4i0hX4FvDXCJuMAKaKSIq3/Q+Bx+tupKpzgZJ6Hj8EWK+qG1S1EngFGKeqy1X1kjqXQm9f01R1GPDdo26gMcYYY8wxpLnTuTwG/BdQ76J+qvqaiPQCXhGR14BJwDcbsf8uwNaw+9uAoZE2FpFzgcuAZGB6hG3GAGOOP76+jkdjjDHGmGNXs/X4icglQKGqLmxoO1V9GCgHngLGquqexjxNfbts4Llmq+qtqvojVX0iwjb/UNUbsrKOYDFrY4wxxphjSHMO9Z4FjBWRTbgh2PNF5IW6G4nIcGAA8Bbwi0Y+xzYgfAXnrsCOr1VbY4wxxpg2rtkCP1W9R1W7qmpPYALwoapeE76NiAwCngXGAd8H2ovI/Y14ms+BPiLSS0SSvOeZ1iQNMMYYY4xpY6Kdxy8NuFJVv1TVWmAisLnuRiLyMjAf6Csi20TkegBVrQZuBmbgZg7/n6qubLHaG2OMMcYcQ0Q14ilxMU1EiqgnCG1iuYC/mZ+jNbP2x277Y7ntYO239sdu+2O57dC87e+hqnmH28gCvygSkX+raswmkrb2x277Y7ntYO239sdu+2O57dA62h/toV5jjDHGGNNCLPAzxhhjjIkRFvhF1zPRrkCUWftjVyy3Haz91v7YFctth1bQfjvHzxhjjDEmRliPnzHGGGNMjLDAL0pEZJSIrBWR9SJyd7Tr05xEpJuIfCQiq0VkpYjc5pX/UkS2i8gS73JxtOvaXERkk4gs99r5b6+svYi8LyLrvOt20a5ncxCRvmHHeImIBEXk9rZ8/EVksogUisiKsLJ6j7c4j3ufBctEZHD0an70IrT99yKyxmvfWyKS7ZX3FJGysNfA09GredOI0P6Ir3URucc79mtF5KLo1LrpRGj/q2Ft3yQiS7zyNnX8G/iua1XvfRvqjQIRiQe+AL6JW3buc+BqVV0V1Yo1ExHpBHRS1UUi4gMWApcCVwF7VPUPUa1gCxC3dOFpquoPK3sYKFHVB73gv52q3hWtOrYE77W/HRiKW62nTR5/ETkH2AM8p6oDvLJ6j7cXBNwCXIz7v/xJVYdGq+5HK0LbR+JWb6oWkYcAvLb3BN7Zv11bEKH9v6Se17qI9ANeBoYAnYEPgBNUtaZFK92E6mt/nb8/AgRU9ddt7fg38F13Ha3ovW89ftExBFivqhtUtRK3lvG4KNep2ahqgaou8m6HcKusdIlurVqFccAU7/YU3AdEW3cB8KWqNndy9KhS1blASZ3iSMd7HO5LUlX1UyDb+wI5JtXXdlWd6a20BPApbl31NinCsY9kHPCKqlao6kZgPe774ZjVUPtFRHA/+F9u0Uq1kAa+61rVe98Cv+joAmwNu7+NGAmEvF94g4AFXtHNXhf35LY61OlRYKaILBSRG7yyDqpaAO4DA8iPWu1azgQO/tCPleMPkY93rH0eTALeDbvfS0QWi8gcERkerUq1gPpe67F27IcDu1R1XVhZmzz+db7rWtV73wK/6JB6ytr8mLuIZABvALerahB4CugNnAIUAI9EsXrN7SxVHQyMBn7iDYfEFBFJAsYCr3lFsXT8GxIznwcich9QDbzoFRUA3VV1EPBT4CURyYxW/ZpRpNd6zBx7z9Uc/MOvTR7/er7rIm5aT1mzH38L/KJjG9At7H5XYEeU6tIiRCQR90Z4UVXfBFDVXapao6q1wLMc40McDVHVHd51IfAWrq279nfre9eF0athixgNLFLVXRBbx98T6XjHxOeBiEwELgG+q97J5d4QZ7F3eyHwJXBC9GrZPBp4rcfEsQcQkQTgMuDV/WVt8fjX911HK3vvW+AXHZ8DfUSkl9cLMgGYFuU6NRvvvI6/AatV9Y9h5eHnMnwbWFH3sW2BiKR7J/oiIunASFxbpwETvc0mAlOjU8MWc9Cv/Vg5/mEiHe9pwLXeDL8zcCe+F0Sjgs1FREYBdwFjVXVfWHmeN+EHETkO6ANsiE4tm08Dr/VpwAQRSRaRXrj2f9bS9WshFwJrVHXb/oK2dvwjfdfR2t77qmqXKFxws3i+wP3CuS/a9Wnmtp6N675eBizxLhcDzwPLvfJpuNlQUa9vM7T/OGCpd1m5/3gDOcAsYJ133T7adW3G/0EaUAxkhZW12eOPC3ALgCrcr/rrIx1v3HDPE95nwXLc7O+ot6GJ274edy7T/vf/0962l3vviaXAImBMtOvfTO2P+FoH7vOO/VpgdLTr3xzt98r/DtxYZ9s2dfwb+K5rVe99S+dijDHGGBMjbKjXGGOMMSZGWOBnjDHGGBMjLPAzxhhjjIkRFvgZY4wxxsQIC/yMMcYYY2KEBX7GGPM1iEiNiCwJu9zdhPvuKSJtPa+hMSYKEqJdAWOMOUaVqeop0a6EMcY0hvX4GWNMExKRTSLykIh85l2O98p7iMgsEVnmXXf3yjuIyFsistS7DPN2FS8iz4rIShGZKSKpUWuUMabNsMDPGGO+ntQ6Q73jw/4WVNUhwJ+Bx7yyPwPPqepA4EXgca/8cWCOqp4MDMatZABu+aonVLU/sBu3yoExxhwVW7nDGGO+BhHZo6oZ9ZRvAs5X1Q3egu07VTVHRPy4pbqqvPICVc0VkSKgq6pWhO2jJ/C+qvbx7t8FJKrq/c3fMmNMW2Y9fsYY0/Q0wu1I29SnIux2DXZOtjGmCVjgZ4wxTW982PV87/a/gAne7e8Cn3i3ZwE3AYhIvIhktlQljTGxx35BGmPM15MqIkvC7r+nqvtTuiSLyALcj+urvbJbgcki8p9AEfB9r/w24BkRuR7Xs3cTUNDstTfGxCQ7x88YY5qQd47faarqj3ZdjDGmLhvqNcYYY4yJEdbjZ4wxxhgTI6zHzxhjjDEmRljgZ4wxxhgTIyzwM8YYY4yJERb4GWOMMcbECAv8jDHGGGNihAV+xhhjjDEx4v8B4AcP/zI1i5cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bf286e5550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "# summarize history for accuracy\n",
    "axs.plot(historyDict[\"val_mean_squared_error\"], label = \"val\", color = \"C1\")\n",
    "axs.plot(historyDict[\"mean_squared_error\"], label = \"train\", color = \"C0\")\n",
    "\n",
    "axs.set_title('Model MSE = '+ str((historyDict['val_mean_squared_error'][-1]))[:8])\n",
    "axs.set_ylabel('Mean squared error')\n",
    "axs.set_xlabel('Epoch')\n",
    "axs.legend( loc='best')\n",
    "plt.yscale('log') #logarithmic scale for y axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: Can I overfit, if I turn off all regularization? With a small dataset\n",
    "## YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: visualize poossible state space from certain starting spaces using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['val_loss'][-1]# MSE for final epoch testing\n",
    "# why are val MSE vs. loss different????  answer : I think loss incorporates regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['val_mean_squared_error'][-1] # it is the same when there is no regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['loss'][-1]# MSE for final epoch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_e(n):\n",
    "    a = '%E' % n\n",
    "    return a.split('E')[0].rstrip('0').rstrip('.') + 'E' + a.split('E')[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nonzero weights\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts\n",
    "\n",
    "def plot_model_history(model_history, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history.history['mean_squared_error'])+1),\n",
    "             model_history.history['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "             model_history.history['val_mean_squared_error'])\n",
    "    axs.set_title('Model MSE = '+ str(format_e(model_history.history['val_mean_squared_error'][-1])) + \"\\n\" +  str(nzwts) + \" non-zero weights\")\n",
    "    axs.set_ylabel('mean_squared_error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "                   len(model_history.history['val_mean_squared_error'])/10)\n",
    "    axs.legend(['train', 'val'], loc='best')\n",
    "    plt.yscale('log') #logarithmic scale for y axis\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining_\" + modelName + \".png\"), dpi = 120, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "plot_model_history(history, saveFig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(os.path.join(savedModels,  modelName + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "wts = model.get_weights().copy()\n",
    "\n",
    "wtsFile = modelName + '_wts.pkl'\n",
    "pickle.dump(wts, open(os.path.join(dataOutput, wtsFile), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file back in \n",
    "wt2 = pickle.load(open(os.path.join(dataOutput, wtsFile), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END\n",
    "os.path.join(dataOutput, wtsFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## START NEW ITEM: train and trim weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and trim weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelParams = {\"optimizer\": \"rmsprop\", \n",
    "              \"dropout_rate\" : 0.0, \n",
    "               \"numUnits\": [32, 32, 32, 32],\n",
    "               \"weightRegularization\": 0\n",
    "              }\n",
    "\n",
    "\n",
    "model = create_network(**modelParams)\n",
    "wts = model.get_weights().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcolors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = model.get_weights().copy()\n",
    "\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,5, figsize=np.array([30, 10])/2, facecolor='w', edgecolor='k', sharex = \"none\", sharey = \"none\")\n",
    "fig.subplots_adjust(hspace = -0.7, wspace=0.1, bottom = -0.1 )\n",
    "axs = axs.ravel(\"F\")\n",
    "\n",
    "\n",
    "viridis = cm.get_cmap('viridis', 256)\n",
    "newcolors = viridis(np.linspace(0, 1, 256))\n",
    "white = np.array([0.8, 0.8, 0.8, 1])\n",
    "newcolors[256//2-5:256//2+5, :] = white\n",
    "newcmp = ListedColormap(newcolors)\n",
    "\n",
    "\n",
    "for jj, ii in enumerate(np.arange(0, len(wts), 1)):\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    if len(wts[ii].shape) < 2: \n",
    "        wtsTmp = wts[ii].reshape(1,-1).copy()\n",
    "    else:\n",
    "        wtsTmp = wts[ii].copy()\n",
    "    \n",
    "    im = axs[jj].matshow(wtsTmp, cmap = newcmp, norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if np.mod(ii+1, 2) == 0:\n",
    "        axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].axes.set_ylim([-1, 1])\n",
    "        axs[jj].axes.set_xlim([-1, 32])\n",
    "        \n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].tick_params(top = False, labelbottom = True, labeltop = False)\n",
    "        \n",
    "\n",
    "    \n",
    "    if (np.mod(jj, 2) == 0) and (jj != 0):\n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([])\n",
    "        axs[jj].get_yaxis().set_ticks([])\n",
    "            \n",
    "        axs[jj].set_title(str(wtsTmp.shape[0]) + \"x\" + str(wtsTmp.shape[1]) + \" matrix\", loc = \"left\")\n",
    "\n",
    "\n",
    "        #axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].axes.set_ylim([-1, 32])\n",
    "        axs[jj].axes.set_xlim([-1, 32])\n",
    "        #axs[jj].axis('off')\n",
    "        \n",
    "        \n",
    "    if ii == 9:\n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([])\n",
    "        axs[jj].get_yaxis().set_ticks([])\n",
    "\n",
    "\n",
    "        #axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([0, 5])\n",
    "        axs[jj].axes.set_xlim([-1, 32])\n",
    "        \n",
    "    if ii == 0:\n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([])\n",
    "        #axs[jj].get_yaxis().set_ticks([])\n",
    "\n",
    "\n",
    "        #axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].get_yaxis().set_ticks([0, 5])\n",
    "        axs[jj].set_title(str(wtsTmp.shape[0]) + \"x\" + str(wtsTmp.shape[1]) + \" matrix\", loc = \"left\")\n",
    "\n",
    "\n",
    "        #axs[jj].axes.set_ylim([-1, 32])\n",
    "\n",
    "    \n",
    "    #axs[jj].axis('off')\n",
    "\n",
    "cbaxes = inset_axes(axs[8], width=\"5%\", height=\"95%\", loc= \"center\") \n",
    "cbar = fig.colorbar(im, cax=cbaxes, orientation=\"vertical\", ticks = [-10, -1, 0, 1, 10])    \n",
    "    \n",
    "#fig.colorbar(im, orientation=\"horizontal\", pad=0.2)\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(os.path.join(figDir, \"RandomWeightMatrices.png\"), dpi = 500, bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\savedModels\\Opt_rmsprop__Dro_0.0__Num_32_32_32_32__Wei_0_pruned_bias.h5\")\n",
    "\n",
    "modelName = ''.join('{}_{}__'.format(key[0:3].capitalize(), val) for  key, val in modelParams.items()).replace(\"[\", \"\").replace(\"]\", \"\").replace(\", \", \"_\")[0:-2]\n",
    "print(modelName)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = model.get_weights().copy()\n",
    "\n",
    "wtsFile = modelName + '_wts.pkl'\n",
    "pickle.dump(wts, open(os.path.join(dataOutput, wtsFile), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "#wts =  pickle.load(open(os.path.join(dataOutput, wtsFile), 'rb'))\n",
    "\n",
    "# print sizes of each weight matrix\n",
    "wtLengths = []\n",
    "for ii in range(len(wts)):\n",
    "    print(wts[ii].shape)\n",
    "    wtLengths.append(np.prod(wts[ii].shape))\n",
    "\n",
    "print(np.sum(wtLengths), \"total weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "historyDict = {\"mean_squared_error\": [], \n",
    "               \"val_mean_squared_error\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCuts = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: \n",
    "# train until I have very good results\n",
    "# then prune, and retrain until results are close\n",
    "# change pruning rate as data get closer to 100% removed\n",
    "def prune_percent_updater(x):\n",
    "    logit = np.exp(x*8) / (np.exp(x*8) + 1)\n",
    "    return((logit - 0.5)*2*50)\n",
    "\n",
    "\n",
    "# cuts a smaller portion as the percent gets closer to 100%\n",
    "cutPercent = prune_percent_updater(np.linspace(0, 1, 25))\n",
    "\n",
    "while True:   \n",
    "   \n",
    "    for numEpocs in range(100):\n",
    "        \n",
    "        MSE_tmp = []\n",
    "\n",
    "        history = model.fit(Xtrain_scaled, Ytrain_scaled, validation_split = 0.3, \n",
    "                            verbose = 2, batch_size=2**14, epochs = 1, \n",
    "                            callbacks = [earlystop])\n",
    "        \n",
    "        # refref: earlystop doesn't do anything here\n",
    "        \n",
    "        # save history\n",
    "        historyDict[\"mean_squared_error\"].append(history.history[\"mean_squared_error\"][0])\n",
    "        historyDict[\"val_mean_squared_error\"].append(history.history[\"val_mean_squared_error\"][0])\n",
    "        \n",
    "        # local MSE\n",
    "        MSE_tmp.append(history.history[\"mean_squared_error\"][0])\n",
    "\n",
    "        # set weights that are close to 0 all the way back to 0, and then retrain for one epoch\n",
    "        # get nonzero weights\n",
    "        wts = model.get_weights().copy()\n",
    "\n",
    "        # set weights close to 0 to 0 (but ignore biases)\n",
    "        for ii in np.arange(0, len(wts), 1):\n",
    "            qants = np.percentile(np.reshape(wts[ii], -1), q = (50 - cutPercent[numCuts], 50 + cutPercent[numCuts]), )\n",
    "            wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "\n",
    "        # print nonzero weights\n",
    "        # calculate number of nonzero weights\n",
    "        nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "        print(nzwts, \"of\", np.sum(wtLengths), \"weights retained\")\n",
    "\n",
    "        # set new weights and calculate new loss\n",
    "        model.set_weights(wts)\n",
    "        \n",
    "        # check the change in mean squared error, and if it's not changing much, then cut out more data\n",
    "        # calculate slope of loss, based on previous 5 data points\n",
    "        if numEpocs > 5:\n",
    "            inputData = historyDict[\"mean_squared_error\"][-5:]\n",
    "\n",
    "            m = np.shape(inputData)\n",
    "            X = np.matrix([np.ones(m), np.arange(0, len(inputData))]).T\n",
    "            y = np.matrix(np.log(inputData)).T\n",
    "\n",
    "            # Solve for projection matrix\n",
    "            intercept, slope = np.array(np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)).reshape(-1,)\n",
    "            print(\"change in log loss:\", slope)\n",
    "    \n",
    "            # break if slope has stopped changing or if the overall min has been surpassed\n",
    "            # in the first training, it will automatically prune after 5 epochs, because the min will be passed\n",
    "            if (np.abs(slope) < 0.001) or (history.history[\"mean_squared_error\"][0] < np.min(historyDict[\"mean_squared_error\"][:-1])): \n",
    "                print(\"************************************************ PRUNING ********************************************************\")\n",
    "                print(\"************************************************ PRUNING ********************************************************\")\n",
    "                print(\"************************************************ PRUNING ********************************************************\")\n",
    "                break\n",
    "                       \n",
    "                    \n",
    "    ## refref: may want to save weights before each pruning, so I can go back, if I need to\n",
    "    ## refref: should I be pruning the biases too?\n",
    "    \n",
    "    ## keep running tally of min mse, and if we can't get back to the min, then break\n",
    "    print(\"Min MSE for this prune \", np.min(MSE_tmp), \"______overall Min MSE \", np.min(historyDict[\"mean_squared_error\"]))\n",
    "    if np.min(MSE_tmp) > np.min(historyDict[\"mean_squared_error\"]):\n",
    "        print(\"no more gain by pruning:  STOPPING Pruning\")\n",
    "        break\n",
    "    \n",
    "    numCuts += 1\n",
    "    if numCuts > len(cutPercent):\n",
    "        break\n",
    "\n",
    "        \n",
    "        #cutPercent += 0.2\n",
    "winsound.PlaySound(\"*\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numCuts)\n",
    "(50 - cutPercent[numCuts]) * 2 # percent of original network size that is used the pruned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numCuts = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history_fromDict(model_history_dictionary, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history_dictionary['mean_squared_error'])+1),\n",
    "             model_history_dictionary['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history_dictionary['val_mean_squared_error'])+1),\n",
    "             model_history_dictionary['val_mean_squared_error'])\n",
    "    axs.set_title('Model MSE = '+ str(format_e(model_history_dictionary['val_mean_squared_error'][-1])) + \"\\n\" +  str(nzwts) + \" non-zero weights\")\n",
    "    axs.set_ylabel('mean_squared_error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(1,len(model_history_dictionary['val_mean_squared_error'])+1),\n",
    "                   len(model_history_dictionary['val_mean_squared_error'])/10)\n",
    "    axs.legend(['train', 'val'], loc='best')\n",
    "    plt.yscale('log') #logarithmic scale for y axis\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining_\" + modelName + \"_pruned_2.png\"), dpi = 120, bbox_inches='tight')\n",
    "        print(os.path.join(figDir, \"ModelTraining_\" + modelName + \"_pruned.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "plot_model_history_fromDict(historyDict, saveFig = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = model.get_weights().copy()\n",
    "\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,5, figsize=np.array([30, 10])/2, facecolor='w', edgecolor='k', sharex = \"none\", sharey = \"none\")\n",
    "fig.subplots_adjust(hspace = -0.7, wspace=0.1, bottom = -0.1 )\n",
    "axs = axs.ravel(\"F\")\n",
    "\n",
    "\n",
    "viridis = cm.get_cmap('viridis', 256)\n",
    "newcolors = viridis(np.linspace(0, 1, 256))\n",
    "white = np.array([0.8, 0.8, 0.8, 1])\n",
    "newcolors[256//2-5:256//2+5, :] = white\n",
    "newcmp = ListedColormap(newcolors)\n",
    "\n",
    "for jj, ii in enumerate(np.arange(0, len(wts), 1)):\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    if len(wts[ii].shape) < 2: \n",
    "        wtsTmp = wts[ii].reshape(1,-1).copy()\n",
    "    else:\n",
    "        wtsTmp = wts[ii].copy()\n",
    "    \n",
    "    im = axs[jj].matshow(wtsTmp, cmap = newcmp, norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if np.mod(ii+1, 2) == 0:\n",
    "        axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].axes.set_ylim([-1, 1])\n",
    "        axs[jj].axes.set_xlim([-1, 32])\n",
    "        \n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].tick_params(top = False, labelbottom = True, labeltop = False)\n",
    "        \n",
    "\n",
    "    \n",
    "    if (np.mod(jj, 2) == 0) and (jj != 0):\n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([])\n",
    "        axs[jj].get_yaxis().set_ticks([])\n",
    "            \n",
    "        axs[jj].set_title(str(wtsTmp.shape[0]) + \"x\" + str(wtsTmp.shape[1]) + \" matrix\", loc = \"left\")\n",
    "\n",
    "\n",
    "        #axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].axes.set_ylim([-1, 32])\n",
    "        axs[jj].axes.set_xlim([-1, 32])\n",
    "        #axs[jj].axis('off')\n",
    "        \n",
    "        \n",
    "    if ii == 9:\n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([])\n",
    "        axs[jj].get_yaxis().set_ticks([])\n",
    "\n",
    "\n",
    "        #axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([0, 5])\n",
    "        axs[jj].axes.set_xlim([-1, 32])\n",
    "        \n",
    "    if ii == 0:\n",
    "        axs[jj].spines['top'].set_visible(False)\n",
    "        axs[jj].spines['right'].set_visible(False)\n",
    "        axs[jj].spines['bottom'].set_visible(False)\n",
    "        axs[jj].spines['left'].set_visible(False)\n",
    "        axs[jj].get_xaxis().set_ticks([])\n",
    "        #axs[jj].get_yaxis().set_ticks([])\n",
    "\n",
    "\n",
    "        #axs[jj].axes.get_yaxis().set_visible(False)\n",
    "        axs[jj].get_yaxis().set_ticks([0, 5])\n",
    "        axs[jj].set_title(str(wtsTmp.shape[0]) + \"x\" + str(wtsTmp.shape[1]) + \" matrix\", loc = \"left\")\n",
    "\n",
    "\n",
    "        #axs[jj].axes.set_ylim([-1, 32])\n",
    "\n",
    "    \n",
    "    #axs[jj].axis('off')\n",
    "\n",
    "cbaxes = inset_axes(axs[8], width=\"5%\", height=\"95%\", loc= \"center\") \n",
    "cbar = fig.colorbar(im, cax=cbaxes, orientation=\"vertical\", ticks = [-10, -1, 0, 1, 10])    \n",
    "    \n",
    "#fig.colorbar(im, orientation=\"horizontal\", pad=0.2)\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(os.path.join(figDir, \"Pruned_WeightMatrices.png\"), dpi = 700, bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how good can I get without trimming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot matrices\n",
    "\n",
    "wts = model.get_weights().copy()\n",
    "for ii in np.arange(0, len(wts), 2):\n",
    "    plt.matshow(wts[ii], cmap = \"PRGn\", norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot biases\n",
    "\n",
    "for ii in np.arange(1, len(wts), 2):\n",
    "    plt.matshow(wts[ii].reshape(1, -1), cmap = \"PRGn\", norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: save weights and model\n",
    "model.save(os.path.join(savedModels,  modelName + '_pruned_bias.h5'))\n",
    "\n",
    "# save weights\n",
    "wts = model.get_weights().copy()\n",
    "\n",
    "wtsFile = modelName + '_pruned_wts_bias.pkl'\n",
    "pickle.dump(wts, open(os.path.join(dataOutput, wtsFile), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(1,5, figsize=(20, 5), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.2)\n",
    "\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "for jj, ii in enumerate(np.arange(0, len(wts), 2)):\n",
    "    im = axs[jj].matshow(wts[ii], cmap = \"PRGn\", norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    \n",
    "    \n",
    "plt.colorbar(im,ax=axs[jj], orientation = \"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(5,1, figsize=(30, 10), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.5)\n",
    "\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "for jj, ii in enumerate(np.arange(1, len(wts), 2)):\n",
    "    im = axs[jj].matshow(wts[ii].reshape(1, -1), cmap = \"PRGn\", norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    axs[jj].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.colorbar(im,ax=axs[jj], orientation = \"horizontal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,5, figsize=(30, 10), facecolor='w', edgecolor='k', )\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.1)\n",
    "\n",
    "\n",
    "axs = axs.ravel(\"F\")\n",
    "\n",
    "for jj, ii in enumerate(np.arange(0, len(wts), 1)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(wts[ii].shape) < 2: \n",
    "        wtsTmp = wts[ii].reshape(1,-1).copy()\n",
    "    else:\n",
    "        wtsTmp = wts[ii].copy()\n",
    "    \n",
    "    im = axs[jj].matshow(wtsTmp, cmap = \"PRGn\", norm=colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                              vmin=-3.0, vmax=3.0))\n",
    "    if np.mod(ii+1, 2) == 0:\n",
    "        axs[jj].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.colorbar(im,ax=axs[jj], orientation = \"horizontal\")\n",
    "plt.savefig(os.path.join(figDir, \"PrunedWeightMatrices.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(dataOutput, wtsFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: if whole node is basically 0, then remove the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(wts[2].reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_network(**modelParams)\n",
    "\n",
    "history = model.fit(Xtrain_scaled, Ytrain_scaled, validation_split = 0.3, \n",
    "                        verbose = 2, batch_size=2**14, epochs = 1, \n",
    "                        callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if model saved: \n",
    "K.clear_session()\n",
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(savedModels, 'my_model_400Units_newData.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nonzero weights\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,8, figsize=np.array((20,3)) , facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    axs[jj].hist(wts[jj].reshape(-1), bins = 100)\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(wts[jj].shape))\n",
    "    axs[jj+1].hist(wts[jj+1], bins = 100)\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(wts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(Xtest_scaled, Ytest_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nonzero weights\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim weights\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (5, 95), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "    \n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new weights and calculate new loss\n",
    "model.set_weights(wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.05*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.05*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# this is the original model\n",
    "inputs = Input(shape=(Xtrain_scaled.shape[1],))\n",
    "x = Dense(400, activation='tanh')(inputs)\n",
    "x = Dense(400, activation='tanh')(x)\n",
    "x = Dense(400, activation='tanh')(x)\n",
    "x = Dense(16, activation='tanh')(x)\n",
    "predictions = Dense(Ytrain_scaled.shape[1], activation='linear')(x)\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics = ['mse'])\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_mean_squared_error', patience=50, \n",
    "                          verbose=1, mode='auto', min_delta = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "print(nzwts)\n",
    "\n",
    "# trim weights\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (2, 98), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "    \n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "print(nzwts)\n",
    "model.set_weights(wts)\n",
    "\n",
    "\n",
    "# start training\n",
    "history = model.fit(Xtrain_scaled, Ytrain_scaled, validation_split = 0.3, \n",
    "                    verbose = 2, batch_size=2**14, epochs = 1, \n",
    "                    callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = model.get_weights().copy()\n",
    "\n",
    "# trim weights\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (2.5, 97.5), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "\n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "print(nzwts)\n",
    "model.set_weights(wts)\n",
    "\n",
    "model.evaluate(Xtest_scaled, Ytest_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history.history['mean_squared_error'])+1),\n",
    "             model_history.history['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "             model_history.history['val_mean_squared_error'])\n",
    "    axs.set_title('Model MSE')\n",
    "    axs.set_ylabel('mean_squared_error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "                   len(model_history.history['val_mean_squared_error'])/10)\n",
    "    axs.legend(['train', 'val'], loc='best')\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining.png\"), dpi = 120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "plot_model_history(history)\n",
    "print(history.history[\"loss\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model that was trained for much longer\n",
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(savedModels, 'my_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nonzero weights\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new weights and calculate new loss\n",
    "model.set_weights(wts)\n",
    "\n",
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.05*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.05*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = model.get_weights().copy()\n",
    "nzwts = [np.nonzero(wts[ii].reshape(-1))[0] for ii in range(len(wts))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,8, figsize=np.array((20,5)) , facecolor='w', edgecolor='k')\n",
    "#fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    axs[jj].hist(nzwts[jj].reshape(-1), bins = 30)\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(nzwts[jj].shape))\n",
    "    axs[jj+1].hist(nzwts[jj+1], bins = 30)\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(nzwts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,8, figsize=np.array((10,10)) , facecolor='w', edgecolor='k')\n",
    "#fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    axs[jj].hist(wts[jj].reshape(-1), bins = 30)\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(wts[jj].shape))\n",
    "    axs[jj+1].hist(wts[jj+1], bins = 30)\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(wts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "nnpreds = model.predict(Xtest_scaled[ :])\n",
    "\n",
    "# rescale\n",
    "nnpreds_unscaled = scalerY.inverse_transform(nnpreds)\n",
    "\n",
    "# show residuals\n",
    "# combine residual and regular plots\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,7, figsize=np.array((30, 8)) / 1.7, facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.5)\n",
    "axs = axs.ravel()\n",
    "\n",
    "# replace lightest colors with white\n",
    "import matplotlib.colors\n",
    "cmap = plt.cm.magma_r\n",
    "cmaplist = np.array([cmap(i) for i in range(cmap.N)])\n",
    "cmaplist[:,0:3] = np.divide(cmaplist[:, 0:3], 1.1)\n",
    "cmaplist[0] = (1,1,1,0.5)\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('mcm',cmaplist, cmap.N)\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    \n",
    "    try:\n",
    "        axs[ii].hexbin(y = Ytest.iloc[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[ii].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[ii].ticklabel_format(style='sci',  axis='y', scilimits=(3,4))\n",
    "        axs[ii].axes.xaxis.set_ticklabels([])\n",
    "        if(ii == 0):\n",
    "            axs[ii].set_ylabel(\"Actual Value\")\n",
    "        axs[ii].set_title(nms2[ii])\n",
    "        axs[ii].plot(Ytest.iloc[0:1000,ii], Ytest.iloc[0:1000,ii], 'grey', linewidth = 1, linestyle  = \"--\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    jj = ii + len(Y.columns)\n",
    "    \n",
    "    try:\n",
    "        axs[jj].hexbin(y = Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[jj].set_xlabel(\"Predicted Value\")\n",
    "        axs[jj].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[jj].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        mmin = np.min(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        mmax = np.max(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        \n",
    "        upper = np.max([np.abs(mmin), np.abs(mmax)])\n",
    "        axs[jj].set_ylim(-upper, upper)\n",
    "\n",
    "        if(ii == 0):\n",
    "            axs[jj].set_ylabel(\"Actual - Predicted\")\n",
    "        axs[jj].hlines(y = 0, xmin = np.min(nnpreds_unscaled[:,ii]), \n",
    "                       xmax = np.max(nnpreds_unscaled[:,ii]), linestyle =  \"--\", linewidth = 1)\n",
    "    except:\n",
    "        pass\n",
    "plt.tight_layout()\n",
    "#fig.savefig(os.path.join(figDir, \"SmallModelResids.png\"), dpi = 120, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim distribution of weights -- cut out middle 20%\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (40, 60), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "    \n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show new histogram of weights (excluding the 0's)\n",
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,5, figsize=np.array((15, 6)) , facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    \n",
    "    d1 = wts[jj].reshape(-1)\n",
    "    axs[jj].hist(d1[d1!=0], bins = 30, facecolor = '#d6bddb' )\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(wts[jj].shape))\n",
    "\n",
    "    d2 = wts[jj+1]\n",
    "    axs[jj+1].hist(d2[d2!=0], bins = 30, facecolor = '#d6bddb')\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(wts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the validation.split is the last X% of the data\n",
    "int(0.3*Xtrain_scaled.shape[0])\n",
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new weights and calculate new loss\n",
    "model.set_weights(wts)\n",
    "\n",
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "nnpreds = model.predict(Xtest_scaled[ :])\n",
    "\n",
    "# rescale\n",
    "nnpreds_unscaled = scalerY.inverse_transform(nnpreds)\n",
    "\n",
    "# show residuals\n",
    "# combine residual and regular plots\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,7, figsize=np.array((30, 8)) / 1.7, facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.5)\n",
    "axs = axs.ravel()\n",
    "\n",
    "# replace lightest colors with white\n",
    "import matplotlib.colors\n",
    "cmap = plt.cm.magma_r\n",
    "cmaplist = np.array([cmap(i) for i in range(cmap.N)])\n",
    "cmaplist[:,0:3] = np.divide(cmaplist[:, 0:3], 1.1)\n",
    "cmaplist[0] = (1,1,1,0.5)\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('mcm',cmaplist, cmap.N)\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    \n",
    "    try:\n",
    "        axs[ii].hexbin(y = Ytest.iloc[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[ii].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[ii].ticklabel_format(style='sci',  axis='y', scilimits=(3,4))\n",
    "        axs[ii].axes.xaxis.set_ticklabels([])\n",
    "        if(ii == 0):\n",
    "            axs[ii].set_ylabel(\"Actual Value\")\n",
    "        axs[ii].set_title(nms2[ii])\n",
    "        axs[ii].plot(Ytest.iloc[0:1000,ii], Ytest.iloc[0:1000,ii], 'grey', linewidth = 1, linestyle  = \"--\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    jj = ii + len(Y.columns)\n",
    "    \n",
    "    try:\n",
    "        axs[jj].hexbin(y = Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[jj].set_xlabel(\"Predicted Value\")\n",
    "        axs[jj].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[jj].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        mmin = np.min(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        mmax = np.max(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        \n",
    "        upper = np.max([np.abs(mmin), np.abs(mmax)])\n",
    "        axs[jj].set_ylim(-upper, upper)\n",
    "\n",
    "        if(ii == 0):\n",
    "            axs[jj].set_ylabel(\"Actual - Predicted\")\n",
    "        axs[jj].hlines(y = 0, xmin = np.min(nnpreds_unscaled[:,ii]), \n",
    "                       xmax = np.max(nnpreds_unscaled[:,ii]), linestyle =  \"--\", linewidth = 1)\n",
    "    except:\n",
    "        pass\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of hyperparameters\n",
    "\n",
    "\n",
    "# regularization, num layers, num nodes, learning rate, optimizer, activation function, batch size\n",
    "\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Create hyperparameter space\n",
    "NumHiddenLayers = randint(low = 2, high = 20)#[4, 2, 8]\n",
    "numUnits  = [2**4, 2**5, 2**6, 2**7, 2**8, 2**9, 2**10]\n",
    "epochs = [200]\n",
    "batches1 = [2**12, 2**10, 2**8, 2**14] \n",
    "optimizers = ['rmsprop', 'adam']\n",
    "dropout_rate =  uniform(loc = 0, scale = 0.5) #[0.0, 0.2, 0.5]\n",
    "weightRegularization = uniform(loc = 0, scale = 0.001) #[0, 0.0001, 0.001, 0.01]\n",
    "secondToLastUnits = [8, 16, 32, 64]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(optimizer=optimizers, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batches1,\n",
    "                        dropout_rate = dropout_rate, \n",
    "                        numUnits = numUnits, \n",
    "                        NumHiddenLayers = NumHiddenLayers, \n",
    "                        weightRegularization = weightRegularization, \n",
    "                        secondToLastUnits = secondToLastUnits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: \n",
    "# train until I have very good results\n",
    "# then prune, and retrain until results are close\n",
    "# change pruning rate as data get closer to 100% removed\n",
    "cutPercent = 49.7\n",
    "for numCuts in range(3):\n",
    "    for numEpocs in range(100):\n",
    "\n",
    "        history = model.fit(Xtrain_scaled, Ytrain_scaled, validation_split = 0.3, \n",
    "                            verbose = 2, batch_size=2**14, epochs = 1, \n",
    "                            callbacks = [earlystop])\n",
    "        # save history\n",
    "        historyDict[\"mean_squared_error\"].append(history.history[\"mean_squared_error\"][0])\n",
    "        historyDict[\"val_mean_squared_error\"].append(history.history[\"val_mean_squared_error\"][0])\n",
    "\n",
    "        # set weights that are close to 0 all the way back to 0, and then retrain for one epoch\n",
    "        # get nonzero weights\n",
    "        wts = model.get_weights().copy()\n",
    "\n",
    "        # set weights close to 0 to 0 (but ignore biases)\n",
    "        for ii in np.arange(0, len(wts), 2):\n",
    "            qants = np.percentile(np.reshape(wts[ii], -1), q = (50 - cutPercent, 50 + cutPercent), )\n",
    "            wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "\n",
    "        # print nonzero weights\n",
    "        # calculate number of nonzero weights\n",
    "        nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "        print(nzwts, \"of\", np.sum(wtLengths), \"weights retained\")\n",
    "\n",
    "        # set new weights and calculate new loss\n",
    "        model.set_weights(wts)\n",
    "        #cutPercent += 0.2\n",
    "winsound.PlaySound(\"*\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
