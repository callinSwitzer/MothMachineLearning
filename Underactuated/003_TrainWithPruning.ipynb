{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callin Switzer\n",
    "___\n",
    "\n",
    "### 16 Jan 2020\n",
    "### - Train and prune a Dense, Feedforward Neural Network with Keras\n",
    "### - Use data that was generated in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calli\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow successfully installed.\n",
      "The installed version of TensorFlow includes GPU support.\n",
      "3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)] \n",
      "\n",
      "last run on 2020-01-16 15:48:12.867818\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "import subprocess\n",
    "import winsound\n",
    "import pickle\n",
    "import glob\n",
    "import sqlite3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow successfully installed.\")\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"The installed version of TensorFlow includes GPU support.\")\n",
    "print(sys.version, \"\\n\")\n",
    "now = datetime.now()\n",
    "print(\"last run on \" + str(now))\n",
    "\n",
    "# define directories\n",
    "baseDir = os.getcwd()\n",
    "dataDir = r'D:\\MothSimulations\\11c-AggressiveManeuver\\Qstore\\hws_am_con'\n",
    "figDir = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\Figs'\n",
    "dataOutput = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\DataOutput'\n",
    "savedModels = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\savedModels'\n",
    "dataDir = r'D:/Dropbox/AcademiaDropbox/mothMachineLearning_dataAndFigs/PythonGeneratedData_oneTorque/'\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "\n",
    "if not os.path.exists(dataOutput):\n",
    "    os.mkdir(dataOutput)\n",
    "if not os.path.exists(savedModels):\n",
    "    os.mkdir(savedModels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "# Keras callcacks\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "# get table names in database\n",
    "con1 = sqlite3.connect(os.path.join(dataDir, \"oneTorqueData_v1.db\"))\n",
    "cursorObj = con1.cursor()\n",
    "res = cursorObj.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tableNames = [name[0] for name in res]\n",
    "con1.close()\n",
    "print(tableNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "con1 = sqlite3.connect(os.path.join(dataDir, \"oneTorqueData_v1.db\"))\n",
    "trainDF = pd.read_sql_query(\"SELECT * FROM train\", con1)\n",
    "testDF = pd.read_sql_query(\"SELECT * FROM test\", con1)\n",
    "con1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.drop(['tau_w'], axis=1, inplace=True)\n",
    "testDF.drop(['tau_w'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check for repeats!\n",
    "np.sum(trainDF.iloc[:, [16,17,18]].duplicated()) # 0 means no repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainDF.shape)\n",
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to be consistent with other code\n",
    "trainDF.rename(columns={\"x0\" : \"x_0\", \"y0\" : \"y_0\", \"phi0\" : \"phi_0\", \"theta0\" : \"theta_0\", \n",
    "                        \"x_f\" : \"x_99\", \"y_f\" : \"y_99\", \"phi_f\" : \"phi_99\", \"theta_f\" : \"theta_99\", \n",
    "                        \"xd_0\" : \"x_dot_0\", \"yd_0\" : \"y_dot_0\", \"phid_0\" : \"phi_dot_0\", \"thetad_0\": \"theta_dot_0\", \n",
    "                        \"xd_f\" : \"x_dot_99\", \"yd_f\": \"y_dot_99\", \"phid_f\": \"phi_dot_99\", \"thetad_f\": \"theta_dot_99\", \n",
    "                        \"tau0\" : \"tau\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert angles to sine and cosine\n",
    "# cosx = np.cos(angle)\n",
    "# sinx = np.sin(angle)\n",
    "\n",
    "# # print(angle, np.arctan2(sinx, cosx))\n",
    "\n",
    "# trainDF[\"cos_phi_0\"] = np.cos(trainDF.phi_0)\n",
    "# trainDF[\"sin_phi_0\"] = np.sin(trainDF.phi_0)\n",
    "# trainDF[\"cos_phi_99\"] = np.cos(trainDF.phi_99)\n",
    "# trainDF[\"sin_phi_99\"] = np.sin(trainDF.phi_99)\n",
    "\n",
    "# trainDF[\"sin_theta_0\"] = np.sin(trainDF.theta_0)\n",
    "# trainDF[\"cos_theta_0\"] = np.cos(trainDF.theta_0)\n",
    "# trainDF[\"sin_theta_99\"] = np.sin(trainDF.theta_99)\n",
    "# trainDF[\"cos_theta_99\"] = np.cos(trainDF.theta_99)\n",
    "\n",
    "# convert to fx and fy\n",
    "trainDF[\"Fx\"] = trainDF.F * np.cos(trainDF.alpha)\n",
    "trainDF[\"Fy\"] = trainDF.F * np.sin(trainDF.alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x_0', 'x_dot_0', 'y_0', 'y_dot_0', 'theta_0', 'theta_dot_0', 'phi_0',\n",
       "       'phi_dot_0', 'x_99', 'x_dot_99', 'y_99', 'y_dot_99', 'theta_99',\n",
       "       'theta_dot_99', 'phi_99', 'phi_dot_99', 'F', 'alpha', 'tau', 'Fx',\n",
       "       'Fy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset\n",
    "X = trainDF.loc[:, [ \"phi_0\", \"theta_0\", \n",
    "                    \"x_99\", \"y_99\", \"phi_99\", \"theta_99\", \n",
    "                   \"x_dot_0\", \"y_dot_0\", \"phi_dot_0\", \"theta_dot_0\"]]\n",
    "\n",
    "Y = trainDF.loc[:, [\"Fx\", \"Fy\", \"tau\", \"x_dot_99\", \"y_dot_99\", \n",
    "                    \"phi_dot_99\", \"theta_dot_99\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phi_0</th>\n",
       "      <th>theta_0</th>\n",
       "      <th>x_99</th>\n",
       "      <th>y_99</th>\n",
       "      <th>phi_99</th>\n",
       "      <th>theta_99</th>\n",
       "      <th>x_dot_0</th>\n",
       "      <th>y_dot_0</th>\n",
       "      <th>phi_dot_0</th>\n",
       "      <th>theta_dot_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.791105</td>\n",
       "      <td>3.973063</td>\n",
       "      <td>-9.057837</td>\n",
       "      <td>3.781440</td>\n",
       "      <td>6.330451</td>\n",
       "      <td>3.560519</td>\n",
       "      <td>-744.410126</td>\n",
       "      <td>403.629247</td>\n",
       "      <td>17.099313</td>\n",
       "      <td>1.765834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.513194</td>\n",
       "      <td>4.864854</td>\n",
       "      <td>-23.398718</td>\n",
       "      <td>-28.787408</td>\n",
       "      <td>3.079297</td>\n",
       "      <td>5.287969</td>\n",
       "      <td>-1173.951346</td>\n",
       "      <td>-1421.176764</td>\n",
       "      <td>-9.854699</td>\n",
       "      <td>19.628124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.049398</td>\n",
       "      <td>4.360883</td>\n",
       "      <td>-20.525825</td>\n",
       "      <td>27.693859</td>\n",
       "      <td>0.109830</td>\n",
       "      <td>3.563285</td>\n",
       "      <td>-952.496493</td>\n",
       "      <td>1246.648094</td>\n",
       "      <td>-23.807232</td>\n",
       "      <td>0.234332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.669289</td>\n",
       "      <td>6.272703</td>\n",
       "      <td>-5.645072</td>\n",
       "      <td>16.358706</td>\n",
       "      <td>0.521193</td>\n",
       "      <td>5.765953</td>\n",
       "      <td>-172.519574</td>\n",
       "      <td>834.968221</td>\n",
       "      <td>-10.130481</td>\n",
       "      <td>7.075612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.279002</td>\n",
       "      <td>2.762207</td>\n",
       "      <td>24.306555</td>\n",
       "      <td>0.368850</td>\n",
       "      <td>5.523865</td>\n",
       "      <td>2.742183</td>\n",
       "      <td>1203.275577</td>\n",
       "      <td>-28.441808</td>\n",
       "      <td>-9.457624</td>\n",
       "      <td>-23.078939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phi_0   theta_0       x_99       y_99    phi_99  theta_99      x_dot_0  \\\n",
       "0  5.791105  3.973063  -9.057837   3.781440  6.330451  3.560519  -744.410126   \n",
       "1  3.513194  4.864854 -23.398718 -28.787408  3.079297  5.287969 -1173.951346   \n",
       "2  1.049398  4.360883 -20.525825  27.693859  0.109830  3.563285  -952.496493   \n",
       "3  0.669289  6.272703  -5.645072  16.358706  0.521193  5.765953  -172.519574   \n",
       "4  6.279002  2.762207  24.306555   0.368850  5.523865  2.742183  1203.275577   \n",
       "\n",
       "       y_dot_0  phi_dot_0  theta_dot_0  \n",
       "0   403.629247  17.099313     1.765834  \n",
       "1 -1421.176764  -9.854699    19.628124  \n",
       "2  1246.648094 -23.807232     0.234332  \n",
       "3   834.968221 -10.130481     7.075612  \n",
       "4   -28.441808  -9.457624   -23.078939  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fx</th>\n",
       "      <th>Fy</th>\n",
       "      <th>tau</th>\n",
       "      <th>x_dot_99</th>\n",
       "      <th>y_dot_99</th>\n",
       "      <th>phi_dot_99</th>\n",
       "      <th>theta_dot_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-11242.020553</td>\n",
       "      <td>35606.393649</td>\n",
       "      <td>-648233.357511</td>\n",
       "      <td>-211.538074</td>\n",
       "      <td>-41.512653</td>\n",
       "      <td>44.506170</td>\n",
       "      <td>-2.419411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>758.528514</td>\n",
       "      <td>-92.198968</td>\n",
       "      <td>716644.704241</td>\n",
       "      <td>-1160.476933</td>\n",
       "      <td>-1453.116332</td>\n",
       "      <td>-26.566195</td>\n",
       "      <td>15.594339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-10970.412433</td>\n",
       "      <td>-18721.155997</td>\n",
       "      <td>247854.582309</td>\n",
       "      <td>-1069.933844</td>\n",
       "      <td>1532.503178</td>\n",
       "      <td>-79.162992</td>\n",
       "      <td>-72.202219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-12208.698603</td>\n",
       "      <td>-5111.826783</td>\n",
       "      <td>-56481.422925</td>\n",
       "      <td>-391.464494</td>\n",
       "      <td>806.760972</td>\n",
       "      <td>-22.274864</td>\n",
       "      <td>-39.941730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>771.440744</td>\n",
       "      <td>-3865.790786</td>\n",
       "      <td>517086.816811</td>\n",
       "      <td>1219.540036</td>\n",
       "      <td>38.803143</td>\n",
       "      <td>-43.310953</td>\n",
       "      <td>-7.143937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Fx            Fy            tau     x_dot_99     y_dot_99  \\\n",
       "0 -11242.020553  35606.393649 -648233.357511  -211.538074   -41.512653   \n",
       "1    758.528514    -92.198968  716644.704241 -1160.476933 -1453.116332   \n",
       "2 -10970.412433 -18721.155997  247854.582309 -1069.933844  1532.503178   \n",
       "3 -12208.698603  -5111.826783  -56481.422925  -391.464494   806.760972   \n",
       "4    771.440744  -3865.790786  517086.816811  1219.540036    38.803143   \n",
       "\n",
       "   phi_dot_99  theta_dot_99  \n",
       "0   44.506170     -2.419411  \n",
       "1  -26.566195     15.594339  \n",
       "2  -79.162992    -72.202219  \n",
       "3  -22.274864    -39.941730  \n",
       "4  -43.310953     -7.143937  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data \n",
    "scalerX = MinMaxScaler([-0.5, 0.5])  \n",
    "scalerY = MinMaxScaler([-0.5, 0.5])  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scalerX.fit(Xtrain)  \n",
    "scalerY.fit(Ytrain) \n",
    "\n",
    "Xtrain_scaled = scalerX.transform(Xtrain)  \n",
    "Ytrain_scaled = scalerY.transform(Ytrain)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "Xtest_scaled = scalerX.transform(Xtest)\n",
    "Ytest_scaled = scalerY.transform(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phi_0</th>\n",
       "      <th>theta_0</th>\n",
       "      <th>x_99</th>\n",
       "      <th>y_99</th>\n",
       "      <th>phi_99</th>\n",
       "      <th>theta_99</th>\n",
       "      <th>x_dot_0</th>\n",
       "      <th>y_dot_0</th>\n",
       "      <th>phi_dot_0</th>\n",
       "      <th>theta_dot_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.277631</td>\n",
       "      <td>-0.456965</td>\n",
       "      <td>-0.269140</td>\n",
       "      <td>0.111676</td>\n",
       "      <td>0.096488</td>\n",
       "      <td>-0.124410</td>\n",
       "      <td>-0.274513</td>\n",
       "      <td>0.065817</td>\n",
       "      <td>-0.181147</td>\n",
       "      <td>-0.380918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.212059</td>\n",
       "      <td>0.489910</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>0.280976</td>\n",
       "      <td>0.050949</td>\n",
       "      <td>0.084479</td>\n",
       "      <td>-0.037885</td>\n",
       "      <td>0.330975</td>\n",
       "      <td>-0.066381</td>\n",
       "      <td>0.278065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.418631</td>\n",
       "      <td>0.306312</td>\n",
       "      <td>0.314160</td>\n",
       "      <td>0.132132</td>\n",
       "      <td>0.166737</td>\n",
       "      <td>0.061896</td>\n",
       "      <td>0.355365</td>\n",
       "      <td>0.223949</td>\n",
       "      <td>-0.344846</td>\n",
       "      <td>-0.316017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.299029</td>\n",
       "      <td>0.134123</td>\n",
       "      <td>-0.286087</td>\n",
       "      <td>-0.020601</td>\n",
       "      <td>-0.107243</td>\n",
       "      <td>0.085676</td>\n",
       "      <td>-0.363650</td>\n",
       "      <td>-0.013231</td>\n",
       "      <td>0.219828</td>\n",
       "      <td>0.393036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.107491</td>\n",
       "      <td>0.070510</td>\n",
       "      <td>-0.215457</td>\n",
       "      <td>0.069055</td>\n",
       "      <td>0.056533</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>-0.236137</td>\n",
       "      <td>0.111233</td>\n",
       "      <td>-0.411587</td>\n",
       "      <td>0.032467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phi_0   theta_0      x_99      y_99    phi_99  theta_99   x_dot_0  \\\n",
       "0  0.277631 -0.456965 -0.269140  0.111676  0.096488 -0.124410 -0.274513   \n",
       "1  0.212059  0.489910  0.005465  0.280976  0.050949  0.084479 -0.037885   \n",
       "2  0.418631  0.306312  0.314160  0.132132  0.166737  0.061896  0.355365   \n",
       "3 -0.299029  0.134123 -0.286087 -0.020601 -0.107243  0.085676 -0.363650   \n",
       "4  0.107491  0.070510 -0.215457  0.069055  0.056533  0.001085 -0.236137   \n",
       "\n",
       "    y_dot_0  phi_dot_0  theta_dot_0  \n",
       "0  0.065817  -0.181147    -0.380918  \n",
       "1  0.330975  -0.066381     0.278065  \n",
       "2  0.223949  -0.344846    -0.316017  \n",
       "3 -0.013231   0.219828     0.393036  \n",
       "4  0.111233  -0.411587     0.032467  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Xtrain_scaled, columns = X.columns).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "# Keras callcacks\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "def create_network(optimizer = 'rmsprop', \n",
    "                    numUnits = [400, 16], \n",
    "                    weightRegularization = 0.0, \n",
    "                    dropout_rate=0.1):\n",
    "    \n",
    "    '''\n",
    "    Create a feed forward network.  Assumes Xtrain & Ytrain have been created and scaled\n",
    "    \n",
    "    Params: \n",
    "    optimizer (str): choice of optimizer\n",
    "    numUnits (list): number of units in each hidden\n",
    "    weightRegularization (float): between 0 and 1\n",
    "    dropout_rate (float): between 0 and 1\n",
    "    \n",
    "    '''\n",
    "    K.clear_session()\n",
    "    inputs = Input(shape=(Xtrain_scaled.shape[1],))    \n",
    "    \n",
    "    # add layers\n",
    "    for ii in np.arange(0, len(numUnits)):\n",
    "        if ii >= 1: \n",
    "            x = Dense(numUnits[ii], activation='tanh', \n",
    "                      kernel_regularizer=regularizers.l1(weightRegularization))(x)\n",
    "\n",
    "        else: \n",
    "            x = Dense(numUnits[ii], activation='tanh')(inputs)\n",
    "\n",
    "\n",
    "        # add dropout\n",
    "        if dropout_rate > 0: \n",
    "            x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    predictions = Dense(Ytrain_scaled.shape[1], activation='linear')(x)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer = optimizer, metrics = ['mse'])\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_e(n):\n",
    "    a = '%E' % n\n",
    "    return a.split('E')[0].rstrip('0').rstrip('.') + 'E' + a.split('E')[1]\n",
    "\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_mean_squared_error', patience=15, \n",
    "                          verbose=1, mode='auto', min_delta = 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history.history['mean_squared_error'])+1),\n",
    "             model_history.history['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "             model_history.history['val_mean_squared_error'])\n",
    "    axs.set_title('Model MSE = '+ str(format_e(model_history.history['val_mean_squared_error'][-1])))\n",
    "    axs.set_ylabel('mean_squared_error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "                   len(model_history.history['val_mean_squared_error'])/10)\n",
    "    axs.legend(['train', 'val'], loc='best')\n",
    "    plt.yscale('log') #logarithmic scale for y axis\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining_\" + modelName + \".png\"), dpi = 120, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_model_history_fromDict(model_history_dictionary, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history_dictionary['mean_squared_error'])+1),\n",
    "             model_history_dictionary['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history_dictionary['val_mean_squared_error'])+1),\n",
    "             model_history_dictionary['val_mean_squared_error'])\n",
    "    axs.set_title('Model MSE = '+ str(format_e(model_history_dictionary['val_mean_squared_error'][-1])) + \"\\n\" +  str(nzwts) + \" non-zero weights\")\n",
    "    axs.set_ylabel('mean_squared_error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(1,len(model_history_dictionary['val_mean_squared_error'])+1),\n",
    "                   len(model_history_dictionary['val_mean_squared_error'])/10)\n",
    "    axs.legend(['train', 'val'], loc='best')\n",
    "    plt.yscale('log') #logarithmic scale for y axis\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining_\" + modelName + \"_pruned.png\"), dpi = 120, bbox_inches='tight')\n",
    "        print(os.path.join(figDir, \"ModelTraining_\" + modelName + \"_pruned.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and trim weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt_rmsprop__Dro_0__Num_200_200_200_16__Wei_0_2020_01_16__03_54_02\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               2200      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                3216      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 119       \n",
      "=================================================================\n",
      "Total params: 85,935\n",
      "Trainable params: 85,935\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "K.clear_session()\n",
    "\n",
    "modelParams = {\"optimizer\": \"rmsprop\", \n",
    "              \"dropout_rate\" : 0, \n",
    "               \"numUnits\": [200, 200, 200, 16],\n",
    "               \"weightRegularization\": 0\n",
    "              }\n",
    "\n",
    "\n",
    "model = create_network(**modelParams)\n",
    "\n",
    "modeltimestamp = datetime.now().strftime(\"%Y_%m_%d__%I_%M_%S\")\n",
    "modelName = ''.join('{}_{}__'.format(key[0:3].capitalize(), val) for  key, val in modelParams.items()).\\\n",
    "                            replace(\"[\", \"\").replace(\"]\", \"\").replace(\", \", \"_\")[0:-2] + \"_\" + modeltimestamp\n",
    "print(modelName)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save scalers, to be used on test set\n",
    "scalerfileX = 'scalerX_fullact_' + modeltimestamp + '.pkl'\n",
    "pickle.dump(scalerX, open(os.path.join(dataOutput, scalerfileX), 'wb'))\n",
    "\n",
    "scalerfileY = 'scalerY_fullact_' + modeltimestamp + '.pkl'\n",
    "pickle.dump(scalerY, open(os.path.join(dataOutput, scalerfileY), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "historyDict = {\"mean_squared_error\": [], \n",
    "               \"val_mean_squared_error\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with pruning\n",
    "numCuts = 1\n",
    "\n",
    "wts = model.get_weights()\n",
    "wtLengths = []\n",
    "for ii in range(len(wts)):\n",
    "    wtLengths.append(np.prod(wts[ii].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 0.0107 - mean_squared_error: 0.0107 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "72300 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "72300 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "72300 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "72300 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 10s - loss: 9.5792e-04 - mean_squared_error: 9.5792e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "72300 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 8.6026e-04 - mean_squared_error: 8.6026e-04 - val_loss: 9.0458e-04 - val_mean_squared_error: 9.0458e-04\n",
      "72300 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 7.9246e-04 - mean_squared_error: 7.9246e-04 - val_loss: 8.0029e-04 - val_mean_squared_error: 8.0029e-04\n",
      "72300 of 85935 weights retained\n",
      "change in log loss: -0.1335512287373377\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 7.4377e-04 - mean_squared_error: 7.4377e-04 - val_loss: 6.5752e-04 - val_mean_squared_error: 6.5752e-04\n",
      "59342 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 7.0960e-04 - mean_squared_error: 7.0960e-04 - val_loss: 6.6727e-04 - val_mean_squared_error: 6.6727e-04\n",
      "59342 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 6.7728e-04 - mean_squared_error: 6.7728e-04 - val_loss: 6.1170e-04 - val_mean_squared_error: 6.1170e-04\n",
      "59342 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 6.5132e-04 - mean_squared_error: 6.5132e-04 - val_loss: 7.6521e-04 - val_mean_squared_error: 7.6521e-04\n",
      "59342 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 6.2908e-04 - mean_squared_error: 6.2908e-04 - val_loss: 5.4633e-04 - val_mean_squared_error: 5.4633e-04\n",
      "59342 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 6.0935e-04 - mean_squared_error: 6.0935e-04 - val_loss: 6.2208e-04 - val_mean_squared_error: 6.2208e-04\n",
      "59342 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.9196e-04 - mean_squared_error: 5.9196e-04 - val_loss: 6.4527e-04 - val_mean_squared_error: 6.4527e-04\n",
      "59342 of 85935 weights retained\n",
      "change in log loss: -0.033590065645204614\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.7650e-04 - mean_squared_error: 5.7650e-04 - val_loss: 5.2070e-04 - val_mean_squared_error: 5.2070e-04\n",
      "47592 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.6787e-04 - mean_squared_error: 5.6787e-04 - val_loss: 5.5348e-04 - val_mean_squared_error: 5.5348e-04\n",
      "47592 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.5062e-04 - mean_squared_error: 5.5062e-04 - val_loss: 5.2359e-04 - val_mean_squared_error: 5.2359e-04\n",
      "47592 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.3724e-04 - mean_squared_error: 5.3724e-04 - val_loss: 5.8369e-04 - val_mean_squared_error: 5.8369e-04\n",
      "47592 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.2548e-04 - mean_squared_error: 5.2548e-04 - val_loss: 4.8423e-04 - val_mean_squared_error: 4.8423e-04\n",
      "47592 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 10s - loss: 5.1447e-04 - mean_squared_error: 5.1447e-04 - val_loss: 5.1875e-04 - val_mean_squared_error: 5.1875e-04\n",
      "47592 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.0396e-04 - mean_squared_error: 5.0396e-04 - val_loss: 4.3944e-04 - val_mean_squared_error: 4.3944e-04\n",
      "47592 of 85935 weights retained\n",
      "change in log loss: -0.02204147268039036\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.9355e-04 - mean_squared_error: 4.9355e-04 - val_loss: 5.0559e-04 - val_mean_squared_error: 5.0559e-04\n",
      "37396 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.9621e-04 - mean_squared_error: 4.9621e-04 - val_loss: 5.6701e-04 - val_mean_squared_error: 5.6701e-04\n",
      "37396 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 10s - loss: 4.8088e-04 - mean_squared_error: 4.8088e-04 - val_loss: 5.8778e-04 - val_mean_squared_error: 5.8778e-04\n",
      "37396 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7153e-04 - mean_squared_error: 4.7153e-04 - val_loss: 5.2927e-04 - val_mean_squared_error: 5.2927e-04\n",
      "37396 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.6308e-04 - mean_squared_error: 4.6308e-04 - val_loss: 5.4999e-04 - val_mean_squared_error: 5.4999e-04\n",
      "37396 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.5546e-04 - mean_squared_error: 4.5546e-04 - val_loss: 4.7320e-04 - val_mean_squared_error: 4.7320e-04\n",
      "37396 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.4875e-04 - mean_squared_error: 4.4875e-04 - val_loss: 5.0387e-04 - val_mean_squared_error: 5.0387e-04\n",
      "37396 of 85935 weights retained\n",
      "change in log loss: -0.017299738381449492\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.4266e-04 - mean_squared_error: 4.4266e-04 - val_loss: 3.9291e-04 - val_mean_squared_error: 3.9291e-04\n",
      "28880 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7995e-04 - mean_squared_error: 4.7995e-04 - val_loss: 4.6417e-04 - val_mean_squared_error: 4.6417e-04\n",
      "28880 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.4766e-04 - mean_squared_error: 4.4766e-04 - val_loss: 4.8869e-04 - val_mean_squared_error: 4.8869e-04\n",
      "28880 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.3768e-04 - mean_squared_error: 4.3768e-04 - val_loss: 4.0703e-04 - val_mean_squared_error: 4.0703e-04\n",
      "28880 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 10s - loss: 4.3017e-04 - mean_squared_error: 4.3017e-04 - val_loss: 4.2249e-04 - val_mean_squared_error: 4.2249e-04\n",
      "28880 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.2403e-04 - mean_squared_error: 4.2403e-04 - val_loss: 4.7659e-04 - val_mean_squared_error: 4.7659e-04\n",
      "28880 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.1859e-04 - mean_squared_error: 4.1859e-04 - val_loss: 4.2351e-04 - val_mean_squared_error: 4.2351e-04\n",
      "28880 of 85935 weights retained\n",
      "change in log loss: -0.016594132513985782\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.1347e-04 - mean_squared_error: 4.1347e-04 - val_loss: 4.3270e-04 - val_mean_squared_error: 4.3270e-04\n",
      "21984 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.3538e-04 - mean_squared_error: 4.3538e-04 - val_loss: 3.8816e-04 - val_mean_squared_error: 3.8816e-04\n",
      "21984 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.1432e-04 - mean_squared_error: 4.1432e-04 - val_loss: 3.9367e-04 - val_mean_squared_error: 3.9367e-04\n",
      "21984 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.0533e-04 - mean_squared_error: 4.0533e-04 - val_loss: 3.5393e-04 - val_mean_squared_error: 3.5393e-04\n",
      "21984 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9914e-04 - mean_squared_error: 3.9914e-04 - val_loss: 5.0099e-04 - val_mean_squared_error: 5.0099e-04\n",
      "21984 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9415e-04 - mean_squared_error: 3.9415e-04 - val_loss: 4.2482e-04 - val_mean_squared_error: 4.2482e-04\n",
      "21984 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8972e-04 - mean_squared_error: 3.8972e-04 - val_loss: 3.4595e-04 - val_mean_squared_error: 3.4595e-04\n",
      "21984 of 85935 weights retained\n",
      "change in log loss: -0.015039121242553133\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8572e-04 - mean_squared_error: 3.8572e-04 - val_loss: 4.2888e-04 - val_mean_squared_error: 4.2888e-04\n",
      "16546 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.4483e-04 - mean_squared_error: 4.4483e-04 - val_loss: 3.6361e-04 - val_mean_squared_error: 3.6361e-04\n",
      "16546 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.1953e-04 - mean_squared_error: 4.1953e-04 - val_loss: 3.9163e-04 - val_mean_squared_error: 3.9163e-04\n",
      "16546 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.0882e-04 - mean_squared_error: 4.0882e-04 - val_loss: 3.4626e-04 - val_mean_squared_error: 3.4626e-04\n",
      "16546 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.0209e-04 - mean_squared_error: 4.0209e-04 - val_loss: 4.1368e-04 - val_mean_squared_error: 4.1368e-04\n",
      "16546 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9645e-04 - mean_squared_error: 3.9645e-04 - val_loss: 3.5513e-04 - val_mean_squared_error: 3.5513e-04\n",
      "16546 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9065e-04 - mean_squared_error: 3.9065e-04 - val_loss: 3.3340e-04 - val_mean_squared_error: 3.3340e-04\n",
      "16546 of 85935 weights retained\n",
      "change in log loss: -0.017334471356012404\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8676e-04 - mean_squared_error: 3.8676e-04 - val_loss: 3.3461e-04 - val_mean_squared_error: 3.3461e-04\n",
      "16546 of 85935 weights retained\n",
      "change in log loss: -0.013981382986891111\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8307e-04 - mean_squared_error: 3.8307e-04 - val_loss: 4.5774e-04 - val_mean_squared_error: 4.5774e-04\n",
      "16546 of 85935 weights retained\n",
      "change in log loss: -0.01216906855718114\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7951e-04 - mean_squared_error: 3.7951e-04 - val_loss: 3.0824e-04 - val_mean_squared_error: 3.0824e-04\n",
      "12344 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7912e-04 - mean_squared_error: 4.7912e-04 - val_loss: 4.1324e-04 - val_mean_squared_error: 4.1324e-04\n",
      "12344 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.4244e-04 - mean_squared_error: 4.4244e-04 - val_loss: 3.3551e-04 - val_mean_squared_error: 3.3551e-04\n",
      "12344 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.3133e-04 - mean_squared_error: 4.3133e-04 - val_loss: 4.3268e-04 - val_mean_squared_error: 4.3268e-04\n",
      "12344 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.2360e-04 - mean_squared_error: 4.2360e-04 - val_loss: 3.5332e-04 - val_mean_squared_error: 3.5332e-04\n",
      "12344 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.1950e-04 - mean_squared_error: 4.1950e-04 - val_loss: 3.8291e-04 - val_mean_squared_error: 3.8291e-04\n",
      "12344 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.1461e-04 - mean_squared_error: 4.1461e-04 - val_loss: 3.7871e-04 - val_mean_squared_error: 3.7871e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.01577653280555036\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.1159e-04 - mean_squared_error: 4.1159e-04 - val_loss: 4.5763e-04 - val_mean_squared_error: 4.5763e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.011519004543276035\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.0748e-04 - mean_squared_error: 4.0748e-04 - val_loss: 3.5014e-04 - val_mean_squared_error: 3.5014e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.009662927569340285\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.0485e-04 - mean_squared_error: 4.0485e-04 - val_loss: 3.5564e-04 - val_mean_squared_error: 3.5564e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.008840985343743224\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.0221e-04 - mean_squared_error: 4.0221e-04 - val_loss: 3.8462e-04 - val_mean_squared_error: 3.8462e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.007720280882402841\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9983e-04 - mean_squared_error: 3.9983e-04 - val_loss: 3.3294e-04 - val_mean_squared_error: 3.3294e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.007095912433702001\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9805e-04 - mean_squared_error: 3.9805e-04 - val_loss: 3.1789e-04 - val_mean_squared_error: 3.1789e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.005933090542349362\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9524e-04 - mean_squared_error: 3.9524e-04 - val_loss: 3.1726e-04 - val_mean_squared_error: 3.1726e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.005847150141117918\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 10s - loss: 3.9240e-04 - mean_squared_error: 3.9240e-04 - val_loss: 2.9634e-04 - val_mean_squared_error: 2.9634e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.006098639549689144\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9043e-04 - mean_squared_error: 3.9043e-04 - val_loss: 3.3966e-04 - val_mean_squared_error: 3.3966e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.006188945592813466\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8824e-04 - mean_squared_error: 3.8824e-04 - val_loss: 3.7979e-04 - val_mean_squared_error: 3.7979e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.006210895703632868\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8585e-04 - mean_squared_error: 3.8585e-04 - val_loss: 3.5674e-04 - val_mean_squared_error: 3.5674e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.0058688906269930285\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8427e-04 - mean_squared_error: 3.8427e-04 - val_loss: 3.0311e-04 - val_mean_squared_error: 3.0311e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.005366501387938261\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8185e-04 - mean_squared_error: 3.8185e-04 - val_loss: 3.9511e-04 - val_mean_squared_error: 3.9511e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.00547270396214139\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8006e-04 - mean_squared_error: 3.8006e-04 - val_loss: 3.4305e-04 - val_mean_squared_error: 3.4305e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.005303118883321467\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7814e-04 - mean_squared_error: 3.7814e-04 - val_loss: 3.2340e-04 - val_mean_squared_error: 3.2340e-04\n",
      "12344 of 85935 weights retained\n",
      "change in log loss: -0.005138115493516837\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7567e-04 - mean_squared_error: 3.7567e-04 - val_loss: 3.0567e-04 - val_mean_squared_error: 3.0567e-04\n",
      "9142 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.4914e-04 - mean_squared_error: 4.4914e-04 - val_loss: 3.4259e-04 - val_mean_squared_error: 3.4259e-04\n",
      "9142 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.1392e-04 - mean_squared_error: 4.1392e-04 - val_loss: 3.3586e-04 - val_mean_squared_error: 3.3586e-04\n",
      "9142 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.0030e-04 - mean_squared_error: 4.0030e-04 - val_loss: 4.1447e-04 - val_mean_squared_error: 4.1447e-04\n",
      "9142 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9247e-04 - mean_squared_error: 3.9247e-04 - val_loss: 3.0482e-04 - val_mean_squared_error: 3.0482e-04\n",
      "9142 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8792e-04 - mean_squared_error: 3.8792e-04 - val_loss: 2.9691e-04 - val_mean_squared_error: 2.9691e-04\n",
      "9142 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8328e-04 - mean_squared_error: 3.8328e-04 - val_loss: 2.9485e-04 - val_mean_squared_error: 2.9485e-04\n",
      "9142 of 85935 weights retained\n",
      "change in log loss: -0.018527138302266932\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7964e-04 - mean_squared_error: 3.7964e-04 - val_loss: 2.9971e-04 - val_mean_squared_error: 2.9971e-04\n",
      "9142 of 85935 weights retained\n",
      "change in log loss: -0.012971639997263917\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7682e-04 - mean_squared_error: 3.7682e-04 - val_loss: 3.6252e-04 - val_mean_squared_error: 3.6252e-04\n",
      "9142 of 85935 weights retained\n",
      "change in log loss: -0.010295900653742862\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7434e-04 - mean_squared_error: 3.7434e-04 - val_loss: 4.2451e-04 - val_mean_squared_error: 4.2451e-04\n",
      "9142 of 85935 weights retained\n",
      "change in log loss: -0.008824548919633024\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7226e-04 - mean_squared_error: 3.7226e-04 - val_loss: 3.1242e-04 - val_mean_squared_error: 3.1242e-04\n",
      "6740 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.9079e-04 - mean_squared_error: 4.9079e-04 - val_loss: 3.2686e-04 - val_mean_squared_error: 3.2686e-04\n",
      "6740 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.2876e-04 - mean_squared_error: 4.2876e-04 - val_loss: 3.4923e-04 - val_mean_squared_error: 3.4923e-04\n",
      "6740 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.0844e-04 - mean_squared_error: 4.0844e-04 - val_loss: 3.5390e-04 - val_mean_squared_error: 3.5390e-04\n",
      "6740 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9915e-04 - mean_squared_error: 3.9915e-04 - val_loss: 3.7760e-04 - val_mean_squared_error: 3.7760e-04\n",
      "6740 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9371e-04 - mean_squared_error: 3.9371e-04 - val_loss: 3.9614e-04 - val_mean_squared_error: 3.9614e-04\n",
      "6740 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8849e-04 - mean_squared_error: 3.8849e-04 - val_loss: 4.1502e-04 - val_mean_squared_error: 4.1502e-04\n",
      "6740 of 85935 weights retained\n",
      "change in log loss: -0.023398612161771748\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8527e-04 - mean_squared_error: 3.8527e-04 - val_loss: 3.0693e-04 - val_mean_squared_error: 3.0693e-04\n",
      "6740 of 85935 weights retained\n",
      "change in log loss: -0.01438690115253749\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8139e-04 - mean_squared_error: 3.8139e-04 - val_loss: 3.4168e-04 - val_mean_squared_error: 3.4168e-04\n",
      "6740 of 85935 weights retained\n",
      "change in log loss: -0.011271395313360744\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7835e-04 - mean_squared_error: 3.7835e-04 - val_loss: 3.5219e-04 - val_mean_squared_error: 3.5219e-04\n",
      "6740 of 85935 weights retained\n",
      "change in log loss: -0.009803099323571152\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7554e-04 - mean_squared_error: 3.7554e-04 - val_loss: 3.1864e-04 - val_mean_squared_error: 3.1864e-04\n",
      "6740 of 85935 weights retained\n",
      "change in log loss: -0.008594585728566972\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7353e-04 - mean_squared_error: 3.7353e-04 - val_loss: 4.3946e-04 - val_mean_squared_error: 4.3946e-04\n",
      "6740 of 85935 weights retained\n",
      "change in log loss: -0.0077362933820595\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7176e-04 - mean_squared_error: 3.7176e-04 - val_loss: 3.1340e-04 - val_mean_squared_error: 3.1340e-04\n",
      "6740 of 85935 weights retained\n",
      "change in log loss: -0.006394890399216591\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.6976e-04 - mean_squared_error: 3.6976e-04 - val_loss: 2.8861e-04 - val_mean_squared_error: 2.8861e-04\n",
      "4948 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 6.0120e-04 - mean_squared_error: 6.0120e-04 - val_loss: 3.9398e-04 - val_mean_squared_error: 3.9398e-04\n",
      "4948 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.0750e-04 - mean_squared_error: 5.0750e-04 - val_loss: 3.4577e-04 - val_mean_squared_error: 3.4577e-04\n",
      "4948 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7261e-04 - mean_squared_error: 4.7261e-04 - val_loss: 3.4795e-04 - val_mean_squared_error: 3.4795e-04\n",
      "4948 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.5315e-04 - mean_squared_error: 4.5315e-04 - val_loss: 4.0556e-04 - val_mean_squared_error: 4.0556e-04\n",
      "4948 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.4309e-04 - mean_squared_error: 4.4309e-04 - val_loss: 3.4164e-04 - val_mean_squared_error: 3.4164e-04\n",
      "4948 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.3472e-04 - mean_squared_error: 4.3472e-04 - val_loss: 3.4530e-04 - val_mean_squared_error: 3.4530e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.0374097669665403\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.2819e-04 - mean_squared_error: 4.2819e-04 - val_loss: 3.5229e-04 - val_mean_squared_error: 3.5229e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.02388992376533805\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.2382e-04 - mean_squared_error: 4.2382e-04 - val_loss: 3.1125e-04 - val_mean_squared_error: 3.1125e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.016800952097880906\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.2013e-04 - mean_squared_error: 4.2013e-04 - val_loss: 3.7611e-04 - val_mean_squared_error: 3.7611e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.013181306022766348\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.1713e-04 - mean_squared_error: 4.1713e-04 - val_loss: 3.2419e-04 - val_mean_squared_error: 3.2419e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.010160515631806089\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.1415e-04 - mean_squared_error: 4.1415e-04 - val_loss: 4.3134e-04 - val_mean_squared_error: 4.3134e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.00825819999342059\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.1029e-04 - mean_squared_error: 4.1029e-04 - val_loss: 3.7879e-04 - val_mean_squared_error: 3.7879e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.007922559654779171\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.0696e-04 - mean_squared_error: 4.0696e-04 - val_loss: 3.2578e-04 - val_mean_squared_error: 3.2578e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.00802308423631426\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.0418e-04 - mean_squared_error: 4.0418e-04 - val_loss: 3.7880e-04 - val_mean_squared_error: 3.7880e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.00806111402093812\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.0016e-04 - mean_squared_error: 4.0016e-04 - val_loss: 3.4838e-04 - val_mean_squared_error: 3.4838e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.008374838969325005\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9745e-04 - mean_squared_error: 3.9745e-04 - val_loss: 3.5214e-04 - val_mean_squared_error: 3.5214e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.00804351438311035\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9619e-04 - mean_squared_error: 3.9619e-04 - val_loss: 3.4489e-04 - val_mean_squared_error: 3.4489e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.007044448743277032\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9409e-04 - mean_squared_error: 3.9409e-04 - val_loss: 3.6732e-04 - val_mean_squared_error: 3.6732e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.006054388559729729\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9299e-04 - mean_squared_error: 3.9299e-04 - val_loss: 3.2651e-04 - val_mean_squared_error: 3.2651e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.004467747138685141\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.9136e-04 - mean_squared_error: 3.9136e-04 - val_loss: 3.7084e-04 - val_mean_squared_error: 3.7084e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.003899123493128265\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8670e-04 - mean_squared_error: 3.8670e-04 - val_loss: 3.9743e-04 - val_mean_squared_error: 3.9743e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.005541875480561398\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8522e-04 - mean_squared_error: 3.8522e-04 - val_loss: 2.9482e-04 - val_mean_squared_error: 2.9482e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.0061621774154991815\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8288e-04 - mean_squared_error: 3.8288e-04 - val_loss: 3.1288e-04 - val_mean_squared_error: 3.1288e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.006791335266055354\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8104e-04 - mean_squared_error: 3.8104e-04 - val_loss: 3.5507e-04 - val_mean_squared_error: 3.5507e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.006337769439434648\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.8052e-04 - mean_squared_error: 3.8052e-04 - val_loss: 3.1196e-04 - val_mean_squared_error: 3.1196e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.004312021890201301\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7948e-04 - mean_squared_error: 3.7948e-04 - val_loss: 3.3022e-04 - val_mean_squared_error: 3.3022e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.003623329989747681\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7722e-04 - mean_squared_error: 3.7722e-04 - val_loss: 3.3255e-04 - val_mean_squared_error: 3.3255e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.0033929658215561265\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7578e-04 - mean_squared_error: 3.7578e-04 - val_loss: 3.1158e-04 - val_mean_squared_error: 3.1158e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.0036558722027690838\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7554e-04 - mean_squared_error: 3.7554e-04 - val_loss: 2.9783e-04 - val_mean_squared_error: 2.9783e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.00361876780435888\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7460e-04 - mean_squared_error: 3.7460e-04 - val_loss: 3.0607e-04 - val_mean_squared_error: 3.0607e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.003035465300402329\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7344e-04 - mean_squared_error: 3.7344e-04 - val_loss: 3.3012e-04 - val_mean_squared_error: 3.3012e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.002328484018817578\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7233e-04 - mean_squared_error: 3.7233e-04 - val_loss: 3.0757e-04 - val_mean_squared_error: 3.0757e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.0024018970612438295\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7196e-04 - mean_squared_error: 3.7196e-04 - val_loss: 3.1369e-04 - val_mean_squared_error: 3.1369e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.0025222381743403854\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7046e-04 - mean_squared_error: 3.7046e-04 - val_loss: 3.7085e-04 - val_mean_squared_error: 3.7085e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.0026184519814969054\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.7060e-04 - mean_squared_error: 3.7060e-04 - val_loss: 3.0149e-04 - val_mean_squared_error: 3.0149e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.002030970876265137\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.6874e-04 - mean_squared_error: 3.6874e-04 - val_loss: 3.3226e-04 - val_mean_squared_error: 3.3226e-04\n",
      "4948 of 85935 weights retained\n",
      "change in log loss: -0.0023028894313101844\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "************************************************ PRUNING ********************************************************\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 3.6769e-04 - mean_squared_error: 3.6769e-04 - val_loss: 3.1888e-04 - val_mean_squared_error: 3.1888e-04\n",
      "3630 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 9.0012e-04 - mean_squared_error: 9.0012e-04 - val_loss: 4.0267e-04 - val_mean_squared_error: 4.0267e-04\n",
      "3630 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 7.5579e-04 - mean_squared_error: 7.5579e-04 - val_loss: 4.6095e-04 - val_mean_squared_error: 4.6095e-04\n",
      "3630 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 6.7275e-04 - mean_squared_error: 6.7275e-04 - val_loss: 3.7286e-04 - val_mean_squared_error: 3.7286e-04\n",
      "3630 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 6.2028e-04 - mean_squared_error: 6.2028e-04 - val_loss: 4.4753e-04 - val_mean_squared_error: 4.4753e-04\n",
      "3630 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.9372e-04 - mean_squared_error: 5.9372e-04 - val_loss: 3.3987e-04 - val_mean_squared_error: 3.3987e-04\n",
      "3630 of 85935 weights retained\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.7102e-04 - mean_squared_error: 5.7102e-04 - val_loss: 3.8279e-04 - val_mean_squared_error: 3.8279e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.06856562286076762\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.5795e-04 - mean_squared_error: 5.5795e-04 - val_loss: 3.6307e-04 - val_mean_squared_error: 3.6307e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.04569378836633731\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.4694e-04 - mean_squared_error: 5.4694e-04 - val_loss: 3.5722e-04 - val_mean_squared_error: 3.5722e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.031378939834216135\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.6089e-04 - mean_squared_error: 5.6089e-04 - val_loss: 3.5492e-04 - val_mean_squared_error: 3.5492e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.015683570823950665\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.2855e-04 - mean_squared_error: 5.2855e-04 - val_loss: 3.5308e-04 - val_mean_squared_error: 3.5308e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.014932095412418134\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.2170e-04 - mean_squared_error: 5.2170e-04 - val_loss: 3.7519e-04 - val_mean_squared_error: 3.7519e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.01685961211961251\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.1261e-04 - mean_squared_error: 5.1261e-04 - val_loss: 3.6821e-04 - val_mean_squared_error: 3.6821e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.02020935773951904\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.1242e-04 - mean_squared_error: 5.1242e-04 - val_loss: 3.3048e-04 - val_mean_squared_error: 3.3048e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.021136843108169323\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.0722e-04 - mean_squared_error: 5.0722e-04 - val_loss: 3.8377e-04 - val_mean_squared_error: 3.8377e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.01003110724887124\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.4043e-04 - mean_squared_error: 5.4043e-04 - val_loss: 3.5956e-04 - val_mean_squared_error: 3.5956e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: 0.006000643593377264\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.3092e-04 - mean_squared_error: 5.3092e-04 - val_loss: 3.5385e-04 - val_mean_squared_error: 3.5385e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: 0.012340132005213444\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.2139e-04 - mean_squared_error: 5.2139e-04 - val_loss: 4.2332e-04 - val_mean_squared_error: 4.2332e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: 0.008034239883282446\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.1463e-04 - mean_squared_error: 5.1463e-04 - val_loss: 4.4082e-04 - val_mean_squared_error: 4.4082e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0006877853339921636\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.1283e-04 - mean_squared_error: 5.1283e-04 - val_loss: 3.6097e-04 - val_mean_squared_error: 3.6097e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.013599781514842535\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.1093e-04 - mean_squared_error: 5.1093e-04 - val_loss: 4.0655e-04 - val_mean_squared_error: 4.0655e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.009328287021662884\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.0660e-04 - mean_squared_error: 5.0660e-04 - val_loss: 3.2501e-04 - val_mean_squared_error: 3.2501e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.006473880614671579\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 5.0360e-04 - mean_squared_error: 5.0360e-04 - val_loss: 3.5507e-04 - val_mean_squared_error: 3.5507e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.005553480216421658\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 10s - loss: 5.0602e-04 - mean_squared_error: 5.0602e-04 - val_loss: 3.4256e-04 - val_mean_squared_error: 3.4256e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.004118294635003572\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 10s - loss: 5.0172e-04 - mean_squared_error: 5.0172e-04 - val_loss: 3.2271e-04 - val_mean_squared_error: 3.2271e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0037527707993507686\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.9595e-04 - mean_squared_error: 4.9595e-04 - val_loss: 3.8351e-04 - val_mean_squared_error: 3.8351e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0046272672498794964\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.9439e-04 - mean_squared_error: 4.9439e-04 - val_loss: 3.7657e-04 - val_mean_squared_error: 3.7657e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.005704391969100442\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.9652e-04 - mean_squared_error: 4.9652e-04 - val_loss: 3.9814e-04 - val_mean_squared_error: 3.9814e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0052614662157428604\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.8982e-04 - mean_squared_error: 4.8982e-04 - val_loss: 3.3190e-04 - val_mean_squared_error: 3.3190e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.004681864266385949\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 10s - loss: 4.8804e-04 - mean_squared_error: 4.8804e-04 - val_loss: 3.6081e-04 - val_mean_squared_error: 3.6081e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.004142174513149577\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.9038e-04 - mean_squared_error: 4.9038e-04 - val_loss: 3.2522e-04 - val_mean_squared_error: 3.2522e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.003354515833919436\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.8248e-04 - mean_squared_error: 4.8248e-04 - val_loss: 3.5696e-04 - val_mean_squared_error: 3.5696e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.005627067906538885\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.8518e-04 - mean_squared_error: 4.8518e-04 - val_loss: 3.1099e-04 - val_mean_squared_error: 3.1099e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.003049689241567455\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.8209e-04 - mean_squared_error: 4.8209e-04 - val_loss: 3.5480e-04 - val_mean_squared_error: 3.5480e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0035179057858223395\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.8015e-04 - mean_squared_error: 4.8015e-04 - val_loss: 4.0767e-04 - val_mean_squared_error: 4.0767e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.004293654249837964\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.8111e-04 - mean_squared_error: 4.8111e-04 - val_loss: 3.5835e-04 - val_mean_squared_error: 3.5835e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0016078391720264484\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7885e-04 - mean_squared_error: 4.7885e-04 - val_loss: 3.8731e-04 - val_mean_squared_error: 3.8731e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0028292773971287932\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7653e-04 - mean_squared_error: 4.7653e-04 - val_loss: 3.3929e-04 - val_mean_squared_error: 3.3929e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0025923376914380736\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7523e-04 - mean_squared_error: 4.7523e-04 - val_loss: 3.8070e-04 - val_mean_squared_error: 3.8070e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.00302039336018578\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7326e-04 - mean_squared_error: 4.7326e-04 - val_loss: 3.9684e-04 - val_mean_squared_error: 3.9684e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.004053700790524584\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7386e-04 - mean_squared_error: 4.7386e-04 - val_loss: 3.3761e-04 - val_mean_squared_error: 3.3761e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.002784548192683145\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7648e-04 - mean_squared_error: 4.7648e-04 - val_loss: 3.5552e-04 - val_mean_squared_error: 3.5552e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0003066440832284467\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7447e-04 - mean_squared_error: 4.7447e-04 - val_loss: 3.4878e-04 - val_mean_squared_error: 3.4878e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: 0.0003607768060165384\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7334e-04 - mean_squared_error: 4.7334e-04 - val_loss: 3.6808e-04 - val_mean_squared_error: 3.6808e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: 0.00016492700351022904\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7248e-04 - mean_squared_error: 4.7248e-04 - val_loss: 3.7883e-04 - val_mean_squared_error: 3.7883e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.001245379992200757\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.7037e-04 - mean_squared_error: 4.7037e-04 - val_loss: 3.6007e-04 - val_mean_squared_error: 3.6007e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0030015726430510092\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.6869e-04 - mean_squared_error: 4.6869e-04 - val_loss: 3.5811e-04 - val_mean_squared_error: 3.5811e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0030795274654324967\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.6453e-04 - mean_squared_error: 4.6453e-04 - val_loss: 3.2858e-04 - val_mean_squared_error: 3.2858e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.004560977909248809\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.6505e-04 - mean_squared_error: 4.6505e-04 - val_loss: 3.1439e-04 - val_mean_squared_error: 3.1439e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.004421721850410787\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.6354e-04 - mean_squared_error: 4.6354e-04 - val_loss: 3.4935e-04 - val_mean_squared_error: 3.4935e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.003710672435843554\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.6377e-04 - mean_squared_error: 4.6377e-04 - val_loss: 3.2052e-04 - val_mean_squared_error: 3.2052e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.002325984242994128\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.6247e-04 - mean_squared_error: 4.6247e-04 - val_loss: 3.1990e-04 - val_mean_squared_error: 3.1990e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0011648019371133245\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.5982e-04 - mean_squared_error: 4.5982e-04 - val_loss: 3.7175e-04 - val_mean_squared_error: 3.7175e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0024893038573137183\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.5894e-04 - mean_squared_error: 4.5894e-04 - val_loss: 3.7282e-04 - val_mean_squared_error: 3.7282e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0028477734649321462\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.5779e-04 - mean_squared_error: 4.5779e-04 - val_loss: 3.6258e-04 - val_mean_squared_error: 3.6258e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0033647522537785246\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 10s - loss: 4.5726e-04 - mean_squared_error: 4.5726e-04 - val_loss: 3.9439e-04 - val_mean_squared_error: 3.9439e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0027077550763985325\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.5680e-04 - mean_squared_error: 4.5680e-04 - val_loss: 3.3066e-04 - val_mean_squared_error: 3.3066e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0016836810561411397\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.5699e-04 - mean_squared_error: 4.5699e-04 - val_loss: 3.7758e-04 - val_mean_squared_error: 3.7758e-04\n",
      "3630 of 85935 weights retained\n",
      "change in log loss: -0.0010670444345590724\n",
      "Train on 5600000 samples, validate on 2400000 samples\n",
      "Epoch 1/1\n",
      " - 9s - loss: 4.5836e-04 - mean_squared_error: 4.5836e-04 - val_loss: 3.4878e-04 - val_mean_squared_error: 3.4878e-04\n"
     ]
    }
   ],
   "source": [
    "# train until I have very good results\n",
    "# then prune, and retrain until results are close\n",
    "# change pruning rate as data get closer to 100% removed\n",
    "def prune_percent_updater(x):\n",
    "    logit = np.exp(x*8) / (np.exp(x*8) + 1)\n",
    "    return((logit - 0.5)*2*50)\n",
    "\n",
    "\n",
    "# cuts a smaller portion as the percent gets closer to 100%\n",
    "cutPercent = prune_percent_updater(np.linspace(0, 1, 26))\n",
    "\n",
    "while True:   \n",
    "   \n",
    "    for numEpocs in range(100):\n",
    "        \n",
    "        MSE_tmp = []\n",
    "\n",
    "        history = model.fit(Xtrain_scaled, Ytrain_scaled, validation_split = 0.3, \n",
    "                            verbose = 2, batch_size=2**12, epochs = 1)\n",
    "\n",
    "        \n",
    "        # save history\n",
    "        historyDict[\"mean_squared_error\"].append(history.history[\"mean_squared_error\"][0])\n",
    "        historyDict[\"val_mean_squared_error\"].append(history.history[\"val_mean_squared_error\"][0])\n",
    "        \n",
    "        # local MSE\n",
    "        MSE_tmp.append(history.history[\"mean_squared_error\"][0])\n",
    "\n",
    "        # set weights that are close to 0 all the way back to 0, and then retrain for one epoch\n",
    "        # get nonzero weights\n",
    "        wts = model.get_weights().copy()\n",
    "\n",
    "        # set weights close to 0 to 0 (but ignore biases)\n",
    "        for ii in np.arange(0, len(wts), 1):\n",
    "            qants = np.percentile(np.reshape(wts[ii], -1), q = (50 - cutPercent[numCuts], 50 + cutPercent[numCuts]), )\n",
    "            wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "\n",
    "        # print nonzero weights\n",
    "        # calculate number of nonzero weights\n",
    "        nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "        print(nzwts, \"of\", np.sum(wtLengths), \"weights retained\")\n",
    "\n",
    "        # set new weights and calculate new loss\n",
    "        model.set_weights(wts)\n",
    "        \n",
    "        # check the change in mean squared error, and if it's not changing much, then cut out more data\n",
    "        # calculate slope of loss, based on previous 5 data points\n",
    "        if numEpocs > 5:\n",
    "            inputData = historyDict[\"mean_squared_error\"][-5:]\n",
    "\n",
    "            m = np.shape(inputData)\n",
    "            X = np.matrix([np.ones(m), np.arange(0, len(inputData))]).T\n",
    "            y = np.matrix(np.log(inputData)).T\n",
    "\n",
    "            # Solve for projection matrix\n",
    "            intercept, slope = np.array(np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)).reshape(-1,)\n",
    "            print(\"change in log loss:\", slope)\n",
    "    \n",
    "            # break if slope has stopped changing or if the overall min has been surpassed\n",
    "            # in the first training, it will automatically prune after 5 epochs, because the min will be passed\n",
    "            if (np.abs(slope) < 0.0001) or (history.history[\"mean_squared_error\"][0] < np.min(historyDict[\"mean_squared_error\"][:-1])): \n",
    "                print(\"************************************************ PRUNING ********************************************************\")\n",
    "                print(\"************************************************ PRUNING ********************************************************\")\n",
    "                print(\"************************************************ PRUNING ********************************************************\")\n",
    "                model.save(os.path.join(figDir,  modelName + str(datetime.now())[0:-7].replace(\"-\", \"_\").replace(\" \", \"__\").replace(\":\", \"_\") + '_Pruned.h5'))\n",
    "                break\n",
    "                       \n",
    "                    \n",
    "    ## refref: may want to save weights before each pruning, so I can go back, if I need to\n",
    "    ## refref: should I be pruning the biases too?\n",
    "    \n",
    "    ## keep running tally of min mse, and if we can't get back to the min, then break\n",
    "#     print(\"Min MSE for this prune \", np.min(MSE_tmp), \"______overall Min MSE \", np.min(historyDict[\"mean_squared_error\"]))\n",
    "#     if np.min(MSE_tmp) > np.min(historyDict[\"mean_squared_error\"]):\n",
    "#         print(\"no more gain by pruning:  STOPPING Pruning\")\n",
    "#         break\n",
    "    \n",
    "    numCuts += 1\n",
    "    if numCuts >= len(cutPercent):\n",
    "        break\n",
    "\n",
    "        \n",
    "        #cutPercent += 0.2\n",
    "winsound.PlaySound(\"*\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sizes of each weight matrix\n",
    "wtLengths = []\n",
    "for ii in range(len(wts)):\n",
    "    wtLengths.append(np.prod(wts[ii].shape))\n",
    "\n",
    "    \n",
    "print(np.sum(wtLengths), \"total weights\")\n",
    "\n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "print(nzwts, \"of\", np.sum(wtLengths), \"weights retained\")\n",
    "    \n",
    "plot_model_history_fromDict(historyDict, saveFig = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historyDict[\"mean_squared_error\"])\n",
    "plt.plot(historyDict[\"val_mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(os.path.join(savedModels,  modelName + '.h5'))\n",
    "\n",
    "# save scaler with same name as model\n",
    "scalerfileX = modelName + '_scalerX.pkl'\n",
    "pickle.dump(scalerX, open(os.path.join(dataOutput, scalerfileX), 'wb'))\n",
    "\n",
    "scalerfileY = modelName + '_scalerY.pkl'\n",
    "pickle.dump(scalerY, open(os.path.join(dataOutput, scalerfileY), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if model saved: \n",
    "K.clear_session()\n",
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(savedModels,  modelName + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nonzero weights\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,8, figsize=np.array((20,3)) , facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    axs[jj].hist(wts[jj].reshape(-1), bins = 100)\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(wts[jj].shape))\n",
    "    axs[jj+1].hist(wts[jj+1], bins = 100)\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(wts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(Xtest_scaled, Ytest_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nonzero weights\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim weights\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (5, 95), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "    \n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new weights and calculate new loss\n",
    "model.set_weights(wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.05*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.05*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# this is the original model\n",
    "inputs = Input(shape=(Xtrain_scaled.shape[1],))\n",
    "x = Dense(400, activation='tanh')(inputs)\n",
    "x = Dense(400, activation='tanh')(x)\n",
    "x = Dense(400, activation='tanh')(x)\n",
    "x = Dense(16, activation='tanh')(x)\n",
    "predictions = Dense(Ytrain_scaled.shape[1], activation='linear')(x)\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics = ['mse'])\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_mean_squared_error', patience=50, \n",
    "                          verbose=1, mode='auto', min_delta = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "print(nzwts)\n",
    "\n",
    "# trim weights\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (2, 98), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "    \n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "print(nzwts)\n",
    "model.set_weights(wts)\n",
    "\n",
    "\n",
    "# start training\n",
    "history = model.fit(Xtrain_scaled, Ytrain_scaled, validation_split = 0.3, \n",
    "                    verbose = 2, batch_size=2**14, epochs = 1, \n",
    "                    callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = model.get_weights().copy()\n",
    "\n",
    "# trim weights\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (2.5, 97.5), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "\n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "print(nzwts)\n",
    "model.set_weights(wts)\n",
    "\n",
    "model.evaluate(Xtest_scaled, Ytest_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history.history['mean_squared_error'])+1),\n",
    "             model_history.history['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "             model_history.history['val_mean_squared_error'])\n",
    "    axs.set_title('Model MSE')\n",
    "    axs.set_ylabel('mean_squared_error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "                   len(model_history.history['val_mean_squared_error'])/10)\n",
    "    axs.legend(['train', 'val'], loc='best')\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining.png\"), dpi = 120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "plot_model_history(history)\n",
    "print(history.history[\"loss\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model that was trained for much longer\n",
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(savedModels, 'my_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nonzero weights\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new weights and calculate new loss\n",
    "model.set_weights(wts)\n",
    "\n",
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.05*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.05*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = model.get_weights().copy()\n",
    "nzwts = [np.nonzero(wts[ii].reshape(-1))[0] for ii in range(len(wts))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,8, figsize=np.array((20,5)) , facecolor='w', edgecolor='k')\n",
    "#fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    axs[jj].hist(nzwts[jj].reshape(-1), bins = 30)\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(nzwts[jj].shape))\n",
    "    axs[jj+1].hist(nzwts[jj+1], bins = 30)\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(nzwts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,8, figsize=np.array((10,10)) , facecolor='w', edgecolor='k')\n",
    "#fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    axs[jj].hist(wts[jj].reshape(-1), bins = 30)\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(wts[jj].shape))\n",
    "    axs[jj+1].hist(wts[jj+1], bins = 30)\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(wts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "nnpreds = model.predict(Xtest_scaled[ :])\n",
    "\n",
    "# rescale\n",
    "nnpreds_unscaled = scalerY.inverse_transform(nnpreds)\n",
    "\n",
    "# show residuals\n",
    "# combine residual and regular plots\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,7, figsize=np.array((30, 8)) / 1.7, facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.5)\n",
    "axs = axs.ravel()\n",
    "\n",
    "# replace lightest colors with white\n",
    "import matplotlib.colors\n",
    "cmap = plt.cm.magma_r\n",
    "cmaplist = np.array([cmap(i) for i in range(cmap.N)])\n",
    "cmaplist[:,0:3] = np.divide(cmaplist[:, 0:3], 1.1)\n",
    "cmaplist[0] = (1,1,1,0.5)\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('mcm',cmaplist, cmap.N)\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    \n",
    "    try:\n",
    "        axs[ii].hexbin(y = Ytest.iloc[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[ii].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[ii].ticklabel_format(style='sci',  axis='y', scilimits=(3,4))\n",
    "        axs[ii].axes.xaxis.set_ticklabels([])\n",
    "        if(ii == 0):\n",
    "            axs[ii].set_ylabel(\"Actual Value\")\n",
    "        axs[ii].set_title(nms2[ii])\n",
    "        axs[ii].plot(Ytest.iloc[0:1000,ii], Ytest.iloc[0:1000,ii], 'grey', linewidth = 1, linestyle  = \"--\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    jj = ii + len(Y.columns)\n",
    "    \n",
    "    try:\n",
    "        axs[jj].hexbin(y = Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[jj].set_xlabel(\"Predicted Value\")\n",
    "        axs[jj].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[jj].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        mmin = np.min(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        mmax = np.max(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        \n",
    "        upper = np.max([np.abs(mmin), np.abs(mmax)])\n",
    "        axs[jj].set_ylim(-upper, upper)\n",
    "\n",
    "        if(ii == 0):\n",
    "            axs[jj].set_ylabel(\"Actual - Predicted\")\n",
    "        axs[jj].hlines(y = 0, xmin = np.min(nnpreds_unscaled[:,ii]), \n",
    "                       xmax = np.max(nnpreds_unscaled[:,ii]), linestyle =  \"--\", linewidth = 1)\n",
    "    except:\n",
    "        pass\n",
    "plt.tight_layout()\n",
    "#fig.savefig(os.path.join(figDir, \"SmallModelResids.png\"), dpi = 120, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim distribution of weights -- cut out middle 20%\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (40, 60), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "    \n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show new histogram of weights (excluding the 0's)\n",
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,5, figsize=np.array((15, 6)) , facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    \n",
    "    d1 = wts[jj].reshape(-1)\n",
    "    axs[jj].hist(d1[d1!=0], bins = 30, facecolor = '#d6bddb' )\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(wts[jj].shape))\n",
    "\n",
    "    d2 = wts[jj+1]\n",
    "    axs[jj+1].hist(d2[d2!=0], bins = 30, facecolor = '#d6bddb')\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(wts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the validation.split is the last X% of the data\n",
    "int(0.3*Xtrain_scaled.shape[0])\n",
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new weights and calculate new loss\n",
    "model.set_weights(wts)\n",
    "\n",
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "nnpreds = model.predict(Xtest_scaled[ :])\n",
    "\n",
    "# rescale\n",
    "nnpreds_unscaled = scalerY.inverse_transform(nnpreds)\n",
    "\n",
    "# show residuals\n",
    "# combine residual and regular plots\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,7, figsize=np.array((30, 8)) / 1.7, facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.5)\n",
    "axs = axs.ravel()\n",
    "\n",
    "# replace lightest colors with white\n",
    "import matplotlib.colors\n",
    "cmap = plt.cm.magma_r\n",
    "cmaplist = np.array([cmap(i) for i in range(cmap.N)])\n",
    "cmaplist[:,0:3] = np.divide(cmaplist[:, 0:3], 1.1)\n",
    "cmaplist[0] = (1,1,1,0.5)\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('mcm',cmaplist, cmap.N)\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    \n",
    "    try:\n",
    "        axs[ii].hexbin(y = Ytest.iloc[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[ii].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[ii].ticklabel_format(style='sci',  axis='y', scilimits=(3,4))\n",
    "        axs[ii].axes.xaxis.set_ticklabels([])\n",
    "        if(ii == 0):\n",
    "            axs[ii].set_ylabel(\"Actual Value\")\n",
    "        axs[ii].set_title(nms2[ii])\n",
    "        axs[ii].plot(Ytest.iloc[0:1000,ii], Ytest.iloc[0:1000,ii], 'grey', linewidth = 1, linestyle  = \"--\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    jj = ii + len(Y.columns)\n",
    "    \n",
    "    try:\n",
    "        axs[jj].hexbin(y = Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[jj].set_xlabel(\"Predicted Value\")\n",
    "        axs[jj].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[jj].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        mmin = np.min(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        mmax = np.max(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        \n",
    "        upper = np.max([np.abs(mmin), np.abs(mmax)])\n",
    "        axs[jj].set_ylim(-upper, upper)\n",
    "\n",
    "        if(ii == 0):\n",
    "            axs[jj].set_ylabel(\"Actual - Predicted\")\n",
    "        axs[jj].hlines(y = 0, xmin = np.min(nnpreds_unscaled[:,ii]), \n",
    "                       xmax = np.max(nnpreds_unscaled[:,ii]), linestyle =  \"--\", linewidth = 1)\n",
    "    except:\n",
    "        pass\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of hyperparameters\n",
    "\n",
    "\n",
    "# regularization, num layers, num nodes, learning rate, optimizer, activation function, batch size\n",
    "\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Create hyperparameter space\n",
    "NumHiddenLayers = randint(low = 2, high = 20)#[4, 2, 8]\n",
    "numUnits  = [2**4, 2**5, 2**6, 2**7, 2**8, 2**9, 2**10]\n",
    "epochs = [200]\n",
    "batches1 = [2**12, 2**10, 2**8, 2**14] \n",
    "optimizers = ['rmsprop', 'adam']\n",
    "dropout_rate =  uniform(loc = 0, scale = 0.5) #[0.0, 0.2, 0.5]\n",
    "weightRegularization = uniform(loc = 0, scale = 0.001) #[0, 0.0001, 0.001, 0.01]\n",
    "secondToLastUnits = [8, 16, 32, 64]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(optimizer=optimizers, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batches1,\n",
    "                        dropout_rate = dropout_rate, \n",
    "                        numUnits = numUnits, \n",
    "                        NumHiddenLayers = NumHiddenLayers, \n",
    "                        weightRegularization = weightRegularization, \n",
    "                        secondToLastUnits = secondToLastUnits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
